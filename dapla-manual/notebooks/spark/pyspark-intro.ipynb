{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4e5c4f-6e00-4bad-8984-b5feb06d9524",
   "metadata": {},
   "source": [
    "# Introduksjon til PySpark\n",
    "\n",
    "[Apache Spark](https://spark.apache.org/) er et sterkt verktøy som utvider mulighetsrommet for databehandling med R og Python. Kjernefunksjonaliteten ligger i at den lar deg kjøre en jobb på flere maskiner samtidig, noe som ikke er mulig med klassiske rammeverk som **Pandas** og **Tidyverse**. Følgelig er det et rammeverk som blant annet er veldig egnet for å prosessere store datamengder eller gjøre store beregninger. Les om mer om [Apache Spark i Dapla-manualen](./spark.html)\n",
    "\n",
    "I denne notebooken vises noen enkle eksempler på hvordan du kan jobbe med data med [PySpark](https://spark.apache.org/docs/latest/api/python/index.html), et Python-grensesnitt mot Spark. \n",
    "\n",
    "## Oppsett\n",
    "\n",
    "Når du logger deg inn på Dapla kan du velge mellom 2 ferdigoppsatte *kernels* for å jobbe med PySpark:\n",
    "\n",
    "1. Pyspark (local)\n",
    "2. Pyspark (k8s cluster)\n",
    "\n",
    "Den første lar deg bruke Spark på en enkeltmaskin, mens den andre lar deg distribuere kjøringen på mange maskiner avhengig av hvor store jobbene er. I eksemplene under brukes **Pyspark (local)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a3d798c-570d-4112-99ba-b3f4d694ca80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| label: first-cell\n",
    "#| code-fold: true\n",
    "# Importer biblioteker\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import date_format, explode, expr, sequence\n",
    "from pyspark.sql.types import DateType, DoubleType, StructField, StructType\n",
    "\n",
    "# Initialierer en SparkSession\n",
    "spark = (\n",
    "    SparkSession.builder.master(\"local[2]\")\n",
    "    .appName(\"Dapla-manual-example\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57296f81-c0b8-4d58-9535-c48ad0ab71db",
   "metadata": {},
   "source": [
    "I koden over importerer vi de bibliotekene vi skal bruke under. Grunnen til at vi importerer `pyspark.sql` er at dette er at **Spark SQL** er Apache Spark sin modul for å jobbe med strukturerte data. Og som navnet tilsier vil det si at vi kan blande Python og SQL i koden vår. Vi skal nærmere på hvordan man bruke SQL fra PySpark-notebook senere. \n",
    "\n",
    "Spark tilbyr også et eget grensesnitt, **Spark UI**, for å monitorere hva som skjer under en SparkSession. Vi kan bruke følgende kommando for å få opp en lenke til Spark UI i notebooken vår:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b10a7561-5cf0-47af-9e34-c07974449c8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/user/obr@ssb.no/proxy/4040/jobs/\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7814e8a6-7c89-44dc-881b-53338a5711ad",
   "metadata": {},
   "source": [
    "Klikker du på **Spark UI**-lenken så tar den deg til dashboard som lar deg monitorere, debugge, optimalisere og forstå kjøringene dine. Det kan være et svært nyttig verktøy i mange tilfeller. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb686b-eed9-42eb-8992-1390adb52e5c",
   "metadata": {},
   "source": [
    "## Generere data\n",
    "\n",
    "Vi kan begynne med å generere en Spark DataFrame med en kolonne som inneholder månedlige datoer for perioden 2000M1-2023M8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d9d1e1d-8173-47b6-a70b-be84c38a7bbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      Date|\n",
      "+----------+\n",
      "|2000-01-01|\n",
      "|2000-02-01|\n",
      "|2000-03-01|\n",
      "|2000-04-01|\n",
      "|2000-05-01|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate a sequence of dates\n",
    "dates_df = spark.range(1).select(\n",
    "    explode(\n",
    "        sequence(\n",
    "            start=expr(\"date '2000-01-01'\"),\n",
    "            stop=expr(\"date '2023-08-01'\"),\n",
    "            step=expr(\"interval 1 month\"),\n",
    "        )\n",
    "    ).alias(\"Date\")\n",
    ")\n",
    "dates_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833ad65e-4b12-4588-a697-dd6e586272e9",
   "metadata": {},
   "source": [
    "En Spark DataFrame er en distribuert samling av data som er organisert inn i kolonner. Siden Spark lar deg distribuere kjøringer på flere maskiner, er DataFrames optimalisert for å kunne splittes opp slik at de kan brukes på flere maskiner. Med andre er dette ikke det samme som en Pandas dataframe mange kjenner fra før. \n",
    "\n",
    "Over genererte vi en datokolonne. For å få litt mer data kan vi også generere 100 kolonner med tidsseriedata og så printer vi de 2 første av disse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b292de78-6144-47a6-ba9e-0fc2947dd7b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|           serie00|           serie01|\n",
      "+------------------+------------------+\n",
      "| 7.265541501656777|16.791320476113313|\n",
      "|10.026567688619423| 20.27479271345087|\n",
      "|10.151112240824128|18.390601668853087|\n",
      "|  9.23812028157886| 19.02831960059486|\n",
      "| 9.480083508895559|19.287135174878998|\n",
      "+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Genererer random walk data\n",
    "schema = StructType(\n",
    "    [StructField(f\"serie{i:02d}\", DoubleType(), True) for i in range(100)]\n",
    ")\n",
    "\n",
    "data = [\n",
    "    tuple((10 + np.random.normal(0, 1, 100)).cumsum().tolist())\n",
    "    for _ in range(284)  # 284 months from 2000-01 to 2023-08\n",
    "]\n",
    "\n",
    "data_df = spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "data_df.select(\"serie00\", \"serie01\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1df18c-f7d4-46ed-adbc-f95b1a2832eb",
   "metadata": {},
   "source": [
    "Til slutt kan vi joine de to datasettene sammen og lage noen kolonner som viser år, kvartal og måned. Deretter printer vi ut noen av kolonnene med kommandoen `show()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aee8d1d-0cd5-4065-8b76-433df3a62085",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-------+-----+------------------+------------------+\n",
      "|      Date|Year|Quarter|Month|           serie00|           serie01|\n",
      "+----------+----+-------+-----+------------------+------------------+\n",
      "|2000-01-01|2000|      1|   01|11.400000236743876|22.549406739178536|\n",
      "|2000-02-01|2000|      1|   02| 9.831046941220402|19.512681072618033|\n",
      "|2000-03-01|2000|      1|   03|10.549444253386536| 21.55248520513885|\n",
      "|2000-04-01|2000|      2|   04|10.841398016100063|19.738459409409366|\n",
      "|2000-05-01|2000|      2|   05| 9.513312884451398| 18.45161583167829|\n",
      "+----------+----+-------+-----+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Legger til row index til DataFrame før join med dates_df\n",
    "data_df = data_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n",
    "\n",
    "# Joiner de to datasettene\n",
    "df = (\n",
    "    dates_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n",
    "    .join(data_df, \"row_index\")\n",
    "    .drop(\"row_index\")\n",
    ")\n",
    "\n",
    "# Legger til år, kvartal og mnd\n",
    "df = df.withColumn(\"Year\", date_format(df.Date, \"yyyy\"))\n",
    "df = df.withColumn(\"Quarter\", expr(\"quarter(Date)\"))\n",
    "df = df.withColumn(\"Month\", date_format(df.Date, \"MM\"))\n",
    "\n",
    "df.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e76df3-f103-4f78-900b-238eec8aed25",
   "metadata": {},
   "source": [
    "Og med det har vi noe data vi kan jobbe med i resten av notebooken. \n",
    "\n",
    "## Skrive til Parquet\n",
    "\n",
    "PySpark tilbyr mange opsjoner ved skriving til parquet-filer som vi kanskje ikke er vant til å forholde oss til med enklere rammeverk som Pandas. Den enkleste måten å skrive ut en fil er som følger:\n",
    "\n",
    "```python\n",
    "df.write.parquet(\n",
    "    \"gs://ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet\"\n",
    ")\n",
    "```\n",
    "\n",
    "Dette vil fungere hvis filen ikke finnes fra før. Hvis den finnes fra før så vil den feile. Grunnen er at vi ikke har spesifisert hva vi ønsker at den skal gjøre. Vi kan velge mellom `overwrite`, `append`, `ignore` eller `errorifexists`. Sistnevnte er også default-oppførsel hvis du ikke ber den gjøre noe annet. \n",
    "\n",
    "Under bruker vi opsjonen `overwrite`, det vil si at den skriver over en evt eksisterende fil med samme navn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b51d6660-e506-42ae-9968-852ec4bc9399",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\n",
    "    \"gs://ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07abd5c4-7dc2-4c01-8947-623446b3e2ad",
   "metadata": {},
   "source": [
    "Vi kan inspisere hva som ble skrevet ved å liste ut innholder i bøtta. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebc60a1f-d443-409d-ac7c-4e6e8460ef9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ssb-prod-dapla-felles-data-delt/temp/',\n",
       " 'ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet',\n",
       " 'ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet/',\n",
       " 'ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet/_SUCCESS',\n",
       " 'ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet/part-00000-a5d51fe9-a30a-42ae-967d-03c6121e7e1a-c000.snappy.parquet',\n",
       " 'ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet/part-00001-a5d51fe9-a30a-42ae-967d-03c6121e7e1a-c000.snappy.parquet']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dapla import FileClient\n",
    "\n",
    "fs = FileClient.get_gcs_file_system()\n",
    "\n",
    "fs.glob(\"gs://ssb-prod-dapla-felles-data-delt/temp/**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26732ae1-0e72-4283-84b6-7b706086be7c",
   "metadata": {},
   "source": [
    "Hvis denne parquet-filen hadde vært partisjonert etter en kolonne, så ville det vært egne undermapper med navnestruktur `column_name=value` som indikerte hva filen er partisjonert på. Siden vi her bruker en maskin og har et lite datasett, valgte Spark å ikke partisjonere. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611262be-b7eb-408a-ad01-4bf9a09d4e2f",
   "metadata": {},
   "source": [
    "## PySpark og SQL\n",
    "\n",
    "Du kan også skrive SQL med Spark. For å skrive SQL må vi først lage et `temporary view`. Her kaller vi de **tidsserie**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18d6a9fd-6daa-4d08-98f7-8be1efe6b4b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"tidsserie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297a779-ffcd-4c4b-9608-5e36d9945dea",
   "metadata": {},
   "source": [
    "Vi kan deretter skrive en SQL-statement som vi ønsker å kjøre på viewet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2c00323-1ce4-4ca6-911a-7197cb35e77a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM tidsserie WHERE Year > 2010\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020ebf7a-27b2-4055-8d50-632f52b434ee",
   "metadata": {},
   "source": [
    "Deretter kan vi bruke det til å filtrere datasettet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ffe42a4-9c6b-4186-8623-3026f82e85ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-------+-----+------------------+------------------+\n",
      "|      Date|Year|Quarter|Month|           serie00|           serie01|\n",
      "+----------+----+-------+-----+------------------+------------------+\n",
      "|2011-01-01|2011|      1|   01| 9.854707963962703|19.133946043669454|\n",
      "|2011-02-01|2011|      1|   02|10.179074587939663|  20.5107178162615|\n",
      "|2011-03-01|2011|      1|   03|10.739177215503856| 21.06622572477892|\n",
      "|2011-04-01|2011|      2|   04|  9.97595627489884| 20.49656707675814|\n",
      "|2011-05-01|2011|      2|   05| 9.498355280804706| 18.63779112173551|\n",
      "|2011-06-01|2011|      2|   06| 10.99071563452657| 21.66622684522529|\n",
      "|2011-07-01|2011|      3|   07| 9.447888296305013|18.147813688294338|\n",
      "|2011-08-01|2011|      3|   08|  9.26056155464723| 20.07349552968892|\n",
      "|2011-09-01|2011|      3|   09|10.280652771851381| 19.91685876971767|\n",
      "|2011-10-01|2011|      4|   10|  11.7096811436004|22.602236548172403|\n",
      "+----------+----+-------+-----+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df = spark.sql(query)\n",
    "result_df.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebc26b0d-608b-47f4-bc20-f86641e4965c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark (local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
