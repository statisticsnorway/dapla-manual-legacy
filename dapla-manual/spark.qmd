# Apache Spark

![](images/spark.png){style="max-width: 30%; float: right;" fig-alt="Apache Spark logo"}

Koding i **R** og **Python** har historisk sett foregått på en enkelt maskin og vært begrenset av minnet (RAM) og prosessorkraften på maskinen. For bearbeiding av små og mellomstore datasett er det sjelden et problem på kjøre på en enkelt maskin. Populære pakker som `dplyr` for **R**,  og `pandas` for **Python**, blir ofte brukt i denne typen databehandling. I senere år har det også kommet pakker som er optimalisert for å kjøre kode parallelt på flere kjerner på en enkelt maskin, skrevet i minne-effektive språk som **Rust** og **C++**. 

Men selv om man kommer langt med å kjøre kode på en enkelt maskin, vil enkelte oppgaver kreve mer minne og prosessorkraft enn dette. For større datasett, eller store beregninger, kan det være nyttig å bruke et rammeverk som kan kjøre kode parallelt på flere maskiner. Et slikt rammeverk er [Apache Spark](https://spark.apache.org/).

## Bruksområder

Apache Spark er et rammeverk for å kjøre kode parallelt på flere maskiner. Det er bygget for å håndtere store datasett og store beregninger. Det er derfor et nyttig verktøy for å løse problemer som er for store for å kjøre på en enkelt maskin. Men det finnes også andre bruksområder som er nyttige for Apache Spark. Her er noen eksempler:

- **Store datasett**: Hvis du har et datasett som er for stort til å lese inn i minnet på en enkelt maskin, kan du bruke Apache Spark til å lese inn datasettet og kjøre kode på det.

- **Store beregninger**: Hvis du har en beregning som er for stor til å kjøre på en enkelt maskin, kan du bruke Apache Spark til å kjøre beregningen.

- **Distribuert databehandling**: Hvis du har en beregning som kan deles opp i flere deler, kan du bruke Apache Spark til å kjøre beregningen parallelt på flere maskiner på en effektiv måte. 

- **Redusert kjøretid**: Hvis du har kjøringer som tar lengre tid en du ønsker, så kan Apache Spark brukes til å redusere kjøretiden ved å kjøre kode parallelt på flere maskiner.

- **Datavarehus**: Hvis du trenger database-funksjonalitet over Parquet-filer, så så kan Apache Spark sammen med for eksempel [Delta Lake](https://delta.io/) brukes som et datavarehus.

- **Strømmende data**: Hvis du trenger å prosessere strømmende data, så kan Apache Spark brukes til å prosessere data i sanntid.

Dette er noen av bruksområdene der Spark kan løse problemer som er for store for å kjøre på en enkelt maskin med for eksempel Pandas eller dplyr.

## Spark på Dapla

Dapla kjører på et Kubernetes-kluster og er derfor er et svært egnet sted for å kjøre kode parallelt på flere maskiner. Jupyter på Dapla har også en flere klargjorte kernels for å kjøre kode i Apache Spark. Denne koden vil kjøre på et eget kluster av maskiner som er dedikert til Apache Spark, slik som vist i @fig-kernels. 

::: {#fig-kernels layout-nrow=2}

![PySpark på kubernetes](images/spark-python-k8.png){#fig-py-k8}

![PySpark på 1 maskin](images/spark-python-local.png){#fig-py-local}

![SparkR på kubernetes](images/spark-r-k8.png){#fig-r-k8}

![SparkR på 1 maskin](images/spark-r-local.png){#fig-r-local}

Ferdigkonfigurerte kernels for Spark på Dapla.
:::

@fig-py-k8 og @fig-r-k8 kan velges hvis du ønsker å bruke Spark for å kjøre store jobber på flere maskiner, for henholdsvis Python- og R-grensesnittene for Spark. 

@fig-py-local og @fig-r-local bør du velge hvis du ønsker å bruke Spark av andre grunner enn å kjøre store jobber på flere maskiner. For eksempel hvis du ønsker å bruke en av de mange pakker som er bygget på Spark, eller hvis du ønsker å bruke Spark til å lese og skrive data fra Dapla.

Hvis du ønsker å sette opp et eget virtuelt miljø for å kjøre Spark, så kan du bruke ssb-project. Se [ssb-project](./jobbe-med-kode.html#ssb-project) for mer informasjon.

## Spark i {{< fa brands r-project >}} og Python{{< fa brands python >}}

Spark er implementert i programmingsspråket **Scala**. Men det tilbys mange grensesnitt for å bruke Spark fra andre språk. De mest populære grensesnittene er **PySpark** for Python og **SparkR** for R. Disse grensesnittene er bygget på Spark, og gir tilgang til Spark-funksjonalitet fra Python og R.

### PySpark

PySpark er et Python-grensesnitt for Apache Spark. Det er en Python-pakke som gir tilgang til Spark-funksjonalitet fra Python. Det er enkelt å bruke, og har mange av de samme funksjonene som Pandas.

Tester om jeg kan inkludere noen kodeceller og output fra en notebook:

{{< embed notebooks/test.ipynb echo=true >}}

### SparkR

SparkR er et R-grensesnitt for Apache Spark. Det er en R-pakke som gir tilgang til Spark-funksjonalitet fra R. Det er enkelt å bruke, og har mange av de samme funksjonene som dplyr.