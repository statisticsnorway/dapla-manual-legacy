{
  "hash": "917126a9dbd43030cdaade38fafcc3db",
  "result": {
    "markdown": "---\ntitle: Introduksjon til PySpark\n---\n\n\n\n[Apache Spark](https://spark.apache.org/) er et sterkt verktøy som utvider mulighetsrommet for databehandling med R og Python. Kjernefunksjonaliteten ligger i at den lar deg kjøre en jobb på flere maskiner samtidig, noe som ikke er mulig med klassiske rammeverk som **Pandas** og **Tidyverse**. Følgelig er det et rammeverk som blant annet er veldig egnet for å prosessere store datamengder eller gjøre store beregninger. Les om mer om [Apache Spark i Dapla-manualen](./spark.html)\n\nI denne notebooken vises noen enkle eksempler på hvordan du kan jobbe med data med [PySpark](https://spark.apache.org/docs/latest/api/python/index.html), et Python-grensesnitt mot Spark. \n\n## Oppsett\n\nNår du logger deg inn på Dapla kan du velge mellom 2 ferdigoppsatte *kernels* for å jobbe med PySpark:\n\n1. Pyspark (local)\n2. Pyspark (k8s cluster)\n\nDen første lar deg bruke Spark på en enkeltmaskin, mens den andre lar deg distribuere kjøringen på mange maskiner avhengig av hvor store jobbene er. I eksemplene under brukes **Pyspark (local)**.\n\n\n::: {#first-cell .cell tags='[]' execution_count=1}\n``` {.python .cell-code}\n# Importer biblioteker\nimport numpy as np\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import date_format, explode, expr, sequence, sum, avg\nfrom pyspark.sql.types import DateType, DoubleType, StructField, StructType\n\n# Initialierer en SparkSession\nspark = (\n    SparkSession.builder.master(\"local[2]\")\n    .appName(\"Dapla-manual-example\")\n    .getOrCreate()\n)\n```\n:::\n\n\nI koden over importerer vi de bibliotekene vi skal bruke under. Grunnen til at vi importerer `pyspark.sql` er at dette er at **Spark SQL** er Apache Spark sin modul for å jobbe med strukturerte data. Og som navnet tilsier vil det si at vi kan blande Python og SQL i koden vår. Vi skal nærmere på hvordan man bruke SQL fra PySpark-notebook senere. \n\nSpark tilbyr også et eget grensesnitt, **Spark UI**, for å monitorere hva som skjer under en SparkSession. Vi kan bruke følgende kommando for å få opp en lenke til Spark UI i notebooken vår:\n\n::: {.cell tags='[]' execution_count=2}\n``` {.python .cell-code}\nspark.sparkContext\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/user/obr@ssb.no/proxy/4041/jobs/\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[*]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>pyspark-shell</code></dd>\n            </dl>\n        </div>\n        \n```\n:::\n:::\n\n\nKlikker du på **Spark UI**-lenken så tar den deg til dashboard som lar deg monitorere, debugge, optimalisere og forstå kjøringene dine. Det kan være et svært nyttig verktøy i mange tilfeller. \n\n## Generere data\n\nVi kan begynne med å generere en Spark DataFrame med en kolonne som inneholder månedlige datoer for perioden 2000M1-2023M8. \n\n::: {.cell tags='[]' execution_count=3}\n``` {.python .cell-code}\n# Genererer månedlige data\ndates_df = spark.range(1).select(\n    explode(\n        sequence(\n            start=expr(\"date '2000-01-01'\"),\n            stop=expr(\"date '2023-08-01'\"),\n            step=expr(\"interval 1 month\"),\n        )\n    ).alias(\"Date\")\n)\ndates_df.show(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----------+\n|      Date|\n+----------+\n|2000-01-01|\n|2000-02-01|\n|2000-03-01|\n|2000-04-01|\n|2000-05-01|\n+----------+\nonly showing top 5 rows\n\n```\n:::\n:::\n\n\nEn Spark DataFrame er en distribuert samling av data som er organisert inn i kolonner. Siden Spark lar deg distribuere kjøringer på flere maskiner, er DataFrames optimalisert for å kunne splittes opp slik at de kan brukes på flere maskiner. Med andre er dette ikke det samme som en Pandas dataframe mange kjenner fra før. \n\nOver genererte vi en datokolonne. For å få litt mer data kan vi også generere 100 kolonner med tidsseriedata og så printer vi de 2 første av disse:\n\n::: {.cell tags='[]' execution_count=4}\n``` {.python .cell-code}\n# Genererer random walk data\nschema = StructType(\n    [StructField(f\"serie{i:02d}\", DoubleType(), True) for i in range(100)]\n)\n\ndata = [\n    tuple((10 + np.random.normal(0, 1, 100)).cumsum().tolist())\n    for _ in range(284)  # 284 months from 2000-01 to 2023-08\n]\n\ndata_df = spark.createDataFrame(data, schema=schema)\n\ndata_df.select(\"serie00\", \"serie01\").show(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+------------------+------------------+\n|           serie00|           serie01|\n+------------------+------------------+\n|10.410703377184355| 21.06318801110079|\n|10.509249410154466|  19.5674295298024|\n| 9.618310122060274|17.635805093465642|\n| 9.691112692298294|18.593842915949082|\n| 9.903675228685067|20.012215769058564|\n+------------------+------------------+\nonly showing top 5 rows\n\n```\n:::\n:::\n\n\nTil slutt kan vi joine de to datasettene sammen og lage noen kolonner som viser år, kvartal og måned. Deretter printer vi ut noen av kolonnene med kommandoen `show()`. \n\n::: {#gen-df .cell tags='[]' execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\n# Legger til row index til DataFrame før join med dates_df\ndata_df = data_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n\n# Joiner de to datasettene\ndf = (\n    dates_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n    .join(data_df, \"row_index\")\n    .drop(\"row_index\")\n)\n\n# Legger til år, kvartal og mnd\ndf = df.withColumn(\"Year\", date_format(df.Date, \"yyyy\"))\ndf = df.withColumn(\"Quarter\", expr(\"quarter(Date)\"))\ndf = df.withColumn(\"Month\", date_format(df.Date, \"MM\"))\n\ndf.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n```\n:::\n:::\n\n\nOg med det har vi noe data vi kan jobbe med i resten av notebooken. \n\n## Skrive til Parquet\n\nPySpark tilbyr mange opsjoner ved skriving til parquet-filer som vi kanskje ikke er vant til å forholde oss til med enklere rammeverk som Pandas. Den enkleste måten å skrive ut en fil er som følger:\n\n```python\ndf.write.parquet(\n    \"gs://ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet\"\n)\n```\n\nDette vil fungere hvis filen ikke finnes fra før. Hvis den finnes fra før så vil den feile. Grunnen er at vi ikke har spesifisert hva vi ønsker at den skal gjøre. Vi kan velge mellom `overwrite`, `append`, `ignore` eller `errorifexists`. Sistnevnte er også default-oppførsel hvis du ikke ber den gjøre noe annet. \n\nUnder bruker vi opsjonen `overwrite`, det vil si at den skriver over en evt eksisterende fil med samme navn. \n\n::: {.cell tags='[]' execution_count=6}\n``` {.python .cell-code}\ndf.write.mode(\"overwrite\").parquet(\n    \"gs://ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet\"\n)\n```\n:::\n\n\nVi kan inspisere hva som ble skrevet ved å liste ut innholder i bøtta. \n\n::: {.cell tags='[]' execution_count=7}\n``` {.python .cell-code}\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-prod-dapla-felles-data-delt/temp/**\")\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n['ssb-prod-dapla-felles-data-delt/temp/',\n 'ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet',\n 'ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet/',\n 'ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet/_SUCCESS',\n 'ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet/part-00000-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet',\n 'ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet/part-00001-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet']\n```\n:::\n:::\n\n\nHvis denne parquet-filen hadde vært partisjonert etter en kolonne, så ville det vært egne undermapper med navnestruktur `column_name=value` som indikerte hva filen er partisjonert på. Siden vi her bruker en maskin og har et lite datasett, valgte Spark å ikke partisjonere. \n\n## Lese inn Parquet\n\nApache Spark kan lese inn flere parquet-filer samtidig. Syntaxen er like enkel som den for å skrive ut. \n\n::: {.cell tags='[]' execution_count=8}\n``` {.python .cell-code}\ndf_ts = spark.read.parquet(\n    \"gs://ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet\"\n)\ndf_ts.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie66\", \"serie55\").show(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----------+----+-------+-----+-----------------+-----------------+\n|      Date|Year|Quarter|Month|          serie66|          serie55|\n+----------+----+-------+-----+-----------------+-----------------+\n|2000-01-01|2000|      1|   01|670.2679830025959|562.4312808525777|\n|2000-02-01|2000|      1|   02|675.4233411662802|562.5168447360121|\n|2000-03-01|2000|      1|   03|687.3412458214908|568.6203957584232|\n|2000-04-01|2000|      2|   04|673.1128047244955|557.4633871253379|\n|2000-05-01|2000|      2|   05| 667.513406101114|561.7766450346327|\n+----------+----+-------+-----+-----------------+-----------------+\nonly showing top 5 rows\n\n```\n:::\n:::\n\n\n## PySpark og SQL\n\nDu kan også skrive SQL med Spark. For å skrive SQL må vi først lage et `temporary view`. Under kaller vi viewt for **tidsserie**.\n\n::: {.cell tags='[]' execution_count=9}\n``` {.python .cell-code}\ndf.createOrReplaceTempView(\"tidsserie\")\n```\n:::\n\n\nVi kan deretter skrive en SQL-statement som vi ønsker å kjøre på viewet:\n\n::: {.cell tags='[]' execution_count=10}\n``` {.python .cell-code}\nquery = \"SELECT * FROM tidsserie WHERE Year = 2010\"\n```\n:::\n\n\nDeretter kan vi bruke det til å filtrere datasettet:\n\n::: {.cell tags='[]' execution_count=11}\n``` {.python .cell-code}\nresult_df = spark.sql(query)\nresult_df.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2010-01-01|2010|      1|   01| 11.26910423907778|21.730128215168268|\n|2010-02-01|2010|      1|   02| 8.722783282690738| 17.46851086792347|\n|2010-03-01|2010|      1|   03|10.376873608348605|20.109386343182802|\n|2010-04-01|2010|      2|   04|11.459959305590926|21.995141825523866|\n|2010-05-01|2010|      2|   05|10.441456792180572| 21.25096473981906|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n```\n:::\n:::\n\n\n## Aggregering\n\nvi kan aggregere opp enten med PySpark-syntax eller SQL-syntax. Under viser vi begge:\n\n::: {.cell tags='[]' execution_count=12}\n``` {.python .cell-code}\nfrom pyspark.sql import functions as F\n\n# Assuming df_ts is your DataFrame\naggregated_df = df_ts.groupBy(\"Quarter\").agg(F.sum(\"serie00\").alias(\"Sum\"),\n                                             F.avg(\"serie00\").alias(\"Average\"),\n                                             F.max(\"serie00\").alias(\"Maximum\"))\n\n# Show the result\naggregated_df.show()\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+-------+------------------+------------------+------------------+\n|Quarter|               Sum|           Average|           Maximum|\n+-------+------------------+------------------+------------------+\n|      1|363.95869885234185|10.109963857009497|11.829453550532005|\n|      3|365.68324879453405| 10.15786802207039|12.233378837422391|\n|      4| 342.2334082209804|10.065688477087658|12.210138970053695|\n|      2|  361.991445506568|10.055317930738001|12.276030776082463|\n+-------+------------------+------------------+------------------+\n\n```\n:::\n:::\n\n\nLa oss gjøre det samme med SQL, men grupperer etter to variabler og sorterer output etterpå.\n\n::: {.cell tags='[]' execution_count=13}\n``` {.python .cell-code}\n# Assuming df_ts is your DataFrame\ndf_ts.createOrReplaceTempView(\"temp_table\")\n\n# Now you can run a SQL query\nquery = \"\"\"\n    SELECT\n        Year,\n        Quarter,\n        SUM(serie00) AS Sum,\n        AVG(serie00) AS Average,\n        MAX(serie00) AS Maximum\n    FROM \n        temp_table\n    GROUP BY \n        Year, Quarter\n    ORDER BY\n        YEAR, QUARTER\n\"\"\"\n\naggregated_df_sql = spark.sql(query)\n\n# Show the result\naggregated_df_sql.show(10)\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n+----+-------+------------------+------------------+------------------+\n|Year|Quarter|               Sum|           Average|           Maximum|\n+----+-------+------------------+------------------+------------------+\n|2000|      1|31.323050432219453|10.441016810739818|11.118293927071951|\n|2000|      2|28.911192473027377| 9.637064157675793| 9.900977410685329|\n|2000|      3|33.670797229797415|11.223599076599138|12.233378837422391|\n|2000|      4|28.094793356286914| 9.364931118762305| 10.32000478359274|\n|2001|      1|31.636678535169537|10.545559511723178|11.367822302191831|\n|2001|      2|29.629770128521507| 9.876590042840503|11.135215930381191|\n|2001|      3| 30.75408440118315| 10.25136146706105|10.723803326978505|\n|2001|      4|30.361048932627902|  10.1203496442093|10.368365984482093|\n|2002|      1|31.184163218551227|10.394721072850409|10.550579652234951|\n|2002|      2|29.128978392451202|   9.7096594641504|10.186365745367246|\n+----+-------+------------------+------------------+------------------+\nonly showing top 10 rows\n\n```\n:::\n:::\n\n\n",
    "supporting": [
      "pyspark-intro_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}