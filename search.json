[
  {
    "objectID": "ordforklaringer.html",
    "href": "ordforklaringer.html",
    "title": "Ordforklaringer",
    "section": "",
    "text": "Ordforklaringer\n\nbip\nbip er det tidligere navnet p√• den underliggende plattformen som SSB bygger i GCP, hovedsakelig ment for utviklere som bygger tjenester p√• Dapla. Plattformen skulle v√¶re selvbetjent for utviklere og basert p√• DevOps-prinsipper. bip eksisterer fortsatt, men er n√• blitt en del av det st√∏rre begrepet dapla.\n\n\nbucket\nbucket (eller b√∏tte p√• norsk) er en lagringsenhet p√• Dapla. Det ligner litt p√• en klassisk diskstasjon, for eksempel X-disken eller C-disken p√• en lokal maskin. I en b√∏tte kan det ligge undermapper slik som i et klassisk filsystem.\n\n\nconsumer\nconsumer er en AD-gruppe som gir tilgang til et Dapla-team sin delt-b√∏tte. En SSB-ansatt som skal bruke data fra et Dapla-team m√• v√¶re medlem av consumer-gruppen til det aktuelle Dapla-teamet.\n\n\ndapla\nDapla er et akronym for den nye dataplattformen til SSB, der Da st√•r for Data og pla st√•r for Plattform. Dapla er en plattform for lagring, prosessering og deling av SSB sine data. Den best√•r b√•de av Jupyter-milj√∏et, som er et verkt√∏y for √• utf√∏re beregninger og analysere data, og et eget omr√•de for lagre data. I tillegg inkluderer begrepet Dapla ogs√• en rekke andre verkt√∏y som er n√∏dvendige for √• kunne bruke plattformen.\n\n\ndapla-team\nKommer snart.\n\n\ndapla-toolbelt\nKommer snart.\n\n\ndata-admin\ndata-admin er en AD-gruppe som gir de videste tilgangene i et dapla-team. En SSB-ansatt som har data-admin-rollen i et Dapla-team har tilgang til alle b√∏tter for det teamet, inkludert kilde-b√∏tta som kan inneha sensitive data.\nKommer snart.\n\n\ndapla-start\n*dapla-start** er et brukergrensesnitt der SSB-ansatte kan s√∏ke om √• f√• opprettet et nytt dapla-team.\n\n\ndelt-b√∏tte\nKommer snart.\n\n\ndeveloper\nKommer snart.\n\n\nPersonidentifiserende Informasjon (PII)\nPII er variabler som kan identifisere en person i et datasett.\nMer informasjon finnes hos Datatilsynet.\n\n\ngoogle cloud platform (gcp)\nAllmenn skyplattform utviklet og levert av Google. Konkurrent med Amazon Web Services (AWS) og Microsoft Azure. Dapla prim√¶rt benytter seg av tjenester p√• GCP.\nVideo som forklarer hva GCP er.\n\n\ngcp\nForkortelse for Google Cloud Platform. Se forklaring under google cloud platform (GCP).\n\n\nInfrastructure as Code (IaC)\nInfrastuktur som kode p√• norsk. Kode som defineres ressurser, typisk p√• en allmenn skyplatform som GCP. Eksempler av ressurser er b√∏tter, databaser, virtuelle maskiner, nettverk og sikkerhetsregler.\n\n\nkilde-b√∏tte\nKommer snart.\n\n\nprodukt-b√∏tte\nKommer snart.\n\n\nPull Request (PR)\nEn PR er en Github konsept, som gir et forum for kodegjennomgang, diskusjon og ikke minst dokumentasjon av kodeendringer.\nDette er anbefalt av KVAKK som m√•ten √• endre kode p√• i SSB.\n\n\nssb-project\nKommer snart.\n\n\ntransfer service\nKommer snart.\n\n\nPyflakes\nPyflakes er et enkelt kodeanalyseverkt√∏y som finner feil i Python kode. Les mer om Pyflakes p√• deres PyPi side"
  },
  {
    "objectID": "hva-er-botter.html",
    "href": "hva-er-botter.html",
    "title": "Hva er b√∏tter?",
    "section": "",
    "text": "P√• Dapla er det Google Cloud Storage (GCS) som benyttes til √• lagre data og filer. F√∏lgelig er det GCS som erstatter det vi kjente som Linux-stammene i prodsonen tidligere. I SSB har vi v√¶rt vant til √• jobbe med data lagret p√• filsystemer i et Linux-milj√∏1. GCS-b√∏ttene skiller seg fra klassiske filsystemer p√• flere m√•ter, og det er viktig √• v√¶re klar over disse forskjellene. I denne kapitlet vil vi g√• gjennom noen av de viktigste forskjellene og hvordan man gj√∏r vanlige operasjoner mot b√∏tter i GCS.\n\n\nI et Linux- eller Windows-filsystem, som vi har v√¶rt vant til tidligere, s√• er filer og mapper organisert i en hierarkisk struktur p√• et operativsystem (OS). I SSB har OS-ene v√¶rt installert p√• fysiske maskiner som vi vedlikeholder selv.\nEn b√∏tte i GCS er derimot en kj√∏pt tjeneste som lar brukeren lagre alle typer objekter i en container. Man trenger alts√• ikke √• tenke p√• om filene ligger i et hierarki, hvilket operativsystem det kj√∏rer p√•, eller hvor mye diskplass som er tilgjengelig.\nAt data er lagret som objekter i en b√∏tte har noen praktiske implikasjoner for hvordan vi jobber med data i SSB:\n\nMange er vant til √• jobbe direkte med filer i en Linux-terminal eller via systemkall fra spr√•k som SAS, Pyton eller R. For √• gj√∏re det samme i Jupyter mot en b√∏tte, s√• kan vi bruke Python-pakken gcsfs. Se eksempler under.\nN√•r vi bruker Python- eller R-pakker for lese eller skrive data fra b√∏tter, s√• er vi avhengig av at pakkene tilbyr integrasjon mot b√∏tter. Mange pakker gj√∏r det, men ikke alle. For de som ikke gj√∏r det kan vi bruke ofte bruke gcsfs til √• gi oss et python-objekt, som igjen kan brukes av de fleste pakker.\nDet finnes egentlig ikke noen mapper i b√∏tter. I motsetning til et vanlig filsystem s√• er det ikke en hierarkisk mappestruktur i en b√∏tte. Det vil si at alt ligger et sted, men de kan ha lange navn med /-tegn slik at det ligner p√• et klassisk filsystem. Bruker du / i objekt-navnet s√• vil ogs√• Google Cloud Console vise det som mapper, men det er bare for √• gj√∏re det enklere √• forholde seg til. En praktisk konsekvens av dette er f.eks. at man ikke trenger √• opprette en mappe f√∏r man legger et fil/objekt i den. Det er bare en tekststreng som er en del av objekt-navnet.\n\nUnder kommer det en del eksempler p√• hvordan man kan jobbe med objekter i b√∏tter p√• samme m√•te som filer i et filsystem.\n\n\n\nP√• Dapla skal data lagres i b√∏tter. Men n√•r du √•pner Jupyterlab s√• f√•r du ogs√• et ‚Äúlokalt‚Äù eller klassisk filsystem, slik vi definerte det i forrige kapittel. Det er dette filsystemet du ser i filutforskeren i Jupyterlab, slik som vist til venstre i Figur¬†1. Det er ogs√• dette filsystemet du ser n√•r du f.eks. bruker ls-kommandoen i en terminal i Jupyterlab.\n\n\n\nFigur¬†1: Til venstre i bildet ser du filutforskeren i Jupyterlab\n\n\nDette filsystemet er ment for √• lagre kode midlertidig mens du jobber med dem. Det er ikke ment for √• lagre data. Det er heller ikke ment som et permanent lagringssted for kode. Permanent lagring av kode skal gj√∏res p√• GitHub. Selv om filene du lagrer der fortsetter √• eksistere for hver gang du logger deg inn i Jupyterlab, s√• b√∏r kode du √∏nsker √• bevare pushes til GitHub f√∏r du avslutter en sesjon i Jupyterlab. Se en mer teknisk beskrivelse av hvordan dette fungerer i boksen under.\n\n\n\n\n\n\nHvem er egentlig denne Jovyan?\n\n\n\nN√•r du logger deg inn i Jupyterlab p√• Dapla, s√• ser du at brukeren din p√• det lokale filsystemet heter jovyan. Grunnen er at det er en Linux-container som kj√∏rer et Jupyterlab Docker-image, og alle bruker denne containeren. Det er derfor vi kaller det et ‚Äúlokalt‚Äù filsystem, fordi det er lokalt i containeren.\nI tillegg er det satt opp persistent volum (PV) og en persistent volume claim (PVC) som er knyttet til denne containeren. Det er dette som gj√∏r at filene du lagrer i Jupyterlab blir lagret mellom hver gang du logger deg inn. I tillegg tas det en backup av filsystemene hver dag, slik at vi kan gjenopprette filene dine hvis noe skulle skje med PV-en.\nLagringsplassen p√• PV-en er begrenset til 10 GB per bruker. Grunnen til at vi ikke tilbyr mer lagringsplass er fordi det kun er kode skal lagres her, og at denne lagringsplassen er relativt dyrt sammenlignet med GCS-b√∏tter. Hvis du jobber i virtuelle milj√∏er og lagrer mange milj√∏er lokalt, kan du oppleve at disken blir full. Les mer her om hvordan man b√∏r h√•ndere dette.\n\n\n\n\n\nTidligere har vi diskutert forskjellene mellom b√∏tter og filsystemer. Mange kjenner hvordan man gj√∏r systemkommandoer2 i klassiske filsystemer fra en terminal eller fra spr√•k som Python, R og SAS. I dette kapitlet viser vi hvordan dette kan gj√∏res mot b√∏tter fra Jupyterlab.\ngcsfs er en Python-pakke som lar deg jobbe med objekter i b√∏tter p√• nesten samme m√•te som filer i et filsystem. For √• kunne gj√∏re det m√• vi f√∏rst sette opp en filsystem-instans som lar oss bruke en b√∏tte som et filsystem. Pakken dapla-toolbelt lar oss gj√∏re det ganske enkelt:\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\nfs er n√• et filsystem-versjon av b√∏ttene vi har tilgang til p√• GCS. Vi kan n√• bruk fs til √• gj√∏re typiske operasjoner vi har v√¶rt vant til √• gj√∏re i en Linux-terminal. Se en fullstendig liste over kommandoer her.\nUnder vises noen flere eksempler p√• nyttige funksjoner enn det som ble vist i kapitlet tidligere i boken\n\n\n\n\n\n\nWarning\n\n\n\nSelv om ordet mapper blir brukt i eksemplene under, s√• er det viktig √• huske at det ikke finnes noen mapper i b√∏tter. Det er bare en tekststreng som er en del av objekt-navnet. Men siden vi her jobber med et filsystem-representasjon av b√∏tten, s√• tillater vi oss √• gj√∏re det for √• gj√∏re det enklere √• lese.\n\n\n\n\nfs.glob() lar oss s√∏ke etter filer i b√∏tten. Vi kan bruke *, **, ? og [..] som wildcard for √• finne det vi trenger. Det som returneres er enten en liste eller dictionary avhengig av hva vi velger √• gj√∏re.\nHent en liste over alle filer i en undermappe R_smoke_test i b√∏tta gs://ssb-prod-dapla-felles-data-delt:\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-prod-dapla-felles-data-delt/R_smoke_test/*\")\nN√•r vi legger til * p√• slutten av filstien s√• returnerer den alle filer i den eksakte undermappen. Men hvis vi √∏nsker √• √• f√• alle filer i alle undermapper, s√• kan vi bruke ** p√• denne m√•ten:\nfs.glob(\"gs://ssb-prod-dapla-felles-data-delt/R_smoke_test/**\").\nVi kan ogs√• s√∏ke mer avansert ved ved √• bruke ?. ?-tegnet sier at en enkeltkarakter kan v√¶re hva som helst. F.eks. kunne vi skrevet\nfs.glob(\"gs://ssb-prod-dapla-felles-data-delt/R_smoke_test/**/R_smoke_test_??.csv\")\nfor √• rekursivt liste ut alle filer som starter med R_smoke_test_, etterfulgt av to karakterer av hvilken som helst type, og slutter med .csv.\nHvis vi viste at de to karakterene ikke skulle v√¶re av hvilken som helst karakterer, men det var en liten bokstav, etterfulgt av et tall mellom 2-6, s√• kunne vi brukt [a-z] og [2-6] for √• spesifisere dette. F.eks. kunne vi skrevet:\nfs.glob(\"gs://ssb-prod-dapla-felles-data-delt/R_smoke_test/**/R_smoke_test_[a-z][2-6].csv\")\nSom vi ser er fs.glob() et verkt√∏y som git oss mye den funksjonaliteten vi er vant til fra Linux-terminalen. Men i tillegg kan vi bruke den til √• hente inn metadataene til de filene/objektene vi f√•r treff p√•, ved √• bruke argumentet detail=True. Her er et eksempel:\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\n    \"gs://ssb-prod-dapla-felles-data-delt/R_smoke_test/**/R_smoke_test_/[a-z][2-6].csv\",\n    detail=True,\n)\nMetadataene gir deg da informasjon om filst√∏rrelse, tidspunkt for opprettelse, tidspunkt for siste endring, og hvem som har opprettet og endret filen.\n\n\n\nI noen tilfeller kan det v√¶re nyttig √• sjekke om en fil eksisterer i b√∏tten. Det kan vi gj√∏re med fs.exists():\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.exists(\"gs://ssb-prod-dapla-felles-data-delt/altinn3/form_dc551844cd74.xml\")\nKoden returnerer enten True eller False avhengig av om filen eksisterer eller ikke.\n\n\n\nHvis du lurer p√• hvor mange GB data du har i en b√∏tte, s√• kan du bruke fs.du():\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\ntotal_bytes = fs.du(\n    \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/\",\n)\n\ntotal_size_gb = total_bytes / (1024**3)\nprint(f\"Total size: {total_size_gb:.3f} GB\")\n\n\n\nHvert objekt i b√∏tta har metadata knyttet til seg som kan hentes inn i Jupyter og benyttes.\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\nfile = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/form_dc551844cd74.xml\"\n\nfs.info(file)\nDette gir deg blant annet informasjon om filst√∏rrelse, tidspunkt for opprettelse, og tidspunkt for siste endring. Hvis du √∏nsker dette for flere filer s√• kan man ogs√• bruke fs.glob(&lt;pattern&gt;, details=True) som vi s√• p√• tidligere.\n\n\n\nfs.open() lar oss √•pne en fil i b√∏tta for lesing og skriving med vanlige Python-funksjoner. Funksjonen returnerer et fil-aktig objekt som kan brukes med vanlige som Python-biblioteker som Pandas og NumPy.\nimport pandas as pd\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\nfile_path = \"gs://ssb-prod-dapla-felles-data-delt/dapla-metrics/number-of-teams.csv\"\n\n# Bruker fs.open() til √• √•pne fil, og deretter leses den med Pandas\nwith fs.open(file_path, \"r\") as file:\n    df = pd.read_csv(file)\ndf\nDu kan ogs√• bruke fs.open() til √• skrive til en fil i b√∏tta. Her er et eksempel p√• hvordan man skriver en parquet-fil med Pandas og PyArrow:\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom dapla import FileClient\n\nfile_path = \"gs://ssb-prod-dapla-felles-data-delt/dapla-metrics/test.parquet\"\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\n# Lager eksempeldata\ndata = {\"A\": [1, 2, 3], \"B\": [4, 5, 6]}\ndf = pd.DataFrame(data)\n\n# Skriver parquet-fil\nwith fs.open(file_path, \"wb\") as file:\n    pq.write_table(pa.Table.from_pandas(df), file)\nOver brukte vi wb for √• √•pne den bin√¶re filen for skriving. Hvis du √∏nsker √• lese fra en bin√¶r fil s√• bruker du rb. Skulle du jobbet en ren tekstfil, s√• hadde man brukt w til √• skrive og r til √• lese.\n\n\n\nfs.touch() lar deg opprette en tom fil i b√∏tta, eller oppdatere metadataene til objektet for n√•r den sist ble modifisert.\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nfs.touch(\"gs://ssb-prod-dapla-felles-data-delt/dapla-metrics/test.parquet\")\n\n\n\nfs.put() lar deg laste opp en fil fra Jupyter-filsystemet3 til b√∏tta. Husk at filstien til ditt hjemmeomr√•de p√• Jupyter er /home/jovyan/. Her er et eksempel p√• hvordan man kan bruke det p√• enkelt-filer:\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/custom.scss\"\ndestination = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/test2.csv\"\n\nfs.put(source, destination)\nDu kan ogs√• kopiere hele mapper mellom jovyan og b√∏ttene:\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/sesongjustering/\"\ndestination = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/sesongjustering/\"\n\nfs.put(source, destination, recursive=True)\nEt brukstilfellet for √• kopiere mellom b√∏tter og jovyan er ved sesongjustering med Jdemetra+ og JWSACruncher. Siden Jdemetra+ ikke kan lese/skrive fra b√∏tter, s√• m√• vi midlertidig kopiere dataene til jovyan med fs.put() f√∏r vi kan kj√∏re sesongjusteringen. N√•r vi er ferdige med kj√∏ringen kopierer vi dataene tilbake til b√∏tta med fs.get().\n\n\n\nfs.get() gj√∏r det samme som fs.put(), bare motsatt vei. Den kopierer fra en b√∏tte til jovyan.\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/sesongjustering/\"\ndestination = \"/home/jovyan/sesongjustering/\"\n\nplaceholder = fs.get(source, destination, recursive=True)\nfs.get() returnerer potensielt mye output. Hvis du ikke vil ha det, s√• kan du bare definere det som et objekt. F.eks. placeholder som i eksempelet over.\n\n\n\nfs.mv() lar deg flytte filer og mapper mellom b√∏tter, samt √• gi objekter nye navn.\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-prod-dapla-felles-data-delt/dapla-metrics/number-of-teams.csv\"\ndestination = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/number-of-teams.csv\"\nfs.mv(source, destination)\n\n\n\nfs.copy() lar deg kopiere filer og mapper mellom b√∏tter. I eksempelet under kopierer vi rekursivt:\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\n\n\n\nfs.rm() lar deg slette filer og mapper i b√∏tta. Her sletter vi en enkeltfil:\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfs.rm(\"gs://ssb-prod-dapla-felles-data-delt/altinn3/number-of-teams.csv\")\nOgs√• denne funksjonen tar et recursive-argument hvis du √∏nsker √• slette en hel mappe."
  },
  {
    "objectID": "hva-er-botter.html#b√∏tter-vs-filsystemer",
    "href": "hva-er-botter.html#b√∏tter-vs-filsystemer",
    "title": "Hva er b√∏tter?",
    "section": "",
    "text": "I et Linux- eller Windows-filsystem, som vi har v√¶rt vant til tidligere, s√• er filer og mapper organisert i en hierarkisk struktur p√• et operativsystem (OS). I SSB har OS-ene v√¶rt installert p√• fysiske maskiner som vi vedlikeholder selv.\nEn b√∏tte i GCS er derimot en kj√∏pt tjeneste som lar brukeren lagre alle typer objekter i en container. Man trenger alts√• ikke √• tenke p√• om filene ligger i et hierarki, hvilket operativsystem det kj√∏rer p√•, eller hvor mye diskplass som er tilgjengelig.\nAt data er lagret som objekter i en b√∏tte har noen praktiske implikasjoner for hvordan vi jobber med data i SSB:\n\nMange er vant til √• jobbe direkte med filer i en Linux-terminal eller via systemkall fra spr√•k som SAS, Pyton eller R. For √• gj√∏re det samme i Jupyter mot en b√∏tte, s√• kan vi bruke Python-pakken gcsfs. Se eksempler under.\nN√•r vi bruker Python- eller R-pakker for lese eller skrive data fra b√∏tter, s√• er vi avhengig av at pakkene tilbyr integrasjon mot b√∏tter. Mange pakker gj√∏r det, men ikke alle. For de som ikke gj√∏r det kan vi bruke ofte bruke gcsfs til √• gi oss et python-objekt, som igjen kan brukes av de fleste pakker.\nDet finnes egentlig ikke noen mapper i b√∏tter. I motsetning til et vanlig filsystem s√• er det ikke en hierarkisk mappestruktur i en b√∏tte. Det vil si at alt ligger et sted, men de kan ha lange navn med /-tegn slik at det ligner p√• et klassisk filsystem. Bruker du / i objekt-navnet s√• vil ogs√• Google Cloud Console vise det som mapper, men det er bare for √• gj√∏re det enklere √• forholde seg til. En praktisk konsekvens av dette er f.eks. at man ikke trenger √• opprette en mappe f√∏r man legger et fil/objekt i den. Det er bare en tekststreng som er en del av objekt-navnet.\n\nUnder kommer det en del eksempler p√• hvordan man kan jobbe med objekter i b√∏tter p√• samme m√•te som filer i et filsystem."
  },
  {
    "objectID": "hva-er-botter.html#lokalt-filsystem-p√•-dapla",
    "href": "hva-er-botter.html#lokalt-filsystem-p√•-dapla",
    "title": "Hva er b√∏tter?",
    "section": "",
    "text": "P√• Dapla skal data lagres i b√∏tter. Men n√•r du √•pner Jupyterlab s√• f√•r du ogs√• et ‚Äúlokalt‚Äù eller klassisk filsystem, slik vi definerte det i forrige kapittel. Det er dette filsystemet du ser i filutforskeren i Jupyterlab, slik som vist til venstre i Figur¬†1. Det er ogs√• dette filsystemet du ser n√•r du f.eks. bruker ls-kommandoen i en terminal i Jupyterlab.\n\n\n\nFigur¬†1: Til venstre i bildet ser du filutforskeren i Jupyterlab\n\n\nDette filsystemet er ment for √• lagre kode midlertidig mens du jobber med dem. Det er ikke ment for √• lagre data. Det er heller ikke ment som et permanent lagringssted for kode. Permanent lagring av kode skal gj√∏res p√• GitHub. Selv om filene du lagrer der fortsetter √• eksistere for hver gang du logger deg inn i Jupyterlab, s√• b√∏r kode du √∏nsker √• bevare pushes til GitHub f√∏r du avslutter en sesjon i Jupyterlab. Se en mer teknisk beskrivelse av hvordan dette fungerer i boksen under.\n\n\n\n\n\n\nHvem er egentlig denne Jovyan?\n\n\n\nN√•r du logger deg inn i Jupyterlab p√• Dapla, s√• ser du at brukeren din p√• det lokale filsystemet heter jovyan. Grunnen er at det er en Linux-container som kj√∏rer et Jupyterlab Docker-image, og alle bruker denne containeren. Det er derfor vi kaller det et ‚Äúlokalt‚Äù filsystem, fordi det er lokalt i containeren.\nI tillegg er det satt opp persistent volum (PV) og en persistent volume claim (PVC) som er knyttet til denne containeren. Det er dette som gj√∏r at filene du lagrer i Jupyterlab blir lagret mellom hver gang du logger deg inn. I tillegg tas det en backup av filsystemene hver dag, slik at vi kan gjenopprette filene dine hvis noe skulle skje med PV-en.\nLagringsplassen p√• PV-en er begrenset til 10 GB per bruker. Grunnen til at vi ikke tilbyr mer lagringsplass er fordi det kun er kode skal lagres her, og at denne lagringsplassen er relativt dyrt sammenlignet med GCS-b√∏tter. Hvis du jobber i virtuelle milj√∏er og lagrer mange milj√∏er lokalt, kan du oppleve at disken blir full. Les mer her om hvordan man b√∏r h√•ndere dette."
  },
  {
    "objectID": "hva-er-botter.html#systemkommandoer-mot-b√∏ttter",
    "href": "hva-er-botter.html#systemkommandoer-mot-b√∏ttter",
    "title": "Hva er b√∏tter?",
    "section": "",
    "text": "Tidligere har vi diskutert forskjellene mellom b√∏tter og filsystemer. Mange kjenner hvordan man gj√∏r systemkommandoer2 i klassiske filsystemer fra en terminal eller fra spr√•k som Python, R og SAS. I dette kapitlet viser vi hvordan dette kan gj√∏res mot b√∏tter fra Jupyterlab.\ngcsfs er en Python-pakke som lar deg jobbe med objekter i b√∏tter p√• nesten samme m√•te som filer i et filsystem. For √• kunne gj√∏re det m√• vi f√∏rst sette opp en filsystem-instans som lar oss bruke en b√∏tte som et filsystem. Pakken dapla-toolbelt lar oss gj√∏re det ganske enkelt:\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\nfs er n√• et filsystem-versjon av b√∏ttene vi har tilgang til p√• GCS. Vi kan n√• bruk fs til √• gj√∏re typiske operasjoner vi har v√¶rt vant til √• gj√∏re i en Linux-terminal. Se en fullstendig liste over kommandoer her.\nUnder vises noen flere eksempler p√• nyttige funksjoner enn det som ble vist i kapitlet tidligere i boken\n\n\n\n\n\n\nWarning\n\n\n\nSelv om ordet mapper blir brukt i eksemplene under, s√• er det viktig √• huske at det ikke finnes noen mapper i b√∏tter. Det er bare en tekststreng som er en del av objekt-navnet. Men siden vi her jobber med et filsystem-representasjon av b√∏tten, s√• tillater vi oss √• gj√∏re det for √• gj√∏re det enklere √• lese.\n\n\n\n\nfs.glob() lar oss s√∏ke etter filer i b√∏tten. Vi kan bruke *, **, ? og [..] som wildcard for √• finne det vi trenger. Det som returneres er enten en liste eller dictionary avhengig av hva vi velger √• gj√∏re.\nHent en liste over alle filer i en undermappe R_smoke_test i b√∏tta gs://ssb-prod-dapla-felles-data-delt:\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-prod-dapla-felles-data-delt/R_smoke_test/*\")\nN√•r vi legger til * p√• slutten av filstien s√• returnerer den alle filer i den eksakte undermappen. Men hvis vi √∏nsker √• √• f√• alle filer i alle undermapper, s√• kan vi bruke ** p√• denne m√•ten:\nfs.glob(\"gs://ssb-prod-dapla-felles-data-delt/R_smoke_test/**\").\nVi kan ogs√• s√∏ke mer avansert ved ved √• bruke ?. ?-tegnet sier at en enkeltkarakter kan v√¶re hva som helst. F.eks. kunne vi skrevet\nfs.glob(\"gs://ssb-prod-dapla-felles-data-delt/R_smoke_test/**/R_smoke_test_??.csv\")\nfor √• rekursivt liste ut alle filer som starter med R_smoke_test_, etterfulgt av to karakterer av hvilken som helst type, og slutter med .csv.\nHvis vi viste at de to karakterene ikke skulle v√¶re av hvilken som helst karakterer, men det var en liten bokstav, etterfulgt av et tall mellom 2-6, s√• kunne vi brukt [a-z] og [2-6] for √• spesifisere dette. F.eks. kunne vi skrevet:\nfs.glob(\"gs://ssb-prod-dapla-felles-data-delt/R_smoke_test/**/R_smoke_test_[a-z][2-6].csv\")\nSom vi ser er fs.glob() et verkt√∏y som git oss mye den funksjonaliteten vi er vant til fra Linux-terminalen. Men i tillegg kan vi bruke den til √• hente inn metadataene til de filene/objektene vi f√•r treff p√•, ved √• bruke argumentet detail=True. Her er et eksempel:\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\n    \"gs://ssb-prod-dapla-felles-data-delt/R_smoke_test/**/R_smoke_test_/[a-z][2-6].csv\",\n    detail=True,\n)\nMetadataene gir deg da informasjon om filst√∏rrelse, tidspunkt for opprettelse, tidspunkt for siste endring, og hvem som har opprettet og endret filen.\n\n\n\nI noen tilfeller kan det v√¶re nyttig √• sjekke om en fil eksisterer i b√∏tten. Det kan vi gj√∏re med fs.exists():\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.exists(\"gs://ssb-prod-dapla-felles-data-delt/altinn3/form_dc551844cd74.xml\")\nKoden returnerer enten True eller False avhengig av om filen eksisterer eller ikke.\n\n\n\nHvis du lurer p√• hvor mange GB data du har i en b√∏tte, s√• kan du bruke fs.du():\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\ntotal_bytes = fs.du(\n    \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/\",\n)\n\ntotal_size_gb = total_bytes / (1024**3)\nprint(f\"Total size: {total_size_gb:.3f} GB\")\n\n\n\nHvert objekt i b√∏tta har metadata knyttet til seg som kan hentes inn i Jupyter og benyttes.\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\nfile = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/form_dc551844cd74.xml\"\n\nfs.info(file)\nDette gir deg blant annet informasjon om filst√∏rrelse, tidspunkt for opprettelse, og tidspunkt for siste endring. Hvis du √∏nsker dette for flere filer s√• kan man ogs√• bruke fs.glob(&lt;pattern&gt;, details=True) som vi s√• p√• tidligere.\n\n\n\nfs.open() lar oss √•pne en fil i b√∏tta for lesing og skriving med vanlige Python-funksjoner. Funksjonen returnerer et fil-aktig objekt som kan brukes med vanlige som Python-biblioteker som Pandas og NumPy.\nimport pandas as pd\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\nfile_path = \"gs://ssb-prod-dapla-felles-data-delt/dapla-metrics/number-of-teams.csv\"\n\n# Bruker fs.open() til √• √•pne fil, og deretter leses den med Pandas\nwith fs.open(file_path, \"r\") as file:\n    df = pd.read_csv(file)\ndf\nDu kan ogs√• bruke fs.open() til √• skrive til en fil i b√∏tta. Her er et eksempel p√• hvordan man skriver en parquet-fil med Pandas og PyArrow:\nimport pandas as pd\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom dapla import FileClient\n\nfile_path = \"gs://ssb-prod-dapla-felles-data-delt/dapla-metrics/test.parquet\"\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\n# Lager eksempeldata\ndata = {\"A\": [1, 2, 3], \"B\": [4, 5, 6]}\ndf = pd.DataFrame(data)\n\n# Skriver parquet-fil\nwith fs.open(file_path, \"wb\") as file:\n    pq.write_table(pa.Table.from_pandas(df), file)\nOver brukte vi wb for √• √•pne den bin√¶re filen for skriving. Hvis du √∏nsker √• lese fra en bin√¶r fil s√• bruker du rb. Skulle du jobbet en ren tekstfil, s√• hadde man brukt w til √• skrive og r til √• lese.\n\n\n\nfs.touch() lar deg opprette en tom fil i b√∏tta, eller oppdatere metadataene til objektet for n√•r den sist ble modifisert.\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nfs.touch(\"gs://ssb-prod-dapla-felles-data-delt/dapla-metrics/test.parquet\")\n\n\n\nfs.put() lar deg laste opp en fil fra Jupyter-filsystemet3 til b√∏tta. Husk at filstien til ditt hjemmeomr√•de p√• Jupyter er /home/jovyan/. Her er et eksempel p√• hvordan man kan bruke det p√• enkelt-filer:\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/custom.scss\"\ndestination = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/test2.csv\"\n\nfs.put(source, destination)\nDu kan ogs√• kopiere hele mapper mellom jovyan og b√∏ttene:\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"/home/jovyan/sesongjustering/\"\ndestination = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/sesongjustering/\"\n\nfs.put(source, destination, recursive=True)\nEt brukstilfellet for √• kopiere mellom b√∏tter og jovyan er ved sesongjustering med Jdemetra+ og JWSACruncher. Siden Jdemetra+ ikke kan lese/skrive fra b√∏tter, s√• m√• vi midlertidig kopiere dataene til jovyan med fs.put() f√∏r vi kan kj√∏re sesongjusteringen. N√•r vi er ferdige med kj√∏ringen kopierer vi dataene tilbake til b√∏tta med fs.get().\n\n\n\nfs.get() gj√∏r det samme som fs.put(), bare motsatt vei. Den kopierer fra en b√∏tte til jovyan.\nfrom dapla import FileClient\n\n# Oppretter filsystem-instans\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/sesongjustering/\"\ndestination = \"/home/jovyan/sesongjustering/\"\n\nplaceholder = fs.get(source, destination, recursive=True)\nfs.get() returnerer potensielt mye output. Hvis du ikke vil ha det, s√• kan du bare definere det som et objekt. F.eks. placeholder som i eksempelet over.\n\n\n\nfs.mv() lar deg flytte filer og mapper mellom b√∏tter, samt √• gi objekter nye navn.\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\nsource = \"gs://ssb-prod-dapla-felles-data-delt/dapla-metrics/number-of-teams.csv\"\ndestination = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/number-of-teams.csv\"\nfs.mv(source, destination)\n\n\n\nfs.copy() lar deg kopiere filer og mapper mellom b√∏tter. I eksempelet under kopierer vi rekursivt:\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\n\n\n\nfs.rm() lar deg slette filer og mapper i b√∏tta. Her sletter vi en enkeltfil:\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfs.rm(\"gs://ssb-prod-dapla-felles-data-delt/altinn3/number-of-teams.csv\")\nOgs√• denne funksjonen tar et recursive-argument hvis du √∏nsker √• slette en hel mappe."
  },
  {
    "objectID": "hva-er-botter.html#footnotes",
    "href": "hva-er-botter.html#footnotes",
    "title": "Hva er b√∏tter?",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nEgentlig har vi jobbet med data-filer p√• b√•de Linux- og Windows-filsystemer. Men Linux-stammene har v√¶rt det anbefalte stedet √• lagre datafiler.‚Ü©Ô∏é\nMed systemkommandoer s√• mener vi bash-kommandoer som ls og mv, eller implementasjoner av disse kommandoene i Python, R eller SAS.‚Ü©Ô∏é\nJupyter-milj√∏et har sitt eget filsystem, ofte kalt jovyan. Det er som et vanlig Linux-filsystem, og vil v√¶re det vi omtaler som ‚Äúlokalt‚Äù p√• maskinen din i Jupyter.‚Ü©Ô∏é"
  },
  {
    "objectID": "altinn3.html",
    "href": "altinn3.html",
    "title": "Altinn 3",
    "section": "",
    "text": "Frem mot sommeren 2025 skal alle skjema-unders√∏kelser i SSB som gjennomf√∏res p√• Altinn 2 flyttes over til Altinn 3. Skjemaer som flyttes til Altinn 3 vil motta sine data p√• Dapla, og ikke p√• bakken som tidligere. Datafangsten h√•ndteres av Team SUV, mens statistikkseksjonene henter sine data fra Team SUV sitt lagringsomr√•de p√• Dapla. I dette kapitlet beskriver vi n√¶rmere hvordan statistikkseksjonene kan jobbe med Altinn3-data p√• Dapla. Kort oppsummert best√•r det av disse stegene:\n\nStatistikkprodusenten avtaler overf√∏ring av skjema fra Altinn 2 til Altinn 3 med planleggere p√• S821, som koordinerer denne jobben.\nN√•r statistikkprodusentene f√•r beskjed om at Altinn3-skjemaet skal sendes ut til oppgavegiverne, s√• m√• de opprette et Dapla-team.\nN√•r Dapla-teamet er opprettet, og f√∏rste skjema er sendt inn, s√• ber de Team SUV om √• gi statistikkteamet tilgang til dataene som har kommet inn fra Altinn 3. I tillegg ber de om at Team SUV gir tilgang til teamets Transfer Service instans. 1 Merk at det m√• gis separate tilganger for data i staging- og produksjonsmilj√∏.\nStatistikkprodusenten setter opp en automatisk overf√∏ring av skjemadata med Transfer Service, fra Team SUV sitt lagringsomr√•de over til Dapla-teamet sin kildeb√∏tte.\nStatistikkprodusentene kan begynne √• jobbe med dataene i Dapla. Blant annet tilbyr Dapla en automatiseringstjeneste man kan bruke for √• prosessere dataene fra kildedata til inndata2.\n\nUnder forklarer vi mer med mer detaljer hvordan man g√•r frem for gjennomf√∏re steg 4-5 over.\n\n\n\n\n\n\nAnsvar for kildedata\n\n\n\nSelv om Team SUV tar ansvaret for datafangst fra Altinn3, s√• er det statistikkteamet som har ansvaret for langtidslagring av dataene i sin kildeb√∏tte. Det vil si at at statistikkteamet m√• s√∏rge for at data overf√∏res til sin kildeb√∏tte, og at de kan ikke regne med at Team SUV tar vare p√• en backup av dataene.\n\n\n\n\nN√•r skjema-dataene kommer inn fra Altinn 3 til Team SUV sitt lagringsomr√•de, s√• er det en del ting som er verdt √• tenke p√•:\n\nHvordan er filene organisert hos Team SUV?\nFilstrukturen der Team SUV lagrer dataene som kommer inn fra Altinn 3, har en klar struktur. Du kan selv g√• inn √• kikke p√• dataene, enten fra Jupyter eller Google Cloud Console, og bli bedre kjent med strukturen3. Figur¬†1 viser en hvordan en typisk filsti ser ut p√• lagringsomr√•det til Team SUV. Det starter med navnet til b√∏tta som Team SUV har opprettet for skjemaet. Deretter viser den innvitteringsdato, deretter et teknisk navn, og til slutt selve skjemanavnet.\n\n\n\n\nFigur¬†1: Typisk filsti for et Altinn3-skjema.\n\n\n\nHvordan organisere dataene i din kildeb√∏tte?\nN√•r vi bruker Transfer Service til √• synkronisere innholdet i Team SUV sitt lagringsomr√•de til Dapla-teamet sitt lagringsomr√•de, s√• er det mest hensiktmessig √• fortsette √• bruke mapppe-strukturen som Team SUV har. Grunnen er at vi ikke kan legge p√• noe logikk som lager en ny struktur. Tjenesten bare kopierer over data. Men du kan kan lage et nytt toppniv√•-mappe som du √∏nsker at dataene skal synkroniseres til. F.eks. at alle Altinn-dataene legger seg inn i en mappe som heter altinn. Det er spesielt nyttig hvis du har flere datakilder som skal ligge i samme kildeb√∏tte.\nUnike skjemanavn\nSkjemanavnet du ser i Figur¬†1 er unike. Dvs. at hvis en oppgavegiver sender inn flere skjemaer p√• samme dag, s√• er fortsatt skjemanavnet unikt. Det er viktig √• v√¶re klar over n√•r man bruker automatiseringstjenesten for kildedata senere. Siden tjenesten trigges per fil, og man er avhengig av unike navn for √• ikke skrive over filer, s√• er det nyttig √• vite at man kan videref√∏re skjemanavnet i overgangen fra kildedata til inndata.\n\n\n\n\nN√•r vi skal overf√∏re filer fra Team SUV sin b√∏tte til v√•r kildeb√∏tte, s√• kan vi gj√∏re det manuelt fra Jupyter som forklart her.. Men det er en bedre l√∏sning √• bruke en tjeneste som gj√∏r dette for deg. Transfer Service er en tjeneste som kan brukes til √• synkronisere innholdet mellom b√∏tter p√• Dapla, samt mellom bakke og sky. N√•r du skal ta i bruk tjenesten for √• overf√∏re data mellom en b√∏tte fra Team SUV sitt prosjekt altinn-data-prod, til en kildedata-b√∏tte i Dapla-teamet ditt, s√• gj√∏r du f√∏lgende:\n\nF√∏lg denne beskrivelsen hvordan man setter opp overf√∏ringsjobber.\nEtter at du har trykket p√• Create Transfer Job velger du Google Cloud Storage p√• begge alternativene under Get Started. Deretter g√•r du videre ved √• klikke p√• Next Step.\nUnder Choose a source s√• skal du velge hvor du skal kopiere data fra. Trykk p√• Browse. I vinduet som dukker opp trykker du p√• üîª-ikonet ved siden av Project ID. I s√∏kevinduet som dukker opp s√∏ker du opp altinn-data-prod og trykker p√• navnet. Da f√•r du listet opp alle b√∏ttene i altinn-data-prod prosjektet. Til slutt trykker du p√• b√∏tta som Team SUV har opprettet for unders√∏kelsen4 og klikker Select til nederst p√• siden. Trykk deretter Next step for √• g√• videre.\nUnder Choose a destination s√• skal du velge hvor du skal kopiere data til. Dette steget er ganske likt som forrige, men du skal n√• velge ditt eget projekt og kildeb√∏tta der. Trykk p√• Browse. I vinduet som dukker opp trykker du p√• üîª-ikonet ved siden av Project ID. I s√∏kevinduet som dukker opp s√∏ker du opp prod-&lt;ditt teamnavn&gt; og trykker p√• navnet. Da f√•r du listet opp alle b√∏ttene i ditt team sitt prosjekt. Velg kildeb√∏tta som har navnet ssb-prod-&lt;teamnavn&gt;-data-kilde. Hvis du √∏nsker √• kopiere data til en undermappe i b√∏tta, s√• trykker du p√• &gt;-ikonet ved b√∏ttenavnet og velger √∏nsket undermappe5. Til slutt trykker du p√• Select til nederst p√• siden. Trykk deretter Next step for √• g√• videre.\nUnder Choose how and when to run job velger du Run with custom frequency og Starting now i Batch-modus. Hvis du √∏nsker √• overf√∏re s√• ofte som mulig, s√• velger du Custom frequency og Repeat every 1 Hours. Til slutt trykker du Next Step nederst p√• siden.\nUnder Choose Settings s√• legger du til en kort beskrivelse av jobben du har opprettet. Under de andre valgene kan du gj√∏re f√∏lgende:\n\nUnder Advanced transfer Options trenger du ikke gj√∏re noen endringer.\nUnder When to overwrite kan du velge If different.\nUnder When to delete kan du velge Never.\nUnder Manifest trenger du ikke huke av.\nUnder Logging options velger du samme som vist i Figur¬†2.\n\n\n\n\n\nFigur¬†2: Valg av opsjoner for logging i Transfer Service\n\n\nTil slutt trykker du Create for √• aktivere tjenesten. Den vil da sjekke Team SUV sin b√∏tte hver time og kopiere over alle filer som ikke allerede eksisterer statistikkteamets kildeb√∏tte.\n\n\n\nN√•r du har satt opp Transfer Service til √• kopiere over filer fra Team SUV sin b√∏tte til statistikkteamets kildeb√∏tte, s√• vil det potensielt komme inn nye skjemaer hver time. Disse kan du lese inn i Jupyter og jobbe med hvis du er kildedata-ansvarlig i teamet. Hvis du ikke er det s√• m√• du vente til dataene er tilgjengeliggjort i produkt-b√∏tta til teamet.\nSiden f√• personer innehar rollen som kildedata-ansvarlig s√• er det laget en automatiseringstjeneste som kan bearbeide alle filer som kommer inn i kildeb√∏tta. Den lar deg kj√∏re et python-script p√• alle filer som kommer inn i kildeb√∏tta.\nLes mer om hvordan du kan bruker tjenesten her.\n\n\n\nI denne delen deles noen tips og triks for √• jobbe med Altinn3-dataene p√• Dapla. Fokuset vil v√¶re p√• hvordan du kan lese inn og transformere xml-filer fra skjema-dataene.\n\n\nFor √• se innholdet i en mappe gir det mest mening √• bruke Google Cloud Console. Her kan du se b√•de filer og mapper i et pek-og-klikk grensesnitt. Men du kan ikke se p√• innholdet i filene der. Til det m√• du bruke Jupyter.\nAnta at vi √∏nsker √• liste ut alle som leverte skjema den 10. mars 2023. Da kan vi bruke gcsfs til √• gj√∏re det6:\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Print the list of parquet files\nxml_files\nHer bruker vi fs.glob()-funksjonen fra gcsfs til √• loope gjennom alle undermapper av gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/, og legge alle filer som slutter med .xml til listen xml_files. Dermed har vi et Python-objekt som kun brukes til √• hente inn de filene vi √∏nsker. Vi bruker da det vi vet om mappestrukturen i Team SUV sin b√∏tte som vi s√• tidligere i Figur¬†1.\n\n\n\nNoen ganger kan det v√¶re nyttig √• se en xml-fil direkte. Da kan vi lese den inn i Jupyter og printe den ut. Her er et eksempel p√• hvordan vi kan gj√∏re det, samtidig som vi formatterer den med xml.dom.minidom:\nfrom xml.dom.minidom import parseString\n\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\n# Sett inn filstien din her\nfile = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/form_dc551844cd74.xml\"\n\ndom = parseString(fs.cat_file(file))\npretty_xml = dom.toprettyxml(indent=\"  \")\nprint(pretty_xml)\nVi brukte da fs.cat_file() til √• hente inn innholdet i mappen, gjorde det om til xml-object, og formatterer outputen med xml.dom.minidom. Resultatet vil da se ut noe s√•nt som dette:\n&lt;?xml version=\"1.0\" ?&gt;\n&lt;melding xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" dataFormatProvider=\"SERES\" dataFormatId=\"7079\" dataFormatVersion=\"47315\"&gt;\n  &lt;InternInfo&gt;\n    &lt;raNummer&gt;RA-XXXX&lt;/raNummer&gt;\n    &lt;skjemaVersjon&gt;1.0&lt;/skjemaVersjon&gt;\n    &lt;undersoekelsesNr&gt;XXXXX&lt;/undersoekelsesNr&gt;\n    &lt;visOppgaveByrde&gt;0&lt;/visOppgaveByrde&gt;\n    &lt;visBrukeropplevelse&gt;0&lt;/visBrukeropplevelse&gt;\n    &lt;delregNr&gt;2XXXXX&lt;/delregNr&gt;\n    &lt;periodeFritekst&gt;3. kvartal 2022&lt;/periodeFritekst&gt;\n    &lt;periodeFomDato&gt;2022-07-01&lt;/periodeFomDato&gt;\n    &lt;periodeTomDato&gt;2022-09-30&lt;/periodeTomDato&gt;\n    &lt;periodeType&gt;KVRT&lt;/periodeType&gt;\n    &lt;periodeNummer&gt;3&lt;/periodeNummer&gt;\n    &lt;periodeAAr&gt;2022&lt;/periodeAAr&gt;\n    &lt;periodeDato&gt; &lt;/periodeDato&gt;\n    &lt;enhetsIdent&gt;XXXXXX&lt;/enhetsIdent&gt;\n    &lt;enhetsType&gt;BEDR&lt;/enhetsType&gt;\n    &lt;enhetsOrgNr&gt;XXXXXXXXX&lt;/enhetsOrgNr&gt;\n    &lt;enhetsNavn&gt;STATISTISK SENTRALBYR√Ö &lt;/enhetsNavn&gt;\n    &lt;enhetsGateadresse&gt;Akersveien 26&lt;/enhetsGateadresse&gt;\n    &lt;enhetsPostnr&gt;0177&lt;/enhetsPostnr&gt;\n    &lt;enhetsPoststed&gt;OSLO&lt;/enhetsPoststed&gt;\n    &lt;enhetsAvdeling&gt;AVD XXXX&lt;/enhetsAvdeling&gt;\n    &lt;reporteeOrgNr&gt;XXXXXXXXX&lt;/reporteeOrgNr&gt;\n  &lt;/InternInfo&gt;\n  &lt;Kontakt&gt;\n    &lt;kontaktPersonNavn&gt;OLA NORDMANN&lt;/kontaktPersonNavn&gt;\n    &lt;kontaktPersonEpost&gt;ola@tull.no&lt;/kontaktPersonEpost&gt;\n    &lt;kontaktPersonTelefon&gt; &lt;/kontaktPersonTelefon&gt;\n    &lt;kontaktInfoBekreftet&gt;1&lt;/kontaktInfoBekreftet&gt;\n    &lt;kontaktInfoKommentar&gt;Dette er en et eksempel for Dapla-manualen&lt;/kontaktInfoKommentar&gt;\n  &lt;/Kontakt&gt;\n  &lt;Skjemadata&gt;\n    &lt;ledigeStillinger&gt;75&lt;/ledigeStillinger&gt;\n    &lt;datoPrefill&gt; &lt;/datoPrefill&gt;\n  &lt;/Skjemadata&gt;\n&lt;/melding&gt;\n\n\n\nDe f√¶rreste √∏nsker √• jobbe direkte med XML-filer. Derfor er det nyttig √• kunne transformere XML-filene til et mer brukervennlig format, f.eks. en Pandas Dataframe. Her er et eksempel p√• hvordan vi kan gj√∏re det med Pandas:\n\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nfrom dapla import FileClient\n\ndef single_xml_to_dataframe(file_path: str) -&gt; pd.DataFrame:\n    fs = FileClient.get_gcs_file_system()\n\n    with fs.open(file_path, mode=\"r\") as f:\n        single_xml = f.read()\n\n    root = ET.fromstring(single_xml)\n    intern_info = root.find(\"InternInfo\")\n    kontakt = root.find(\"Kontakt\")\n    skjemadata = root.find(\"Skjemadata\")\n\n    data = []\n    all_tags = set()\n\n    for element in intern_info:\n        all_tags.add(element.tag)\n\n    for element in kontakt:\n        all_tags.add(element.tag)\n\n    for element in skjemadata:\n        all_tags.add(element.tag)\n\n    for tag in all_tags:\n        element = intern_info.find(tag)\n        if element is None:\n            element = kontakt.find(tag)\n        if element is None:\n            element = skjemadata.find(tag)\n        if element is not None:\n            value = element.text\n            data.append(value)\n        else:\n            data.append(None)\n\n    # Include the full path to the XML file in the data list\n    data.append(file_path)\n\n    # Create the DataFrame inside the function\n    df = pd.DataFrame([data], columns=list(all_tags) + [\"pathToXmlFile\"])\n    path_to_xml_file_split = df[\"pathToXmlFile\"].str.split(\"/\")\n    return df\n\n# Run the function\nfile_path = \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/24/28c34dce4232_e7bed8a4-def5-42dc-b1e2-b9fc66beb769/form_28c34dce4232.xml\"\ndf = single_xml_to_dataframe(file_path=file_path)\nI funksjonen s√• s√∏ker vi etter alle elementer under taggene intern_info, kontakt og skjemadata. Dvs. at vi lager en dataframe med en rad, der vi tar med all data fra xml-filen. Til slutt legger vi til en kolonne med filstien til filen. Det kan v√¶re nyttig senere hvis man g√• tilbake til xml-filen for √• sjekke om konverteringen til en dataframe er riktig.\nFunksjonen single_xml_to_dataframe() kan brukes til √• loope over en liste med filstier (vi viste hvordan man lager en slik liste tidligere i kapitlet). Men et typisk problem som vil oppst√• da er at xml-filene har forskjellig antall elementer i seg, som gir dataframes med ulikt antall kolonner, og derfor vil ikke Pandas vite hvordan man konkatinerer disse. For √• fikse dette m√• du modifisere funksjonen til √• ta h√∏yde for dette.\n\n\n\nHvis vi √∏nsker √• kopiere filer manuelt (dvs. ikke med Transfer Service) fra Team SUV sine b√∏tter til egen kildeb√∏tte, kan vi gj√∏re det fra Jupyter. Vi har tidligere sett hvordan vi kan kopiere enkeltfiler mellom to b√∏tter. Under viser vi hvordan man kan kopiere alle filer under en viss filsti med kommandoen fs.copy():\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\nI koden over s√• kopierer vi alle filer under from_path til to_path. Vi bruker recursive=True for √• s√∏rge for at vi kopierer alle filer under from_path.\nI eksempelet over s√• kopierer vi over alt som er from_path og dets undermapper. I tilfellet med Altinn-data s√• ligger det ogs√• pdf-filer av skjemaet som kanskje ikke √∏nsker √• kopiere. I de tilfellene kan vi f√∏rst s√∏ke etter de filene vi √∏nsker √• kopiere og legge de i en liste. Deretter kan vi kopiere over en og en fil fra listen. Denne tiln√¶rmingen er veldig nyttig hvis vi √∏nsker √• filtrere ut filer som ikke er XML-filer, eller vi √∏nsker en annen mappestruktur en den som ligger i from_path. Her er en m√•te vi kan gj√∏re det p√•:\nfrom dapla import FileClient\n\n# Lager en filsystem-instans av GCS\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Stien du √∏nsker √• kopiere til.\n# Koden under foutsetter at du har med gs:// f√∏rst\nto_folder = \"gs://ssb-prod-dapla-felles-data-delt/\"\n\n# Kopierer over filene\nfor file in xml_files:\n    from_bucket = file.split(\"/\")[0]\n    to_bucket = to_folder.split(\"/\")[2]\n    to_path = file.replace(\n        from_bucket, to_bucket\n    )\n    fs.cp(file, to_path)\nI koden over s√• bruker vi fs.glob() og ** til √• s√∏ke rekursivt etter alle xml-filer under filstien gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10. Deretter kopierer vi over filene til egen kildeb√∏tte med fs.cp(). N√•r vi skal kopiere over til en ny b√∏tte m√• vi bestemme oss for hvor filene skal ligge og hvor mye av den gamle filstien vi skal beholde. Anta at vi skal beholde hele mappestrukturen i Team SUV sin b√∏tte. Da kan vi egentlig bare erstatte ra0678-01-altinn-data-prod-e17d-ssb-altinn/ med den nye b√∏tte-navnet, og vi vil f√• den samme strukturen som i Team SUV sin b√∏tte."
  },
  {
    "objectID": "altinn3.html#forberedelse",
    "href": "altinn3.html#forberedelse",
    "title": "Altinn 3",
    "section": "",
    "text": "N√•r skjema-dataene kommer inn fra Altinn 3 til Team SUV sitt lagringsomr√•de, s√• er det en del ting som er verdt √• tenke p√•:\n\nHvordan er filene organisert hos Team SUV?\nFilstrukturen der Team SUV lagrer dataene som kommer inn fra Altinn 3, har en klar struktur. Du kan selv g√• inn √• kikke p√• dataene, enten fra Jupyter eller Google Cloud Console, og bli bedre kjent med strukturen3. Figur¬†1 viser en hvordan en typisk filsti ser ut p√• lagringsomr√•det til Team SUV. Det starter med navnet til b√∏tta som Team SUV har opprettet for skjemaet. Deretter viser den innvitteringsdato, deretter et teknisk navn, og til slutt selve skjemanavnet.\n\n\n\n\nFigur¬†1: Typisk filsti for et Altinn3-skjema.\n\n\n\nHvordan organisere dataene i din kildeb√∏tte?\nN√•r vi bruker Transfer Service til √• synkronisere innholdet i Team SUV sitt lagringsomr√•de til Dapla-teamet sitt lagringsomr√•de, s√• er det mest hensiktmessig √• fortsette √• bruke mapppe-strukturen som Team SUV har. Grunnen er at vi ikke kan legge p√• noe logikk som lager en ny struktur. Tjenesten bare kopierer over data. Men du kan kan lage et nytt toppniv√•-mappe som du √∏nsker at dataene skal synkroniseres til. F.eks. at alle Altinn-dataene legger seg inn i en mappe som heter altinn. Det er spesielt nyttig hvis du har flere datakilder som skal ligge i samme kildeb√∏tte.\nUnike skjemanavn\nSkjemanavnet du ser i Figur¬†1 er unike. Dvs. at hvis en oppgavegiver sender inn flere skjemaer p√• samme dag, s√• er fortsatt skjemanavnet unikt. Det er viktig √• v√¶re klar over n√•r man bruker automatiseringstjenesten for kildedata senere. Siden tjenesten trigges per fil, og man er avhengig av unike navn for √• ikke skrive over filer, s√• er det nyttig √• vite at man kan videref√∏re skjemanavnet i overgangen fra kildedata til inndata."
  },
  {
    "objectID": "altinn3.html#transfer-service",
    "href": "altinn3.html#transfer-service",
    "title": "Altinn 3",
    "section": "",
    "text": "N√•r vi skal overf√∏re filer fra Team SUV sin b√∏tte til v√•r kildeb√∏tte, s√• kan vi gj√∏re det manuelt fra Jupyter som forklart her.. Men det er en bedre l√∏sning √• bruke en tjeneste som gj√∏r dette for deg. Transfer Service er en tjeneste som kan brukes til √• synkronisere innholdet mellom b√∏tter p√• Dapla, samt mellom bakke og sky. N√•r du skal ta i bruk tjenesten for √• overf√∏re data mellom en b√∏tte fra Team SUV sitt prosjekt altinn-data-prod, til en kildedata-b√∏tte i Dapla-teamet ditt, s√• gj√∏r du f√∏lgende:\n\nF√∏lg denne beskrivelsen hvordan man setter opp overf√∏ringsjobber.\nEtter at du har trykket p√• Create Transfer Job velger du Google Cloud Storage p√• begge alternativene under Get Started. Deretter g√•r du videre ved √• klikke p√• Next Step.\nUnder Choose a source s√• skal du velge hvor du skal kopiere data fra. Trykk p√• Browse. I vinduet som dukker opp trykker du p√• üîª-ikonet ved siden av Project ID. I s√∏kevinduet som dukker opp s√∏ker du opp altinn-data-prod og trykker p√• navnet. Da f√•r du listet opp alle b√∏ttene i altinn-data-prod prosjektet. Til slutt trykker du p√• b√∏tta som Team SUV har opprettet for unders√∏kelsen4 og klikker Select til nederst p√• siden. Trykk deretter Next step for √• g√• videre.\nUnder Choose a destination s√• skal du velge hvor du skal kopiere data til. Dette steget er ganske likt som forrige, men du skal n√• velge ditt eget projekt og kildeb√∏tta der. Trykk p√• Browse. I vinduet som dukker opp trykker du p√• üîª-ikonet ved siden av Project ID. I s√∏kevinduet som dukker opp s√∏ker du opp prod-&lt;ditt teamnavn&gt; og trykker p√• navnet. Da f√•r du listet opp alle b√∏ttene i ditt team sitt prosjekt. Velg kildeb√∏tta som har navnet ssb-prod-&lt;teamnavn&gt;-data-kilde. Hvis du √∏nsker √• kopiere data til en undermappe i b√∏tta, s√• trykker du p√• &gt;-ikonet ved b√∏ttenavnet og velger √∏nsket undermappe5. Til slutt trykker du p√• Select til nederst p√• siden. Trykk deretter Next step for √• g√• videre.\nUnder Choose how and when to run job velger du Run with custom frequency og Starting now i Batch-modus. Hvis du √∏nsker √• overf√∏re s√• ofte som mulig, s√• velger du Custom frequency og Repeat every 1 Hours. Til slutt trykker du Next Step nederst p√• siden.\nUnder Choose Settings s√• legger du til en kort beskrivelse av jobben du har opprettet. Under de andre valgene kan du gj√∏re f√∏lgende:\n\nUnder Advanced transfer Options trenger du ikke gj√∏re noen endringer.\nUnder When to overwrite kan du velge If different.\nUnder When to delete kan du velge Never.\nUnder Manifest trenger du ikke huke av.\nUnder Logging options velger du samme som vist i Figur¬†2.\n\n\n\n\n\nFigur¬†2: Valg av opsjoner for logging i Transfer Service\n\n\nTil slutt trykker du Create for √• aktivere tjenesten. Den vil da sjekke Team SUV sin b√∏tte hver time og kopiere over alle filer som ikke allerede eksisterer statistikkteamets kildeb√∏tte."
  },
  {
    "objectID": "altinn3.html#automatiseringstjeneste-for-kildedata",
    "href": "altinn3.html#automatiseringstjeneste-for-kildedata",
    "title": "Altinn 3",
    "section": "",
    "text": "N√•r du har satt opp Transfer Service til √• kopiere over filer fra Team SUV sin b√∏tte til statistikkteamets kildeb√∏tte, s√• vil det potensielt komme inn nye skjemaer hver time. Disse kan du lese inn i Jupyter og jobbe med hvis du er kildedata-ansvarlig i teamet. Hvis du ikke er det s√• m√• du vente til dataene er tilgjengeliggjort i produkt-b√∏tta til teamet.\nSiden f√• personer innehar rollen som kildedata-ansvarlig s√• er det laget en automatiseringstjeneste som kan bearbeide alle filer som kommer inn i kildeb√∏tta. Den lar deg kj√∏re et python-script p√• alle filer som kommer inn i kildeb√∏tta.\nLes mer om hvordan du kan bruker tjenesten her."
  },
  {
    "objectID": "altinn3.html#tips-og-triks",
    "href": "altinn3.html#tips-og-triks",
    "title": "Altinn 3",
    "section": "",
    "text": "I denne delen deles noen tips og triks for √• jobbe med Altinn3-dataene p√• Dapla. Fokuset vil v√¶re p√• hvordan du kan lese inn og transformere xml-filer fra skjema-dataene.\n\n\nFor √• se innholdet i en mappe gir det mest mening √• bruke Google Cloud Console. Her kan du se b√•de filer og mapper i et pek-og-klikk grensesnitt. Men du kan ikke se p√• innholdet i filene der. Til det m√• du bruke Jupyter.\nAnta at vi √∏nsker √• liste ut alle som leverte skjema den 10. mars 2023. Da kan vi bruke gcsfs til √• gj√∏re det6:\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Print the list of parquet files\nxml_files\nHer bruker vi fs.glob()-funksjonen fra gcsfs til √• loope gjennom alle undermapper av gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/, og legge alle filer som slutter med .xml til listen xml_files. Dermed har vi et Python-objekt som kun brukes til √• hente inn de filene vi √∏nsker. Vi bruker da det vi vet om mappestrukturen i Team SUV sin b√∏tte som vi s√• tidligere i Figur¬†1.\n\n\n\nNoen ganger kan det v√¶re nyttig √• se en xml-fil direkte. Da kan vi lese den inn i Jupyter og printe den ut. Her er et eksempel p√• hvordan vi kan gj√∏re det, samtidig som vi formatterer den med xml.dom.minidom:\nfrom xml.dom.minidom import parseString\n\nfrom dapla import FileClient\n\n# Kobler oss p√• b√∏ttene\nfs = FileClient.get_gcs_file_system()\n\n# Sett inn filstien din her\nfile = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/form_dc551844cd74.xml\"\n\ndom = parseString(fs.cat_file(file))\npretty_xml = dom.toprettyxml(indent=\"  \")\nprint(pretty_xml)\nVi brukte da fs.cat_file() til √• hente inn innholdet i mappen, gjorde det om til xml-object, og formatterer outputen med xml.dom.minidom. Resultatet vil da se ut noe s√•nt som dette:\n&lt;?xml version=\"1.0\" ?&gt;\n&lt;melding xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" dataFormatProvider=\"SERES\" dataFormatId=\"7079\" dataFormatVersion=\"47315\"&gt;\n  &lt;InternInfo&gt;\n    &lt;raNummer&gt;RA-XXXX&lt;/raNummer&gt;\n    &lt;skjemaVersjon&gt;1.0&lt;/skjemaVersjon&gt;\n    &lt;undersoekelsesNr&gt;XXXXX&lt;/undersoekelsesNr&gt;\n    &lt;visOppgaveByrde&gt;0&lt;/visOppgaveByrde&gt;\n    &lt;visBrukeropplevelse&gt;0&lt;/visBrukeropplevelse&gt;\n    &lt;delregNr&gt;2XXXXX&lt;/delregNr&gt;\n    &lt;periodeFritekst&gt;3. kvartal 2022&lt;/periodeFritekst&gt;\n    &lt;periodeFomDato&gt;2022-07-01&lt;/periodeFomDato&gt;\n    &lt;periodeTomDato&gt;2022-09-30&lt;/periodeTomDato&gt;\n    &lt;periodeType&gt;KVRT&lt;/periodeType&gt;\n    &lt;periodeNummer&gt;3&lt;/periodeNummer&gt;\n    &lt;periodeAAr&gt;2022&lt;/periodeAAr&gt;\n    &lt;periodeDato&gt; &lt;/periodeDato&gt;\n    &lt;enhetsIdent&gt;XXXXXX&lt;/enhetsIdent&gt;\n    &lt;enhetsType&gt;BEDR&lt;/enhetsType&gt;\n    &lt;enhetsOrgNr&gt;XXXXXXXXX&lt;/enhetsOrgNr&gt;\n    &lt;enhetsNavn&gt;STATISTISK SENTRALBYR√Ö &lt;/enhetsNavn&gt;\n    &lt;enhetsGateadresse&gt;Akersveien 26&lt;/enhetsGateadresse&gt;\n    &lt;enhetsPostnr&gt;0177&lt;/enhetsPostnr&gt;\n    &lt;enhetsPoststed&gt;OSLO&lt;/enhetsPoststed&gt;\n    &lt;enhetsAvdeling&gt;AVD XXXX&lt;/enhetsAvdeling&gt;\n    &lt;reporteeOrgNr&gt;XXXXXXXXX&lt;/reporteeOrgNr&gt;\n  &lt;/InternInfo&gt;\n  &lt;Kontakt&gt;\n    &lt;kontaktPersonNavn&gt;OLA NORDMANN&lt;/kontaktPersonNavn&gt;\n    &lt;kontaktPersonEpost&gt;ola@tull.no&lt;/kontaktPersonEpost&gt;\n    &lt;kontaktPersonTelefon&gt; &lt;/kontaktPersonTelefon&gt;\n    &lt;kontaktInfoBekreftet&gt;1&lt;/kontaktInfoBekreftet&gt;\n    &lt;kontaktInfoKommentar&gt;Dette er en et eksempel for Dapla-manualen&lt;/kontaktInfoKommentar&gt;\n  &lt;/Kontakt&gt;\n  &lt;Skjemadata&gt;\n    &lt;ledigeStillinger&gt;75&lt;/ledigeStillinger&gt;\n    &lt;datoPrefill&gt; &lt;/datoPrefill&gt;\n  &lt;/Skjemadata&gt;\n&lt;/melding&gt;\n\n\n\nDe f√¶rreste √∏nsker √• jobbe direkte med XML-filer. Derfor er det nyttig √• kunne transformere XML-filene til et mer brukervennlig format, f.eks. en Pandas Dataframe. Her er et eksempel p√• hvordan vi kan gj√∏re det med Pandas:\n\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nfrom dapla import FileClient\n\ndef single_xml_to_dataframe(file_path: str) -&gt; pd.DataFrame:\n    fs = FileClient.get_gcs_file_system()\n\n    with fs.open(file_path, mode=\"r\") as f:\n        single_xml = f.read()\n\n    root = ET.fromstring(single_xml)\n    intern_info = root.find(\"InternInfo\")\n    kontakt = root.find(\"Kontakt\")\n    skjemadata = root.find(\"Skjemadata\")\n\n    data = []\n    all_tags = set()\n\n    for element in intern_info:\n        all_tags.add(element.tag)\n\n    for element in kontakt:\n        all_tags.add(element.tag)\n\n    for element in skjemadata:\n        all_tags.add(element.tag)\n\n    for tag in all_tags:\n        element = intern_info.find(tag)\n        if element is None:\n            element = kontakt.find(tag)\n        if element is None:\n            element = skjemadata.find(tag)\n        if element is not None:\n            value = element.text\n            data.append(value)\n        else:\n            data.append(None)\n\n    # Include the full path to the XML file in the data list\n    data.append(file_path)\n\n    # Create the DataFrame inside the function\n    df = pd.DataFrame([data], columns=list(all_tags) + [\"pathToXmlFile\"])\n    path_to_xml_file_split = df[\"pathToXmlFile\"].str.split(\"/\")\n    return df\n\n# Run the function\nfile_path = \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/24/28c34dce4232_e7bed8a4-def5-42dc-b1e2-b9fc66beb769/form_28c34dce4232.xml\"\ndf = single_xml_to_dataframe(file_path=file_path)\nI funksjonen s√• s√∏ker vi etter alle elementer under taggene intern_info, kontakt og skjemadata. Dvs. at vi lager en dataframe med en rad, der vi tar med all data fra xml-filen. Til slutt legger vi til en kolonne med filstien til filen. Det kan v√¶re nyttig senere hvis man g√• tilbake til xml-filen for √• sjekke om konverteringen til en dataframe er riktig.\nFunksjonen single_xml_to_dataframe() kan brukes til √• loope over en liste med filstier (vi viste hvordan man lager en slik liste tidligere i kapitlet). Men et typisk problem som vil oppst√• da er at xml-filene har forskjellig antall elementer i seg, som gir dataframes med ulikt antall kolonner, og derfor vil ikke Pandas vite hvordan man konkatinerer disse. For √• fikse dette m√• du modifisere funksjonen til √• ta h√∏yde for dette.\n\n\n\nHvis vi √∏nsker √• kopiere filer manuelt (dvs. ikke med Transfer Service) fra Team SUV sine b√∏tter til egen kildeb√∏tte, kan vi gj√∏re det fra Jupyter. Vi har tidligere sett hvordan vi kan kopiere enkeltfiler mellom to b√∏tter. Under viser vi hvordan man kan kopiere alle filer under en viss filsti med kommandoen fs.copy():\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\nI koden over s√• kopierer vi alle filer under from_path til to_path. Vi bruker recursive=True for √• s√∏rge for at vi kopierer alle filer under from_path.\nI eksempelet over s√• kopierer vi over alt som er from_path og dets undermapper. I tilfellet med Altinn-data s√• ligger det ogs√• pdf-filer av skjemaet som kanskje ikke √∏nsker √• kopiere. I de tilfellene kan vi f√∏rst s√∏ke etter de filene vi √∏nsker √• kopiere og legge de i en liste. Deretter kan vi kopiere over en og en fil fra listen. Denne tiln√¶rmingen er veldig nyttig hvis vi √∏nsker √• filtrere ut filer som ikke er XML-filer, eller vi √∏nsker en annen mappestruktur en den som ligger i from_path. Her er en m√•te vi kan gj√∏re det p√•:\nfrom dapla import FileClient\n\n# Lager en filsystem-instans av GCS\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Stien du √∏nsker √• kopiere til.\n# Koden under foutsetter at du har med gs:// f√∏rst\nto_folder = \"gs://ssb-prod-dapla-felles-data-delt/\"\n\n# Kopierer over filene\nfor file in xml_files:\n    from_bucket = file.split(\"/\")[0]\n    to_bucket = to_folder.split(\"/\")[2]\n    to_path = file.replace(\n        from_bucket, to_bucket\n    )\n    fs.cp(file, to_path)\nI koden over s√• bruker vi fs.glob() og ** til √• s√∏ke rekursivt etter alle xml-filer under filstien gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10. Deretter kopierer vi over filene til egen kildeb√∏tte med fs.cp(). N√•r vi skal kopiere over til en ny b√∏tte m√• vi bestemme oss for hvor filene skal ligge og hvor mye av den gamle filstien vi skal beholde. Anta at vi skal beholde hele mappestrukturen i Team SUV sin b√∏tte. Da kan vi egentlig bare erstatte ra0678-01-altinn-data-prod-e17d-ssb-altinn/ med den nye b√∏tte-navnet, og vi vil f√• den samme strukturen som i Team SUV sin b√∏tte."
  },
  {
    "objectID": "altinn3.html#footnotes",
    "href": "altinn3.html#footnotes",
    "title": "Altinn 3",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nForslag til e-post til Team SUV etter at teamet er opprettet:\nVi har opprettet et Dapla-tema som heter &lt;ditt teamnavn&gt; for √• jobbe med skjema &lt;RA-XXXX&gt;. Kan dere gi oss tilgang til riktig lagringsomr√•de og ogs√• gi v√•r Transfer Service lesetilgang.‚Ü©Ô∏é\nEn typisk prosessering som de fleste vil √∏nske √• gj√∏re er √• konvertere fra xml-formatet det kom p√•, og over til parquet-formatet.‚Ü©Ô∏é\nDu kan g√• inn i Google Cloud Console og s√∏ke opp prosjektet til Team SUV som de bruker for √• dele data. Det heter altinn-data-prod, og du finner b√∏ttene ved √• klikke deg inn p√• Cloud Storage‚Ü©Ô∏é\nB√∏ttenavnet starter alltid med RA-nummeret til unders√∏kelsen.‚Ü©Ô∏é\nAlternativt oppretter du en mappe direkte vinduet ved √• trykke p√• mappe-ikonet med en +-tegn i seg.‚Ü©Ô∏é\nFor √• jobbe mot datat i GCS som i et ‚Äúvanlig‚Äù filsysten kan vi bruke FileClient.get_gcs_file_system() fra dapla-toolbelt.‚Ü©Ô∏é"
  },
  {
    "objectID": "opprette-dapla-team.html",
    "href": "opprette-dapla-team.html",
    "title": "Opprette Dapla-team",
    "section": "",
    "text": "Opprette Dapla-team\nFor √• komme i gang med √• opprette et Dapla-team trengs det en oversikt over teamets medlemmer og hvilke tilgangsgrupper medlemmene skal v√¶re med i. Det trengs ogs√• informasjon om hvilke Dapla-tjenester som er aktuelle for teamet √• ta i bruk. Derfor har det blitt opprettet en egen veileder for dette kalt Dapla Start.\n\n\n\n\n\n\nG√• til Dapla Start for starte bestilling av et nytt Dapla-team.\n\n\n\nN√•r teamet er opprettet f√•r alle medlemmene tilgang til sitt eget prosjekt i Google Cloud Platform (GCP), som er SSBs leverand√∏r av skytjenester. Videre f√•r hvert prosjekt et sett med tjenester og tilganger som knyttes til teamet. Det opprettes ogs√• datalagringsomr√•der (kalt b√∏tter) som bare kan aksesseres av brukere som er med i teamets tilgangsgrupper.\nDapla-teamet vil ogs√• f√• sin egen gruppe i SSBs Active Directory slik at medlemskapet i gruppen kan administreres av Kundeservice."
  },
  {
    "objectID": "github-app-integrasjon.html",
    "href": "github-app-integrasjon.html",
    "title": "Koble prosjektet til Github",
    "section": "",
    "text": "For at automatiseringsl√∏sningen p√• Dapla skal kunne settes opp automatisk m√• denne ha tilgang til √• lese fra prosjektets IAC-repo1. Dette avsnittet vil beskrive denne prosessen. Merk at dette er en engangsjobb som m√• gj√∏res av prosjektets kildedataansvarlige.\n\n\n\n\n\n\nViktig: Prosjektets kildedataansvarlige ogs√• m√• ha administrator-rettigheter til IAC-repoet i Github.\n\n\n\n\nLogg inn p√• Google Cloud Console og velg det prosjektet som skal konfigureres √∏verst venstre hj√∏rte. S√∏k opp Cloud Build i s√∏kefeltet og trykk p√• det valget som kommer opp.\nDet skal n√• v√¶re en venstremeny tilgjengelig med tittel Cloud Build. Trykk p√• menyvalget som heter Triggers (Figur¬†1)\n\n\n\n\nFigur¬†1: Bilde av venstremeny\n\n\n\nI nedtrekkslisten Region s√∏rg for at europe-north1 er valgt (Figur¬†2)\n\n\n\n\nFigur¬†2: Velg korrekt region\n\n\n\nTrykk deretter p√• en link som heter CONNECT REPOSITORY ca. midt p√• siden.\n\n\n\n\nFigur¬†3: Oversikt over triggers\n\n\n\nN√• vil det dukke opp et vindu p√• h√∏yre side med overskrift Connect repository (Figur¬†4). Velg GitHub (Cloud Build GitHub App) og trykk p√• CONTINUE\n\n\n\n\nFigur¬†4: Vindu for √• velge Cloud Build Github App\n\n\n\nEt pop-up vindu tilsvarende Figur¬†5 vil komme opp. Trykk p√• Authorize. Vinduet vil etter hvert lukke seg og man kommer videre til et steg som heter Select repository (Figur¬†6)\n\n\n\n\nFigur¬†5: Pop-up vindu for Github\n\n\n\n\n\nFigur¬†6: Valg av Github repository\n\n\n\nTrykk p√• nedtrekkslisten Repository og skriv inn teamets navn. Huk av boksen ved teamets IAC-repo og trykk OK.\n\n\n\n\nFigur¬†7: Gi Google Build tilgang til Github repository\n\n\n\nKryss s√• av i sjekkboksen som i (Figur¬†8) og trykk CONNECT.\n\n\n\n\nFigur¬†8: Bekreft nytt Github repository\n\n\n\nTil slutt vil skjermbildet se ut som vist i Figur¬†9. Det siste steget Create a trigger kan du hoppe over. Dette vil bli satt opp av automatiseringsl√∏sningen senere. Trykk p√• knappen DONE\n\n\n\n\nFigur¬†9: Siste steg - Create a trigger"
  },
  {
    "objectID": "github-app-integrasjon.html#footnotes",
    "href": "github-app-integrasjon.html#footnotes",
    "title": "Koble prosjektet til Github",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nIAC-repo er et en kodebase i Github p√• formen https://github.com/statisticsnorway/team-navn-iac.‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Velkommen",
    "section": "",
    "text": "Denne boken er ment som enkel-√•-bruke manual for √• ta i bruk SSBs nye dataplattform Dapla. Plattformen fors√∏ker √• gj√∏re statistikkprodusenter og forskere s√• selvhjulpne som mulig. M√•lsetningen er at tjenestene som tilbys skal kunne tas i bruk p√• en enkel og intuitiv m√•te. Men uansett hvor lett tilgjengelig tjenester er, og hvor mye arbeid som er lagt √• gj√∏re l√∏sninger brukervennlige, s√• trenger de fleste i SSB en klar og tydelig veiledning for hvordan de skal brukes og i hvilken st√∏rre sammenheng tjenestene inng√•r. Dapla-manualen er ment √• v√¶re en s√•nn st√∏tte i statistikkernes hverdag. Uansett om man lurer p√• hvordan man logger seg inn p√• plattformen, eller om man √∏nsker informasjon om kj√∏remilj√∏et for mer kompliserte maskinl√¶ringsmodeller, s√• skal man finne veiledning i denne manualen. M√•lgruppen er b√•de nybegynneren og den mer erfarne.\n\n\n\n\n\n\nAlle ansatte i SSB kan bidra til manualen ved √• f√∏lge instruksjonene her.\n\n\n\n\n\nDapla-manualen er initiert og skrevet av Team Statistikktjenester i SSB. Bidragsytere er √òyvind Bruer-Skarsb√∏, Miles Winther, Bj√∏rn Andre Skaar, Anders Lunde og Damir Medakovic. Ved behov for oppdateringer og nytt innhold h√•per vi at alle i SSB kan bidra.\n\n\n\nLes mer om hvordan du kan bidra til manualen her."
  },
  {
    "objectID": "index.html#bidragsytere",
    "href": "index.html#bidragsytere",
    "title": "Velkommen",
    "section": "",
    "text": "Dapla-manualen er initiert og skrevet av Team Statistikktjenester i SSB. Bidragsytere er √òyvind Bruer-Skarsb√∏, Miles Winther, Bj√∏rn Andre Skaar, Anders Lunde og Damir Medakovic. Ved behov for oppdateringer og nytt innhold h√•per vi at alle i SSB kan bidra."
  },
  {
    "objectID": "index.html#bidra-til-dapla-manualen",
    "href": "index.html#bidra-til-dapla-manualen",
    "title": "Velkommen",
    "section": "",
    "text": "Les mer om hvordan du kan bidra til manualen her."
  },
  {
    "objectID": "jobbe-med-kode.html",
    "href": "jobbe-med-kode.html",
    "title": "Jobbe med kode",
    "section": "",
    "text": "P√• Dapla jobber vi med utvikling av Python- og R-kode i et Jupyter-milj√∏. For de som √∏nsker det, er det mulig √• enkelt √•pne en notebook med en av v√•re forh√•ndskonfigurerte kernels1. Man kan umiddelbart begynne √• skrive kode og deretter lagre den i det lokale filsystemet. Dette er ideelt for enkel datautforskning eller for pedagogiske form√•l.\nN√•r koden skal settes i produksjon, er det essensielt √• ta hensyn til f√∏lgende:\n\nResultater b√∏r v√¶re reproduserbare.\nKoden m√• kunne deles med andre.\nKoden b√∏r v√¶re organisert slik at den er gjenkjennelig for kollegaer.\n\nFor √• lette etterlevelsen av beste praksis for kodeutvikling p√• Dapla, har vi utviklet et verkt√∏y kalt ssb-project. Dette er et CLI-verkt√∏y2 som enkelt lar deg opprette et prosjekt med en standard mappestruktur, et virtuelt milj√∏ og integrasjon med Git for versjonsh√•ndtering. Som en bonus kan det ogs√• opprette et GitHub-repositorium for deg ved behov.\nI dette kapitlet vil vi veilede deg gjennom bruken av ssb-project. Du vil l√¶re √• opprette et nytt prosjekt, installere pakker, h√•ndtere versjoner med Git, bygge et eksisterende prosjekt og vedlikeholde prosjektet over tid.\n\n\n\n\n\n\nSSB-project st√∏tter ikke R enda\n\n\n\nPer n√• st√∏tter SSB-project kun prosjekter skrevet i Python. Dette skyldes begrensninger ved det popul√¶re virtuelle milj√∏-verkt√∏yet for R, renv. Mens renv effektivt h√•ndterer versjoner av R-pakker, har det ikke kapasitet til √• ta vare p√• spesifikke R-installasjonsversjoner. Dette kan potensielt gj√∏re det mer utfordrende √• reprodusere tidligere publiserte resultater ved bruk av ssb-project. Vi arbeider mot en l√∏sning for √• inkludere st√∏tte for R i fremtiden."
  },
  {
    "objectID": "jobbe-med-kode.html#forberedelser",
    "href": "jobbe-med-kode.html#forberedelser",
    "title": "Jobbe med kode",
    "section": "Forberedelser",
    "text": "Forberedelser\nF√∏r du kan ta i bruk ssb-project s√• er det et par ting som m√• v√¶re p√• plass:\n\nDu m√• ha konfigurert Git etter SSB sin standard (les mer om hvordan her).\nHvis du √∏nsker at ssb-project ogs√• skal opprette et GitHub-repo for deg m√• du ogs√• f√∏lgende v√¶re p√• plass:\n\nDu m√• ha en GitHub-bruker (les hvordan her)\nSkru p√• 2-faktor autentifisering for GitHub-brukeren din (les hvordan her)\nV√¶re koblet mot SSBs organisasjon statisticsnorway p√• GitHub (les hvordan her)\nOpprette Personal Access Token (PAT) og godkjenne det for bruk mot statisticsnorway (les hvordan her)\n\n\nDet er ogs√• √• anbefale at du lagrer PAT lokalt slik at du ikke trenger √• forholde deg til det n√•r jobber med Git og GitHub. Hvis du har alt dette p√• plass s√• kan du bare fortsette √• f√∏lge de neste kapitlene."
  },
  {
    "objectID": "jobbe-med-kode.html#opprett-ssb-project",
    "href": "jobbe-med-kode.html#opprett-ssb-project",
    "title": "Jobbe med kode",
    "section": "Opprett ssb-project",
    "text": "Opprett ssb-project\n\n\n\n\n\n\nHar du Github bruker? Noe funksjonalitet i ssb-project krever det. Finn ut hvordan ved √• lese forrige kapittel.\n\n\n\nssb-project lar deg opprette en prosjekt-mappe med og uten GitHub-repo. La oss ta for oss hver av alternativene.\n\nUten GitHub-repo\nFor √• opprette et nytt ssb-project uten GitHub-repo gj√∏r du f√∏lgende:\n\n√Öpne en terminal. De fleste vil gj√∏re dette i Jupyterlab p√• bakke eller sky og da kan de bare trykke p√• det bl√• ‚ûï-tegnet i Jupyterlab og velge Terminal.\nF√∏r vi kj√∏rer programmet m√• vi v√¶re obs p√• at ssb-project vil opprette en ny mappe der vi st√•r. G√• derfor til den mappen du √∏nsker √• ha den nye prosjektmappen. For √• opprette et prosjekt som heter stat-testprod s√• skriver du f√∏lgende i terminalen:\n\nssb-project create stat-testprod\n\n\nHvis du stod i hjemmemappen din p√• n√•r du skrev inn kommandoen over i terminalen, s√• har du f√•tt mappestrukturen som vises i Figur¬†1. 3. Den inneholder f√∏lgende :\n\n.git-mappe som blir opprettet for √• versjonsh√•ndtere med Git.\nsrc-mappe som skal inneholde all koden som utgj√∏r produksjonsl√∏pet.\ntests-mappe som inneholder tester du skriver for koden din.\nLICENCE-fil som skal benyttes for public-repos i SSB.\npoetry.lock-fil som inneholder alle versjoner av Python-pakker som blir brukt.\npyproject.toml-fil som inneholder informasjon om prosjektet og hvilke pakker som blir brukt.\nREADME.md-fil som brukes for tekstlig innhold p√• GitHub-siden for prosjektet.\n\n\n\n\n\n\nFigur¬†1: Mappen som ble opprettet av ssb-project.\n\n\n\n\n\n\nMed Github-repo\nOver s√• opprettet vi et ssb-project uten √• opprette et GitHub-repo. Hvis du √∏nsker √• opprette et GitHub-repo ogs√• m√• du endre kommandoen over til:\nssb-project create stat-testprod --github --github-token='blablabla'\nKommandoen over oppretter en mappestruktur slik vi s√• tidligere, men ogs√• et ssb-project som heter stat-testprod med et GitHub-repo med samme navn. Som du ser s√• m√• vi da sende med opsjonen --github og PAT med opsjonen --github-token='blablabla'. Repoet i GitHub ser da ut som i Figur¬†2. Hvis du √∏nsker √• slippe m√•tte forholde deg til PAT hver gang interagerer med GitHub, kan du f√∏lge denne beskrivelsen for √• lagre den lokalt. Da kan droppe --github-token='blablabla' fra kommandoen over.\n\n\n\nFigur¬†2: GitHub-repo som er opprettet av ssb-project\n\n\n\n\n\n\n\n\nN√•r du har opprettet et nytt ssb-project, eller bygget et eksisterende prosjekt, s√• kan det ta rundt 30 sekunder f√∏r kernelen viser seg i Jupterlab-launcher. V√¶r t√•lmodig!"
  },
  {
    "objectID": "jobbe-med-kode.html#installere-pakker",
    "href": "jobbe-med-kode.html#installere-pakker",
    "title": "Jobbe med kode",
    "section": "Installere pakker",
    "text": "Installere pakker\nN√•r du har opprettet et ssb-project s√• kan du installere de python-pakkene du trenger fra PyPI. Men f√∏r du installerer en pakke b√∏r gj√∏re f√∏lgende for √• sikre deg at du ikke installerer en pakke med skadelig kode:\n\nS√∏k opp pakken p√• PyPI.\nSjekk om pakken er et popul√¶rt/velkjent prosjekt ved √• bes√∏ke repoet der koden ligger. Antall Stars og Forks p√• gitHub er en grei indikasjon p√• dette.\nHvis du er i tvil om pakken er trygg √• installere, s√• kan du sp√∏rre kollegaer om de har erfaring med den, eller sp√∏rre p√• en egnet Yammer-kanal i SSB.\nHvis du fortsatt √∏nsker √• installere pakken s√• anbefaler vi √• copy-paste navnet fra PyPi, ikke skrive det inn manuelt n√•r du installerer.\n\nSelve installeringen av pakken gj√∏res enkelt p√• f√∏lgende m√•te:\n\n√Öpne en terminal i Jupyterlab.\nG√• inn i prosjektmappen din ved √• skrive:\n\ncd &lt;sti til prosjektmappe&gt;\n\nLag en branch/utviklingsbranch som f.eks. heter install-pandas:\n\ngit checkout -b install-pandas\n\nInstaller, f.eks. Pandas, ved √• skrive f√∏lgende:\n\npoetry add pandas\n\n\n\nFigur¬†3: Installasjon av Pandas med ssb-project\n\n\nFigur¬†3 viser hvordan dette vil se ut i en Jupyterlab-terminal. Kommandoen for √• installere noe er poetry add etterfulgt av pakkenavnet. Vi ser ogs√• at den automatisk legger til Pandas-versjonen i filen poetry.lock.\nDu kan ogs√• spesifisere en konkret versjon av pakken som skal installeres med f√∏lgende kommando:\npoetry add pandas@1.2.3\n\nAvinstallere pakker\nOfte eksperimenterer man med nye pakker, og alle blir ikke med videre i produksjonskoden. Det er god praksis √• fjerne pakker som ikke brukes, blant annet for √• unng√• at de blir en sikkerhetsrisiko. Det gj√∏r du enkelt ved √• skrive f√∏lgende i terminalen:\npoetry remove pandas\n\n\nOppdatere pakker\nHvis det kommer en ny versjon av en pakke du bruker, s√• kan du oppdatere den med f√∏lgende kommando:\npoetry update pandas\nhvis du kj√∏rer poetry update uten noe pakkenavn, s√• vil alle pakkene dine oppdateres til siste versjon, med mindre du har spesifisert versjonsbegrensninger i pyproject.toml-filen.\n\n\nUnders√∏k avhengigheter\nHvis du lurer p√• hvilke pakker som har hvilke avhengigheter, s√• kan du lett liste ut dette i terminalen med f√∏lgende kommando:\npoetry show --tree\nDet vil gi en grafisk fremstilling av avhengighetene som vist i Figur¬†4.\n\n\n\nFigur¬†4: Visning av pakke-avhengigheter i ssb-project"
  },
  {
    "objectID": "jobbe-med-kode.html#push-til-github",
    "href": "jobbe-med-kode.html#push-til-github",
    "title": "Jobbe med kode",
    "section": "Push til GitHub",
    "text": "Push til GitHub\nN√•r du n√• har installert en pakke s√• har filen poetry.lock endret seg. For at dine samarbeidspartnere skal f√• tilgang til denne endringen i et SSB-project, s√• m√• du pushe en ny versjon av poetry.lock-filen opp Github, og kollegaene m√• pulle ned og bygge prosjektet p√• nytt. Du kan gj√∏re dette p√• f√∏lgende m√•te etter at du har installert en ny pakke:\n\nVi kan stage alle endringer med f√∏lgende kommando i terminalen n√•r vi st√•r i prosjektmappen:\n\ngit add -A\n\nDeretter commite en endring, dvs. ta et stillbilde av koden i dette √∏yeblikket, ved √• skrive f√∏lgende:\n\ngit commit -m \"Installert pandas\"\n\nPush det opp til GitHub4. Anta at vi gjorde dette i branchen install-pandas som ble opprettet tidligere. Da kan vi skrive f√∏lgende :\n\ngit push --set-upstream origin install-pandas\nDeretter kan kollegaene dine pulle ned endringene og bygge prosjektet p√• nytt. Vi forklarer hvordan man kan bygge prosjektet p√• nytt senere i kapitlet."
  },
  {
    "objectID": "jobbe-med-kode.html#dependabot",
    "href": "jobbe-med-kode.html#dependabot",
    "title": "Jobbe med kode",
    "section": "Dependabot",
    "text": "Dependabot\nN√•r man installerer pakker s√• vil det etter hvert utvikle seg et sett av potensielt kompliserte avhengigheter mellom disse pakkene. Dette skyldes at en pakke kan benytte seg av funksjonalitet i andre pakker, som igjen benytter seg av funksjonalitet i andre pakker, osv.. Hvis noen finner en sikkerhetss√•rbarhet i en pakke s√• kan det fikses ved at en ny versjon av den pakken slippes, som igjen kan f√• konsekvenser for pakker som er avhengig av denne.\nI SSB er det tilrettelagt for at alle som versjonsh√•ndterer koden sin p√• GitHub kan skanne pakkene sine for s√•rbarheter og nye versjoner av pakker med Dependabot. Dependabot hjelper oss med √• finne og fikse s√•rbarheter og gamle pakkeversjoner. Dette er spesielt viktig n√•r man installerer sine egne pakker.\nDependabot sjekker med jevne mellomrom om det finnes oppdateringer i pakkene som er listet i din pyproject.toml-fil, og den tilh√∏rende poetry.lock. Hvis det finnes oppdateringer s√• vil den lage en pull request som du kan godkjenne. N√•r du godkjenner den s√• vil den oppdatere poetry.lock-filen og lage en ny commit som du kan pushe til GitHub. Dependabot gir ogs√• en sikkerhetsvarslinger hvis det finnes kjente s√•rbarheter i pakkene du bruker.\nDet er anbefalt at alle som installerer sine egne pakker i SSB skrur p√• Dependabot i sine GitHub-repoer.\n\nAktivere Dependabot\nDu kan aktivere Dependabot ved √• gi inn i GitHub-repoet ditt og gj√∏re f√∏lgende:\n\nG√• inn repoet\nTrykk p√• Settings for det repoet som vist p√• Figur¬†5.\n\n\n\n\nFigur¬†5: √Öpne Settings for et GitHub-repo.\n\n\n\nI menyen til venstre velger du Code security and analysis\nUnder seksjonen Dependabot velger Enable p√• minst Dependabot alerts og Dependabot security updates, slik som vist i Figur¬†6.\n\n\n\n\nFigur¬†6: Skru p√• Dependabot i GitHub.\n\n\nN√•r du har gjort dette vil GitHub varsle deg hvis det finnes en kjent s√•rbarhet i pakkene som benyttes.\n\n\nOppdatere pakker\nHvis en av pakkene du bruker kommer med en oppdatering, s√• vil Dependabot lage en pull request (PR) i GitHub som du kan godkjenne. Dependabot sjekker ogs√• om oppdateringen er i konflikt med andre pakker du bruker. Hvis det er tilfellet s√• vil den lage en pull request som oppdaterer alle pakkene som er i konflikt.\nHvis en av pakkene du bruker har en kjent sikkerhetss√•rbarhet, s√• vil Dependabot varsle deg om dette under Security-fanen i GitHub-repoet ditt. Hvis du trykker p√• View Dependabot alerts s√• vil du f√• en oversikt over alle s√•rbarhetene som er funnet, og hvilken alvorlighetsgrad den har. Hvis du trykker p√• en av s√•rbarhetene s√• vil du f√• mer informasjon om den, og du kan trykke p√• Create pull request for √• oppdatere pakken.\nSom nevnt over kan du enkelt oppdatere pakker fra GitHub ved hjelp av Dependabot. Men det finnes tilfeller der du vil teste om en oppdatering gj√∏r at deler av koden din ikke fungerer lenger. Anta at du bruker en Python-pakken Pandas i koden din, og at du f√•r en pull request fra Dependabot om √• oppdatere den fra versjon 1.5 til 2.0. Hvis du √∏nsker √• teste om koden din fortsatt fungerer med den nye versjonen av Pandas, s√• kan du gj√∏re dette i Jupyterlab ved √• f√∏lge ved √• lage en branch som f.eks. heter update-pandas. Deretter kan du installere den nye versjonen av Pandas med f√∏lgende kommando fra terminalen:\npoetry update pandas@2.0\nHvis du n√• kj√∏rer koden din kan du teste om den fortsatt fungerer som forventet. Gj√∏r den ikke det kan du tilpasse koden din og pushe endringene til Github. Deretter kan du godkjenne pull requesten fra Dependabot og merge den. Etter dette kan du slette den PR-en som Dependabot lagde for deg."
  },
  {
    "objectID": "jobbe-med-kode.html#bygg-eksisterende-ssb-project",
    "href": "jobbe-med-kode.html#bygg-eksisterende-ssb-project",
    "title": "Jobbe med kode",
    "section": "Bygg eksisterende ssb-project",
    "text": "Bygg eksisterende ssb-project\nN√•r vi skal samarbeide med andre om kode s√• gj√∏r vi dette via GitHub. N√•r du pusher koden din til GitHub, s√• kan samarbeidspartnere pulle ned koden og jobbe videre med den. Men n√•r de henter ned koden s√• vil de bare hente ned selve koden, ikke pakker og Python-versjonen som ble brukt. De m√• installere alt som du hadde installert. I tillegg trenger de en kernel hvis de skal jobbe i Jupyterlab. ssb-project gj√∏r det sv√¶rt enkelt √• bygge opp det du trenger, siden det virtuelle milj√∏et har all informasjon om hva som trengs.\nFor at samarbeidspartneren din skal kunne bygge milj√∏et p√• nytt, m√• de ha gjort en minimal konfigurering av Git. Les mer om hvordan du frem for √• gj√∏re dette her.\nFor √• bygge opp et eksisterende milj√∏ gj√∏r du f√∏lgende:\n\nF√∏rst m√• du kopiere prosjektet ned lokalt, eller klone repoet med git-terminologi\n\ngit clone https://github.com/statisticsnorway/&lt;prosjektnavn&gt;\n\nG√• inn i mappen du klonet\n\ncd &lt;prosjektnavn&gt;\n\nSkape et virtuelt milj√∏ og installere en tilsvarende Jupyter kernel med\n\nssb-project build"
  },
  {
    "objectID": "jobbe-med-kode.html#slette-ssb-project",
    "href": "jobbe-med-kode.html#slette-ssb-project",
    "title": "Jobbe med kode",
    "section": "Slette ssb-project",
    "text": "Slette ssb-project\nDet vil v√¶re tilfeller hvor man √∏nsker √• slette et ssb-project, enten fordi man ikke trenger koden lenger eller fordi man bare testet litt.\n\nLokalt\nHvis man jobber med flere prosjekter s√• kan det fort bli mange Jupyter kerneler hengende igjen. Derfor er det ogs√• mulighet √• kj√∏re\nssb-project clean stat-testprod\nsom sletter Jupyter-kernelen og de installerte pakkene i prosjektet. Hvis du ogs√• √∏nsker √• slette selve mappen med kode m√• du gj√∏re det manuelt5:\nrm -rf ~/stat-testprod/\nProsjektmappen over l√• direkte i hjemmemappen min og hjemmemappen p√• Linux kan alltid referes til med et tilda-tegn ~.\n\n\nArkiver GitHub-repo\nGitHub-repoer som er opprettet under SSB sin organinasjons statisticsnorway p√• GitHub kan ikke slettes, bare arkiveres. Grunnen er at hvis man oppdager en s√•rbarhet senere s√• er det viktig √• kunne se repoet for √• forst√• hva som har skjedd.\nHvis du ikke trenger et GitHub-repo lenger kan man arkivere repoet. Det gj√∏r du p√• f√∏lgende m√•te:\n\nGi inn i repoet Settings slik som vist med r√∏d pil i Figur¬†7.\n\n\n\n\nFigur¬†7: Settings for repoet.\n\n\n\nUnder General scroller du deg ned til delen som heter Danger Zone og velger Archive this repository, slik som vist p√• Figur¬†8.\n\n\n\n\nFigur¬†8: Arkivering av et repo.\n\n\n\nI dialogboksen som dukker opp fyller du inn reponavnet som beskrevet og trykker p√• I understand the consequences, archive this repository.\n\nN√•r det er gjort s√• er repoet lesbart, men man kan ikke jobbe med det. Men som vi ser av @#fig-github-repo-settings-archive-warning kan man omgj√∏re arkiveringen senere hvis det skulle v√¶re √∏nskelig."
  },
  {
    "objectID": "jobbe-med-kode.html#spark-i-ssb-project",
    "href": "jobbe-med-kode.html#spark-i-ssb-project",
    "title": "Jobbe med kode",
    "section": "Spark i ssb-project",
    "text": "Spark i ssb-project\nFor √• kunne bruke Spark i et ssb-project m√• man f√∏rst installere pyspark. Det gj√∏r du ved √• skrive f√∏lgende i en terminal:\npoetry add pyspark==$(pip show pyspark | grep Version | egrep -o \"([0-9]{1,}\\.)+[0-9]{1,}\") --no-dev\nHer installerer du samme versjon av pyspark som p√• Jupyterlab.\nVidere kan vi konfigurere Spark til √• enten kj√∏re p√• lokal maskin eller p√• flere maskiner (s√•kalte clusters). Under beskriver vi begge variantene.\n\nLokal maskin\nOppsettet for Pyspark p√• lokal maskin er det enkleste √• sette opp siden Pyspark vil ha direkte tilgang til det lokale filsystemet. Man kan bruke milj√∏variabelen PYSPARK_PYTHON til √• peke p√• det virtuelle milj√∏et, og dermed vil Pyspark ogs√• ha tilgang til alle pakkene som er installert der. I en notebook vil dette kunne settes opp slik:\nimport os\nimport subprocess\n\n# Finner filstien til det virtuelle milj√∏et\npython_path = subprocess.run(['poetry', 'run', 'which', 'python'],\n                             capture_output=True, text=True).stdout.rstrip('\\n')\n\nos.environ[\"PYSPARK_PYTHON\"] = python_path\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = os.environ[\"PYSPARK_LOCAL_SUBMIT_ARGS\"]\nN√•r du oppretter en Notebook og bruker den kernelen du har laget s√• m√• du alltid ha denne p√• toppen av notebooken. Det initialiserer Spark slik at det kan brukes i notebooken.\n%run /usr/local/share/jupyter/kernels/pyspark_local/init.py\nDette scriptet vil sette et spark objekt som brukes for √• kalle API‚Äôet til pyspark.\n\n\nCluster\nHvis man vil kj√∏re Pyspark i et cluster (dvs. p√• flere maskiner) s√• vil databehandlingen foreg√• p√• andre maskiner som ikke har tilgang til det lokale filsystemet. Man m√• dermed lage en ‚Äúpakke‚Äù av det virtuelle milj√∏et p√• lokal maskin og tilgjengeliggj√∏re dette for alle maskinene i clusteret. For √• lage en slik ‚Äúpakke‚Äù kan man bruke et bibliotek som heter venv-pack. Dette kan kj√∏res fra et terminalvindu slik:\nvenv-pack -p .venv -o pyspark_venv.tar.gz\nMerk at kommandoen over m√• kj√∏res fra rot-mappen i prosjektet ditt. Her er pyspark_venv.tar.gz et tilfeldig valgt filnavn, men dette filnavnet skal brukes videre i notebooken.\nimport os\nimport subprocess\n\n# Milj√∏variabel som peker p√• en utpakket versjon av det virtuelle milj√∏et\nos.environ[\"PYSPARK_PYTHON\"] = \"./environment/bin/python\"\n\n# Legg til et flagg, --archives, som peker p√• \"pakken\" med det virtuelle milj√∏et\nconf = os.environ[\"PYSPARK_K8S_SUBMIT_ARGS\"].split(' ')\nlast_index = conf.index('pyspark-shell')\nconf[last_index:last_index] = ['--archives', 'pyspark_venv.tar.gz#environment']\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = ' '.join(conf)\nN√•r du oppretter en Notebook og bruker den kernelen du har laget s√• m√• du alltid ha denne p√• toppen av notebooken. Det initialiserer Spark slik at det kan brukes i notebooken.\n%run /usr/local/share/jupyter/kernels/pyspark_k8s/init.py\nDette scriptet vil sette et spark objekt som brukes for √• kalle API‚Äôet til pyspark."
  },
  {
    "objectID": "jobbe-med-kode.html#tips-og-triks",
    "href": "jobbe-med-kode.html#tips-og-triks",
    "title": "Jobbe med kode",
    "section": "Tips og triks",
    "text": "Tips og triks\nI denne delen av kapitlet vil vi gi deg noen tips og triks som kan v√¶re nyttige n√•r du jobber med ssb-project.\n\nPoetry\nssb-project bruker Poetry for √• h√•ndtere virtuelle milj√∏er. Poetry er et verkt√∏y som gj√∏r det enkelt √• installere pakker og h√•ndtere versjoner av disse. Det er ogs√• Poetry som h√•ndterer Jupyter-kernelen for deg.\nHvis du etterlyser funksjonalitet i et ssb-project s√• kan det v√¶re nyttig √• lese dokumentasjonen til Poetry for √• se om det er mulig √• f√• til det du √∏nsker.\n\n\nFull disk p√• Dapla\nDet ‚Äúlokale‚Äù filsystemet p√• Dapla har kun 10GB diskplass. Har du mange virtuelle milj√∏er p√• denne disken kan det fort bli fullt, siden alle pakker blir installert her. Vanligvis er det 2 grunner til at disken blir full:\n\nFor mange virtuelle milj√∏er (ssb-projects) lagret lokalt.\nDette vil ofte kunne l√∏ses ved √• slette virtuelle milj√∏er som ikke lenger er i bruk. Hvis du har 5 virtuelle milj√∏er som hver bruker 1GB, og du kun jobber p√• en av de n√•, s√• vil du frigj√∏re 40% av disken ved √• slette 4 av dem. Husk at det permanente lagringsstedet for kode er p√• GitHub, og du kan alltid klone ned et prosjekt senere og bygge det hvis det trengs.\n/home/jovyan/.cache/ har blitt for stort.\nDette er en mappe som brukes av applikasjoner til √• lagre midlertidig data slik at de kan kj√∏re raskere. Denne kan bli ganske stor etter hvert. Ofte kan man frigj√∏re flere GB ved √• slette denne. Du sletter denne mappen ved √• skrive f√∏lgende i en terminal:\n\nrm -rf /home/jovyan/.cache/\nHvis du opplever at disken er full, s√• kan det anbefales √• unders√∏ke hvilke mapper som tar st√∏rst plass med f√∏lgende kommando i terminalen:\ncd ~ && du -h --max-depth=5 | sort -rh | head -n 10 && cd -\nKommandoen over sjekker, fra hjemmemappen din, hvilke mapper og undermapper som tar mest plass. Den viser de 10 st√∏rste mappene. Hvis du √∏nsker √• se flere mapper s√• kan du endre tallet etter head -n. Hvis du √∏nsker √• se alle mapper s√• kan du fjerne head -n. --max-depth=5 betyr at den kun sjekker mapper som er 5 mapper dype fra hjemmemappen din.\nN√•r du har gjort det kan selv vurdere hvilke som kan slettes for √• frigj√∏re plass.\n\n\nHold prosjektet oppdatert\nHvis du sitter med en lokal kopi av prosjektet ditt, og flere andre jobber med den samme kodebasen, s√• er det viktig at du holder din lokale kopi oppdatert. Hvis du jobber i en branch p√• en lokal kopi, b√∏r du holde denne oppdatert med main-branchen p√• GitHub. Det er vanlig Git-praksis. N√•r man ogs√• bruker ssb-project, s√• man huske √• ogs√• bygge prosjektet p√• nytt hver gang det er endringer som er gjort av andre i poetry.lock.-filen."
  },
  {
    "objectID": "jobbe-med-kode.html#footnotes",
    "href": "jobbe-med-kode.html#footnotes",
    "title": "Jobbe med kode",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nEn kernel refererer til en Python- eller R-installasjon som er optimalisert for bruk med Jupyterlab Notebooks.‚Ü©Ô∏é\nCLI = Command-Line-Interface, som betyr et program designet for bruk i terminalen med kommandoer.‚Ü©Ô∏é\nFiler og mapper som starter med punktum er skjulte med mindre man ber om √• se dem. I Jupyterlab kan disse vises i filutforskeren ved √• velge View fra menylinjen, og deretter velge Show hidden files. I en terminal skriver man ls -a for √• se de.‚Ü©Ô∏é\n√Ö pushe til GitHub uten √• sende ved Personal Access Token fordrer at du har lagret det lokalt s√• Git kan finne det. Her et eksempel p√• hvordan det kan gj√∏res.‚Ü©Ô∏é\nDette kan ogs√• gj√∏res ved √• h√∏yreklikke p√• mappen i Jupyterlab sin filutforsker og velge Delete.‚Ü©Ô∏é"
  },
  {
    "objectID": "produksjonsl√∏p.html",
    "href": "produksjonsl√∏p.html",
    "title": "Produksjonsl√∏p",
    "section": "",
    "text": "Produksjonsl√∏p\nI forrige del beskrev vi noen grunnleggende ting rundt Dapla. I denne delen tar vi for oss hvordan du kan begynne √• jobbe med skarpe data p√• plattformen.\nKapittelet som beskriver hvordan man logger seg inn p√• Dapla vil fungere uten at du m√• gj√∏re noen forberedelser. Er man koblet p√• SSB sitt nettverk s√• vil alle SSB-ansatte kunne g√• inn p√• plattformen og kode i Python og R. Men du f√•r ikke tilgang til SSBs omr√•de for datalagring p√• plattformen. I praksis vil det si at man kan generere data med kode, men man kan ikke jobbe med skarpe data.\nFor √• f√• muligheten til √• jobbe med skarpe data M√Ö du f√∏rst opprette et dapla-team. Dette er det f√∏rste naturlige steget √• ta n√•r man skal begynne √• jobbe med statistikkproduksjon p√• Dapla. I dette kapittelet vil vi forklare det du trenger √• vite om det √• opprette og jobbe innenfor et team."
  },
  {
    "objectID": "spark.html",
    "href": "spark.html",
    "title": "Apache Spark",
    "section": "",
    "text": "Koding i R og Python har historisk sett foreg√•tt p√• en enkelt maskin og v√¶rt begrenset av minnet (RAM) og prosessorkraften p√• maskinen. For bearbeiding av sm√• og mellomstore datasett er det sjelden et problem p√• kj√∏re p√• en enkelt maskin. Popul√¶re pakker som dplyr for R, og pandas for Python, blir ofte brukt i denne typen databehandling. I senere √•r har det ogs√• kommet pakker som er optimalisert for √• kj√∏re kode parallelt p√• flere kjerner p√• en enkelt maskin, skrevet i minne-effektive spr√•k som Rust og C++.\nMen selv om man kommer langt med √• kj√∏re kode p√• en enkelt maskin, vil enkelte oppgaver kreve mer minne og prosessorkraft enn dette. For st√∏rre datasett, eller store beregninger, kan det v√¶re nyttig √• bruke et rammeverk som kan kj√∏re kode parallelt p√• flere maskiner. Et slikt rammeverk er Apache Spark.\nApache Spark er et rammeverk for √• kj√∏re kode parallelt p√• flere maskiner. Det er bygget for √• h√•ndtere store datasett og store beregninger. Det er derfor et nyttig verkt√∏y for √• l√∏se problemer som er for store for √• kj√∏re p√• en enkelt maskin. Men det finnes ogs√• andre bruksomr√•der som er nyttige for Apache Spark. Her er noen eksempler:\nDette er noen av bruksomr√•dene der Spark kan l√∏se problemer som er for store for √• kj√∏re p√• en enkelt maskin med for eksempel Pandas eller dplyr."
  },
  {
    "objectID": "spark.html#spark-p√•-dapla",
    "href": "spark.html#spark-p√•-dapla",
    "title": "Apache Spark",
    "section": "Spark p√• Dapla",
    "text": "Spark p√• Dapla\nDapla kj√∏rer p√• et Kubernetes-kluster og er derfor er et sv√¶rt egnet sted for √• kj√∏re kode parallelt p√• flere maskiner. Jupyter p√• Dapla har ogs√• en flere klargjorte kernels for √• kj√∏re kode i Apache Spark. Denne koden vil kj√∏re p√• et eget kluster av maskiner som er dedikert til Apache Spark, slik som vist i Figur¬†1.\n\n\n\n\n\n\n\n(a) PySpark p√• kubernetes\n\n\n\n\n\n\n\n(b) PySpark p√• 1 maskin\n\n\n\n\n\n\n\n\n\n(c) SparkR p√• kubernetes\n\n\n\n\n\n\n\n(d) SparkR p√• 1 maskin\n\n\n\n\nFigur¬†1: Ferdigkonfigurerte kernels for Spark p√• Dapla.\n\n\nFigur¬†1 (a) og Figur¬†1 (c) kan velges hvis du √∏nsker √• bruke Spark for √• kj√∏re store jobber p√• flere maskiner, for henholdsvis Python- og R-grensesnittene for Spark.\nFigur¬†1 (b) og Figur¬†1 (d) b√∏r du velge hvis du √∏nsker √• bruke Spark av andre grunner enn √• kj√∏re store jobber p√• flere maskiner. For eksempel hvis du √∏nsker √• bruke en av de mange pakker som er bygget p√• Spark, eller hvis du √∏nsker √• bruke Spark til √• lese og skrive data fra Dapla.\nHvis du √∏nsker √• sette opp et eget virtuelt milj√∏ for √• kj√∏re Spark, s√• kan du bruke ssb-project. Se ssb-project for mer informasjon."
  },
  {
    "objectID": "spark.html#spark-i-fa-brands-r-project-og-python-fa-brands-python",
    "href": "spark.html#spark-i-fa-brands-r-project-og-python-fa-brands-python",
    "title": "Apache Spark",
    "section": "Spark i  og Python",
    "text": "Spark i  og Python\nSpark er implementert i programmeringsspr√•ket Scala. Men det tilbys ogs√• mange grensesnitt for √• bruke Spark fra andre spr√•k. De mest popul√¶re grensesnittene er PySpark for Python og SparkR for R. Disse grensesnittene er bygget p√• Spark, og gir tilgang til Spark-funksjonalitet fra Python og R.\n\nPySpark\nPySpark er et Python-grensesnitt for Apache Spark. Det er en Python-pakke som gir tilgang til Spark-funksjonalitet fra Python. Det er enkelt √• bruke, og har mange av de samme funksjonene som Pandas.\nUnder ser du datasettet som benyttes for i vedlagt notebook pyspark-intro.ipynb. Den viser hvordan man kan gj√∏re vanlige databehandling med PySpark. I eksempelet brukes kernel som vist i Figur¬†1 (b).\n\n\n\nCode\n# Legger til row index til DataFrame f√∏r join med dates_df\ndata_df = data_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n\n# Joiner de to datasettene\ndf = (\n    dates_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n    .join(data_df, \"row_index\")\n    .drop(\"row_index\")\n)\n\n# Legger til √•r, kvartal og mnd\ndf = df.withColumn(\"Year\", date_format(df.Date, \"yyyy\"))\ndf = df.withColumn(\"Quarter\", expr(\"quarter(Date)\"))\ndf = df.withColumn(\"Month\", date_format(df.Date, \"MM\"))\n\ndf.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\n\nSource: pyspark-intro.ipynb\nDet finnes ogs√• et Pandas API/grensesnitt mot Spark. M√•let med en er √• gj√∏re overgangen fra Pandas til Spark lettere for nybegynneren. Men hvis man skal gj√∏re litt mer avansert databehandling anbefales det at man bruker PySpark direkte og ikke Pandas API-et.\n\n\nSparkR\nSparkR er et R-grensesnitt for Apache Spark. Det er en R-pakke som gir tilgang til Spark-funksjonalitet fra R. Det er enkelt √• bruke, og har mange av de samme funksjonene som dplyr. Se eksempel i notebook under:\n\n\nselectedColumns &lt;- select(file, \"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\")\nshowDF(selectedColumns, numRows = 5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\nSource: sparkr-intro.ipynb"
  },
  {
    "objectID": "spark.html#lakehouse-arkitektur",
    "href": "spark.html#lakehouse-arkitektur",
    "title": "Apache Spark",
    "section": "Lakehouse-arkitektur",
    "text": "Lakehouse-arkitektur\nEn av utvidelsene som er laget rundt Apache Spark er den s√•kalte Lakehouse-arkitekturen. Kort fortalt kan den dekke behovene som et klassisk datavarehus har tatt seg av tidligere. I kontekst av SSB sine teknologier kan det ogs√• benyttes som et databaselag over Parquet-filer i b√∏tter. Det finnes flere open source l√∏sninger for dette, men mest aktuelle er:\n\nDelta Lake\nApache Hudi\nApache Iceberg\n\nI det f√∏lgende omtaler vi hovedsakelig egenskapene til Delta Lake, men alle rammeverkene har mange av de samme egenskapene. Delta Lake kan ogs√• benyttes p√• Dapla n√•.\nSentrale egenskaper ved Delta Lake er:\n\nACID-transactions som sikrer data-integritet og stabilitet, ogs√• n√•r det skjer feil.\nMetadata som bli h√•ndtert akkurat som all annen data og er veldig skalebar. Den st√∏tter ogs√• egendefinert metadata.\nSchema Enforcement and Evolution sikrer at skjemaet til dataene blir h√•ndhevet, og den tillater ogs√• den kan endres over tid.\nTime travel sikrer at alle versjoner av dataene er blitt lagret, og at du kan g√• tilbake til tidligere versjoner av en fil.\nAudit history sikrer at du kan f√• full oversikt over hvilke operasjoner som utf√∏rt p√• dataene.\nInserts, updates and deletes betyr at du lettere kan manipulere data enn hva som er tilfellet med vanlige Parquet-filer.\nIndexing er st√∏ttes for forbedre sp√∏rringer mot store datamengder."
  },
  {
    "objectID": "standarder.html",
    "href": "standarder.html",
    "title": "Standarder",
    "section": "",
    "text": "Standarder"
  },
  {
    "objectID": "innlogging.html",
    "href": "innlogging.html",
    "title": "Innlogging",
    "section": "",
    "text": "Innlogging p√• Dapla er veldig enkelt. Dapla er en nettadresse som alle SSB-ere kan g√• inn p√• hvis de er logget p√• SSB sitt nettverk. √Ö v√¶re logget p√• SSB sitt nettverk betyr i denne sammenhengen at man er logget p√• med VPN, enten man er p√• kontoret eller p√• hjemmekontor. For √• gj√∏re det enda enklere har vi laget en fast snarvei til denne nettadressen p√• v√•rt intranett/Byr√•nettet(se Figur¬†1).\n\n\n\nFigur¬†1: Snarvei til Dapla fra intranett\n\n\nMen samtidig som det er lett √• logge seg p√•, s√• er det noen kompliserende ting som fortjener en forklaring. Noe skyldes at vi mangler et klart spr√•k for √• definere bakkemilj√∏et og skymilj√∏et slik at alle skj√∏nner hva man snakker om. I denne boken definerer bakkemilj√∏et som stedet der man har drevet med statistikkproduksjon de siste ti√•rene. Skymilj√∏et er den nye dataplattformen Dapla p√• Google Cloud.\nDet som gj√∏r ting litt komplisert er at vi har 2 Jupyter-milj√∏er p√• b√•de bakke og sky. √Örsaken er at vi har ett test- og ett prod-omr√•de for hver, og det blir i alt 4 Jupyter-milj√∏er. Figur¬†2 viser dette.\n\n\n\nFigur¬†2: De 4 Jupyter-milj√∏ene i SSB. Et test-milj√∏ og et prod-milj√∏ p√• bakke og sky/Dapla\n\n\nHver av disse milj√∏ene har sin egen nettadresse og sitt eget bruksomr√•de.\n\n\nI de fleste tilfeller vil en statistikker eller forsker √∏nske √• logge seg inn i prod-milj√∏et. Det er her man skal kj√∏re koden sin i et produksjonsl√∏p som skal publiseres eller utvikles. I noen tilfeller hvor man ber om √• f√• tilgjengliggjort en ny tjeneste s√• vil denne f√∏rst rulles ut i testomr√•det som vi kaller staging-omr√•det. √Örsaken er at vi √∏nsker √• beskytte prod-milj√∏et fra software som potensielt √∏delegger for eksisterende funksjonalitet. Derfor ruller vi ut nye ting i staging f√∏rst. Av den grunn vil de fleste oppleve √• bli bedt om √• logge seg inn der for testing en eller annen gang. Under forklarer vi hvordan man g√•r frem for √• logge seg p√• de to ulike milj√∏ene p√• Dapla.\n\n\nFor √• logge seg inn inn i prod-milj√∏et p√• Dapla kan man gj√∏re f√∏lgende:\n\nG√• inn p√• lenken https://jupyter.dapla.ssb.no/ i en Chrome-nettleser eller klikk p√• lenken p√• Byr√•nettet som vist i Figur¬†1.\nAlle i SSB har en Google Cloud-konto som m√• brukes n√•r man logger seg p√• Dapla. Brukernavnet i Google er det samme som din korte epost-adresse (f.eks. cth@ssb.no). Hvis du ikke allerede er logget inn i Google vil du f√• sp√∏rsm√•l om √• velge hvilken Google-konto som skal brukes (Figur¬†3). Logg inn med din Google-konto (ssb.no) og ditt AD-passord.\n\n\n\n\nFigur¬†3: Velg en Google-konto\n\n\n\nDeretter blir man spurt om man godtar at ssb.no (alts√• Dapla) kan bruke din Google Cloud-konto (Figur¬†4). Trykk Allow.\n\n\n\n\nFigur¬†4: Tillat at ssb.no f√•r bruke din Google Cloud-konto\n\n\n\nDeretter lander man p√• en side som lar deg avgj√∏re hvor mye maskinkraft som skal holdes av til deg (Figur¬†5). Det √∏verste alternativet er valgt som standard, og er tilstrekkelig for de fleste.\n\n\n\n\nFigur¬†5: Velg hvor mye maskinkraft du trenger\n\n\n\nVent til maskinen din starter opp (Figur¬†6). Oppstartstiden kan variere.\n\n\n\n\nFigur¬†6: Starter opp Jupyter\n\n\nEtter dette er man logget inn i et Jupyter-milj√∏ som kj√∏rer p√• en minimal Ubuntu-maskin. Hvis man er del av et Dapla-team f√•r man ogs√• tilgang til alt teamet har tilgang til.\n\n\n\nInnlogging til staging-milj√∏et er identisk med innloggingen til prod-milj√∏et, med ett viktig unntak: nettadressen er n√• https://jupyter.dapla-staging.ssb.no/.\nLitt mer om hva som er tilgjenglig her kommer.\n\n\n\n\n\nNoen ganger kan man komme i en situasjon hvor l√∏sningen for Single Sign-On (p√•logging p√• tvers av flere systemer) gir en feilmelding a la Figur¬†7:\n\n\n\nFigur¬†7: Feil som kan oppst√• ved p√•logging\n\n\nI denne situasjonen m√• man trykke p√• knappen ‚ÄúAdd to existing account‚Äù. Da vil skjermbildet Figur¬†8 dukke opp:\n\n\n\nFigur¬†8: Klikk p√• Google-knappen for √• logge p√• igjen\n\n\nHer m√• man tykke p√• Google-knappen (se pil), og deretter logge inn som vist i Figur¬†3 tidligere i dette avsnittet.\n\n\n\n\n\nJupyter-milj√∏et p√• bakken bruker samme base-image1 for √• installere Jupyterlab, og er derfor identisk p√• mange m√•ter. Men innloggingen er ganske forskjellig.\n\n\n\n\n\n\nNote\n\n\n\nFom. 5. desember 2022 har vi byttet ut Jupyter-milj√∏et p√• bakken. Beskrivelsene under gjelder derfor det nye milj√∏et. Fram til 15. januar vil du kunne bruke det gamle milj√∏et ved √• g√• inn p√• lenken https://jupyter-prod.ssb.no/ manuelt i Google Chrome. Etter 15. januar blir det gamle Jupyter-milj√∏et avviklet.\n\n\n\n\nDu logger deg inn p√• prod i bakkemilj√∏et p√• f√∏lgende m√•te:\n\nLogg deg inn p√• Citrix-Windows i bakkemilj√∏et. Det kan gj√∏res ved √• bruke lenken Citrix p√• Byr√•nettet, som ogs√• vises i Figur¬†1.\nTrykk p√• Jupyterlab-ikonet, som vist p√• Figur¬†9, og logg deg inn med vanlig brukernavn og passord.\n\n\n\n\nFigur¬†9: Jupyterlab-ikon p√• Skrivebordet i Citrix-Windows.\n\n\nN√•r du trykker p√• ikonet blir du tatt til nettadressen https://sl-jupyter-p.ssb.no/. Du kunne ogs√• √•pnet Jupyterlab ved √•pne Chrome-nettleseren og skrive inn adressen manuelt.\n\n\n\nInnlogging til staging-milj√∏et har ingen snarvei p√• Skrivebordet, og du m√• gj√∏re f√∏lgende for √• √•pne milj√∏et:\n\n√Öpne Chrome-nettleseren i Citrix-Windows.\nSkriv inn url-en https://sl-jupyter-t.ssb.no/"
  },
  {
    "objectID": "innlogging.html#dapla",
    "href": "innlogging.html#dapla",
    "title": "Innlogging",
    "section": "",
    "text": "I de fleste tilfeller vil en statistikker eller forsker √∏nske √• logge seg inn i prod-milj√∏et. Det er her man skal kj√∏re koden sin i et produksjonsl√∏p som skal publiseres eller utvikles. I noen tilfeller hvor man ber om √• f√• tilgjengliggjort en ny tjeneste s√• vil denne f√∏rst rulles ut i testomr√•det som vi kaller staging-omr√•det. √Örsaken er at vi √∏nsker √• beskytte prod-milj√∏et fra software som potensielt √∏delegger for eksisterende funksjonalitet. Derfor ruller vi ut nye ting i staging f√∏rst. Av den grunn vil de fleste oppleve √• bli bedt om √• logge seg inn der for testing en eller annen gang. Under forklarer vi hvordan man g√•r frem for √• logge seg p√• de to ulike milj√∏ene p√• Dapla.\n\n\nFor √• logge seg inn inn i prod-milj√∏et p√• Dapla kan man gj√∏re f√∏lgende:\n\nG√• inn p√• lenken https://jupyter.dapla.ssb.no/ i en Chrome-nettleser eller klikk p√• lenken p√• Byr√•nettet som vist i Figur¬†1.\nAlle i SSB har en Google Cloud-konto som m√• brukes n√•r man logger seg p√• Dapla. Brukernavnet i Google er det samme som din korte epost-adresse (f.eks. cth@ssb.no). Hvis du ikke allerede er logget inn i Google vil du f√• sp√∏rsm√•l om √• velge hvilken Google-konto som skal brukes (Figur¬†3). Logg inn med din Google-konto (ssb.no) og ditt AD-passord.\n\n\n\n\nFigur¬†3: Velg en Google-konto\n\n\n\nDeretter blir man spurt om man godtar at ssb.no (alts√• Dapla) kan bruke din Google Cloud-konto (Figur¬†4). Trykk Allow.\n\n\n\n\nFigur¬†4: Tillat at ssb.no f√•r bruke din Google Cloud-konto\n\n\n\nDeretter lander man p√• en side som lar deg avgj√∏re hvor mye maskinkraft som skal holdes av til deg (Figur¬†5). Det √∏verste alternativet er valgt som standard, og er tilstrekkelig for de fleste.\n\n\n\n\nFigur¬†5: Velg hvor mye maskinkraft du trenger\n\n\n\nVent til maskinen din starter opp (Figur¬†6). Oppstartstiden kan variere.\n\n\n\n\nFigur¬†6: Starter opp Jupyter\n\n\nEtter dette er man logget inn i et Jupyter-milj√∏ som kj√∏rer p√• en minimal Ubuntu-maskin. Hvis man er del av et Dapla-team f√•r man ogs√• tilgang til alt teamet har tilgang til.\n\n\n\nInnlogging til staging-milj√∏et er identisk med innloggingen til prod-milj√∏et, med ett viktig unntak: nettadressen er n√• https://jupyter.dapla-staging.ssb.no/.\nLitt mer om hva som er tilgjenglig her kommer.\n\n\n\n\n\nNoen ganger kan man komme i en situasjon hvor l√∏sningen for Single Sign-On (p√•logging p√• tvers av flere systemer) gir en feilmelding a la Figur¬†7:\n\n\n\nFigur¬†7: Feil som kan oppst√• ved p√•logging\n\n\nI denne situasjonen m√• man trykke p√• knappen ‚ÄúAdd to existing account‚Äù. Da vil skjermbildet Figur¬†8 dukke opp:\n\n\n\nFigur¬†8: Klikk p√• Google-knappen for √• logge p√• igjen\n\n\nHer m√• man tykke p√• Google-knappen (se pil), og deretter logge inn som vist i Figur¬†3 tidligere i dette avsnittet."
  },
  {
    "objectID": "innlogging.html#bakkemilj√∏et",
    "href": "innlogging.html#bakkemilj√∏et",
    "title": "Innlogging",
    "section": "",
    "text": "Jupyter-milj√∏et p√• bakken bruker samme base-image1 for √• installere Jupyterlab, og er derfor identisk p√• mange m√•ter. Men innloggingen er ganske forskjellig.\n\n\n\n\n\n\nNote\n\n\n\nFom. 5. desember 2022 har vi byttet ut Jupyter-milj√∏et p√• bakken. Beskrivelsene under gjelder derfor det nye milj√∏et. Fram til 15. januar vil du kunne bruke det gamle milj√∏et ved √• g√• inn p√• lenken https://jupyter-prod.ssb.no/ manuelt i Google Chrome. Etter 15. januar blir det gamle Jupyter-milj√∏et avviklet.\n\n\n\n\nDu logger deg inn p√• prod i bakkemilj√∏et p√• f√∏lgende m√•te:\n\nLogg deg inn p√• Citrix-Windows i bakkemilj√∏et. Det kan gj√∏res ved √• bruke lenken Citrix p√• Byr√•nettet, som ogs√• vises i Figur¬†1.\nTrykk p√• Jupyterlab-ikonet, som vist p√• Figur¬†9, og logg deg inn med vanlig brukernavn og passord.\n\n\n\n\nFigur¬†9: Jupyterlab-ikon p√• Skrivebordet i Citrix-Windows.\n\n\nN√•r du trykker p√• ikonet blir du tatt til nettadressen https://sl-jupyter-p.ssb.no/. Du kunne ogs√• √•pnet Jupyterlab ved √•pne Chrome-nettleseren og skrive inn adressen manuelt.\n\n\n\nInnlogging til staging-milj√∏et har ingen snarvei p√• Skrivebordet, og du m√• gj√∏re f√∏lgende for √• √•pne milj√∏et:\n\n√Öpne Chrome-nettleseren i Citrix-Windows.\nSkriv inn url-en https://sl-jupyter-t.ssb.no/"
  },
  {
    "objectID": "innlogging.html#footnotes",
    "href": "innlogging.html#footnotes",
    "title": "Innlogging",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nHva er base-image?‚Ü©Ô∏é"
  },
  {
    "objectID": "git-og-github.html",
    "href": "git-og-github.html",
    "title": "Git og Github",
    "section": "",
    "text": "I SSB anbefales det man versjonh√•ndterer koden sin med Git og deler koden via GitHub. For √• l√¶re seg √• bruke disse verkt√∏yene p√• en god m√•te er det derfor viktig √• forst√• forskjellen mellom Git og Github. Helt overordnet er forskjellen f√∏lgende:\n\nGit er programvare som er installert p√• maskinen du jobber p√• og som sporer endringer i koden din.\nGitHub er et slags felles mappesystem p√• internett som lar deg dele og samarbeide med andre om kode.\n\nAv definisjonene over s√• skj√∏nner vi at det er Git som gir oss all funksjonalitet for √• lagre versjoner av koden v√•r. GitHub er mer som et valg av mappesystem. Men m√•ten kodemilj√∏ene v√•re er satt opp p√• Dapla s√• har vi ingen fellesmappe som alle kan kj√∏re koden fra. Man utvikler kode i sin egen hjemmemappe, som bare du har tilgang til, og n√•r du skal samarbeide med andre, s√• m√• du sende koden til GitHub. De du samarbeider med m√• deretter hente ned denne koden f√∏r de kan kj√∏re den.\nI dette kapittelet ser vi n√¶rmere p√• Git og Github og hvordan de er implementert i SSB. Selv om SSB har laget programmet ssb-project for √• gj√∏re det lettere √• bl.a. forholde seg til Git og GitHub, s√• vil vi dette kapittelet forklare n√¶rmere hvordan det funker uten dette hjelpemiddelet. Forh√•pentligvis vil det gj√∏re det lettere √• h√•ndtere mer kompliserte situasjoner som oppst√•r i arbeidshverdagen som statistikker.\n\n\nGit er terminalprogram som installert p√• maskinen du jobber. Hvis man ikke liker √• bruke terminalen finnes det mange pek-og-klikk versjoner av Git, blant annet i Jupyterlab, SAS EG og RStudio. Men typisk vil det en eller annen gang oppst√• situasjoner der det ikke finnes l√∏sninger i pek-og-klikk versjonen, og man m√• ordne opp i terminalen. Av den grunn velger vi her √• fokusere p√• hvordan Git fungerer fra terminalen. Vi vil ogs√• fokusere p√• hvordan Git fungerer fra terminalen i Jupyterlab p√• Dapla.\n\n\nGit er en programvare for distribuert versjonsh√•ndtering av filer. Det vil si at den tar vare p√• historien til koden din. At Git er distribuert betyr at alle som jobber med koden har en kopi av koden hos seg selv. N√•r man √∏nsker √• dele koden med andre, s√• laster man opp koden til et felles kodelager som GitHub. Typisk vil Git brukes til √• passe p√• historien til rene tekstfiler, f.eks. kode-script, hvor det gir mening √• se p√• forskjeller mellom filen p√• ulike tidspunkter. Men n√•r det er sagt, s√• kan Git ogs√• brukes til √• f√∏lge med p√• endringer i andre filtyper, f.eks. bin√¶re filer som bilder, PDF-filer, etc.. Men bin√¶re filer er ikke s√• vanlig inkludere i Git-prosjekter siden forskjellene i historikken til filene er vanskelig √• forst√• for mennesker.\nMan aktiverer Git p√• en mappe i filsystemet sitt med kommandoen git init n√•r man st√•r i mappen som skal versjonsh√•nderes. Da vil Git versjonsh√•ndtere alle filer som er i den mappen og i eventuelle undermapper. N√•r du s√• gj√∏r endringer p√• en fil i mappen, s√• vil Git registrere endringer. √ònsker du at endringen skal bli et punkt i historikken til prosjektet, s√• m√• du f√∏rst legge til filen i Git med kommandoen git add filnavn. N√•r du har gjort dette, s√• kan du lagre endringen med kommandoen git commit -m \"Din melding her\". N√•r du har gjort dette, s√• vil endringen v√¶re lagret i Git. N√•r du har gjort mange endringer, s√• kan du sende endringene til GitHub med kommandoen git push. N√•r du har gjort dette, s√• vil endringene v√¶re synlige for alle som har tilgang til GitHub-prosjektet.\nI SSB anbefaler vi at du starter et nytt Git-prosjekt ved √• benytte ssb-project. Da vil du ikke bare aktivere Git, men du kan ogs√• f√• implementert en del andre gode praksiser for √• holde koden din ryddig, oversiktlig og sikker.\nMen f√∏r vi kan begynne √• bruke Git m√• vi konfigurere v√•r egen bruker for Git, som er forklart i neste seksjon.\nLes mer om Git p√• https://git-scm.com/.\n\n\n\n\n\n\n\n\n\nBruk SSB sin konfigurasjon!\n\n\n\nAlle som bruker GitHub i SSB b√∏r bruke SSB sin Git-konfigurasjon. Hvis ikke kan man bl.a. risikere at output fra Notebooks blir pushet til GitHub. Du kan bruke SSBs konfigurasjon ved √• kj√∏re ssb-gitconfig.py i terminalen og svare p√• sp√∏rsm√•lene som dukker opp.\n\n\nFor √• jobbe med Git s√• m√• man konfigurere brukeren sin slik at Git vet hvem som gj√∏r endringer i koden. I praksis betyr det at du m√• ha filen .gitconfig p√• hjemmeomr√•det ditt (f.eks. /home/jovyan/.gitconfig p√• Dapla) med noe grunnleggende informasjon:\n# /home/jovyan/.gitconfig p√• Dapla\n[user]\n        name = Ola Nordmann\n        email = &lt;ini&gt;@ssb.no\nMed denne konfigurasjonen s√• kan man bruke Git lokalt. Men skal man ogs√• bruke GitHub i SSB, dvs. dele kode med andre, s√• m√• man ogs√• legge til mer informasjon i .gitconfig. Heldigvis er det skrevet et script som gj√∏r dette for deg. For √• f√• anbefalt konfigurasjon for Git s√• kan du kj√∏re f√∏lgende kommando i terminalen:\nssb-gitconfig.py\nDette scriptet vil sp√∏rre deg om ditt brukernavn i SSB, og s√• vil det opprette en fil som heter .gitconfig i hjemmeomr√•det ditt. Denne filen vil inneholde informasjon om brukernavn, e-post og tilgang til GitHub. I tillegg vil den s√∏rge for at ditt ssb-project ikke sender output fra notebooks til GitHub.\n\n\n\nP√• Dapla er det Jupyterlab som er utviklingsmilj√∏et for kode. I Jupyterlab vil man ofte jobbe i notebooks, og det har noen konsekvenser for versjonsh√•ndtering. En notebook er en ipynb-fil som inneholder b√•de tekst og kode. √Öpner vi disse filene i Jupyterlab s√• ser vi det kjente notebook-grensesnittet med kode- og markdown-celler. Men egentlig er det bare et grensesnitt bygd over en json-fil1. Denne json-filen inneholder metadata og masse annet som gj√∏r det vanskelig √• se forskjellen p√• en fil over tid. Dette er derfor noe som √• fikses f√∏r Git blir et nyttig verkt√∏y for disse filene.\nI SSB sin anbefalte konfigurasjon setter opp nbdime for √• f√• leselige differ mellom notebooks. Har du satt opp Git med ssb-gitconfig.py s√• vil dette v√¶re automatisk satt opp for deg.\nDet finnes ogs√• alternativer til √• bruke nbdime. P√• Dapla er Jupytext installert for de som ikke √∏nsker √• versjonsh√•ndtere ipynb-filer, men heller vil bruke rene script-filer som .py- eller .R-filer. M√•ten Jupytext gj√∏r dette p√• er konvertere f.eks. en ipynb-fil py-fil for deg, Den kan gj√∏re det automatisk n√•r du lagrer, eller du kan gj√∏re det manuelt. Med denne tiln√¶rmingen s√• kan du be Git ignorere alle ipynb-filer og bare versjonsh√•ndtere de rene script.filene. Oppsett er Jupytext er ikke en del av SSB sin standard-konfigurasjon og er noe du m√• sett opp selv.\n\n\n\nGit er veldig sterkt verkt√∏y med mange muligheter. Det er ikke tenkt at alt skal adresseres i dette kapitlet. Men det er noen vanlig operasjoner som er s√• vanlige at alle som jobber med kode i SSB b√∏r kjenne dem.\nVi har tidligere nevnt at kommandoen for √• aktivere versjonsh√•ndtering med Git p√• en mappe, er git init. Dette gj√∏res ogs√• automatisk n√•r man oppretter et nytt ssb-project.\nHva skjer hvis man gj√∏r en endring i en fil i mappa? For det f√∏rste kan du kj√∏re kommandoen git status for √• se hvilke endringer Git har oppdaget. Men det betyr ikke at endringene er under versjonskontroll enda. For √• fortelle Git om at disse endringene skal registreres s√• m√• du kj√∏re git add &lt;filnavn&gt;. Men fortsatt er ikke endringene et punkt i historien til koden din. For √• gj√∏re det m√• du kj√∏re git commit -m \"Din melding her\". Ved √• gj√∏re det s√• har du laget et unikt punkt i historien til koden din, med en egen ID, som du kan g√• tilbake til senere hvis du √∏nsker.\nN√•r man utvikler kode s√• gj√∏r man det fra s√•kalte branches2. Hvis vi tenker oss at din eksisterende kodebase er stammen p√• et tre (ofte kalt master eller main), s√• legger Git opp til at man gj√∏r endringer p√• denne koden via branches eller grener av treet. Med andre ord holder vi stammen/master ur√∏rt helt til vi vet at endringen fungerer som den skal. Til slutt merger vi den inn i master. Vi kan opprette en ny branch og g√• inn i den ved √• skrive git checkout -b &lt;branch navn&gt;. Da st√•r du i en branch og kan bruke kommandoer som git add og git commit som vist tidligere.\nVi kan merge endringene fra v√•r branch inn i main ved √• f√∏rst g√• inn i main, git switch main, og deretter skrive git merge &lt;branch navn&gt;. Men typisk vil ikke dette v√¶re fremgangsm√•ten i SSB. N√•r man er forn√∏yd med endringene i en branch, s√• vil man pushe den opp til GitHub, slik at en kollega kan vurdere om den skal merges inn i main. Dermed gj√∏res selve mergen i GitHub-grensenittet. Vi skal se n√¶rmere p√• GitHub i neste kapittel.\n\n\n\n\nGitHub er et nettsted som bl.a. fungerer som v√•rt felles mappesystem for deling av kode. SSB har sin egen organisasjonskonto med navn statisticsnorway. Men selv om SSB har en organisasjonskonto p√• GitHub m√• alle ansatte opprette sin egen brukerprofil, og knytte den mot SSB sin organisasjonskonto. Under forklarer vi hvordan du gj√∏r dette.\n\n\nDette kapittelet er bare relevant hvis man ikke har en GitHub-brukerkonto fra f√∏r. For √• bruke ssb-project-programmet til √• generere et remote repo p√• GitHub m√• du ha en konto. Derfor starter vi med √• gj√∏re dette. Det er en engangsjobb og du trenger aldri gj√∏re det igjen.\n\n\n\n\n\n\nSSB har valgt √• ikke sette opp SSB-brukerne til de ansatte som GitHub-brukere. En viktig √•rsak er at er en GitHub-konto ofte regnes som en del av den ansattes CV. For de som aldri har brukt GitHub f√∏r kan det virke fremmed, men det er nok en fordel p√• sikt n√•r alle blir godt kjent med denne arbeidsformen.\n\n\n\nSlik gj√∏r du det:\n\nG√• til https://github.com/\nTrykk Sign up √∏verst i h√∏yre hj√∏rne\nI dialogboksen som √•pnes, se Figur¬†1, skriver du inn din e-postkonto og lager et passord. Dette trenger ikke v√¶re din SSB-bruker og e-post. Vi anbefaler at du bruker en personlig e-postkonto og velger ditt eget passord. Det samme gjelder brukernavn ogs√•.\n\n\n\n\nFigur¬†1: Dialogboks for opprettelse av GitHub-bruker.\n\n\nDu har n√• laget en egen GitHub-bruker. I neste steg skal vi knytte denne kontoen til din SSB-bruker.\n\n\n\nHvis du har fullf√∏rt forrige steg s√• har du n√• en GitHub-konto. Hvis du st√•r p√• din profil-side s√• ser den ut som i Figur¬†2.\n\n\n\nFigur¬†2: Et eksempel p√• hjemmeomr√•det til en GitHub-bruker\n\n\nDet neste vi m√• gj√∏re er √• aktivere 2-faktor autentisering, siden det er dette som benyttes i SSB. Hvis du st√•r p√• siden i bildet over, s√• gj√∏r du f√∏lgende for √• aktivere 2-faktor autentisering mot GitHub:\n\n\n\nTrykk p√• den lille pilen √∏verst til h√∏yre og velg Settings(se Figur¬†3).\nDeretter velger du Password and authentification i menyen til venstre.\nUnder Two-factor authentication trykker du p√• Enable.\n\n\n\n\n\n\nFigur¬†3: √Öpne settings for din GitHub-bruker.\n\n\n\n\n\nFigur¬†4 viser dialogboksen som vises. Velg Enable two-factor authentification.\n\n\n\n\nFigur¬†4: Dialogboks som √•pnes n√•r 2FA skrus p√• f√∏rste gang.\n\n\n\nFigur¬†5 viser dialogboksen som vises for √• velge hvordan man skal autentisere seg. Her anbefales det √• velge Set up using an app, slik at du kan bruke Microsoft Authenticator-appen p√• din mobil.\n\n\n\n\nFigur¬†5: Dialogboks for √• velge hvordan man skal autentisere seg med 2FA.\n\n\nFigur¬†6 viser QR-koden som vises. Denne skal vi bruke i neste steg.\n\n\n\nFigur¬†6: QR-kode som skannes av Microsoft Authenticator.\n\n\n\n\n\nStrekkoden over skal skannes i din Microsoft Authenticator-app p√• mobilen, som vist i Figur¬†7. √Öpne appen, trykk p√• Bekreftede ID-er, og til slutt trykk p√• Skann QR-kode. Deretter skanner du QR-koden fra punkt 5.\nN√•r koden er skannet har du f√•tt opp f√∏lgende bilde p√• appens hovedside (se bilde til h√∏yre). Skriv inn den 6-siffer koden p√• GitHub-siden med QR-koden.\nTil slutt lagrer du Recovery-codes et trygt sted som bare du har tilgang til.\n\n\n\n\n\n\nFigur¬†7: Mobilappen Microsoft authenticator\n\n\n\n\nN√• har vi aktivert 2-faktor autentisering for GitHub og er klare til √• knytte v√•r personlige konto til v√•r SSB-bruker p√• SSBs ‚ÄúGithub organisation‚Äù statisticsnorway.\n\n\n\nI forrige steg aktiverte vi 2-faktor autentisering for GitHub. Det neste vi m√• gj√∏re er √• koble oss til Single Sign On (SSO) for SSB sin organisasjon p√• GitHub:\n\nTrykk p√• lenken https://github.com/orgs/statisticsnorway/sso\nI dialogboksen som dukker opp trykker du p√• Continue, slik som vist i Figur¬†8.\n\n\n\n\nFigur¬†8: Single Sign on (SSO) for SSB sin organisasjon p√• GitHub\n\n\nN√•r du har gjennomf√∏rt dette s√• har du tilgang til statisticsnorway p√• GitHub. G√•r du inn p√• denne lenken s√• skal du n√• kunne lese b√•de Public, Private og Internal repoer, slik som vist i Figur¬†9.\n\n\n\nFigur¬†9: Medlemsvisning for SSB sin GitHub-organisasjon.\n\n\n\n\n\nN√•r vi skal jobbe med SSB-kode som ligger lagret hos statistcsnorway p√• GitHub, s√• m√• vi autentisere oss. M√•ten vi gj√∏re det p√• er ved √• generere et Personal Access Token (ofte forkortet PAT) som vi oppgir n√•r vi vil hente eller oppdatere kode p√• GitHub. Da sender vi med PAT for √• autentisere oss for GitHub.\n\n\nFor √• lage en PAT som er godkjent mot statisticsnorway s√• gj√∏r man f√∏lgende:\n\nG√• til din profilside p√• GitHub og √•pne Settings slik som ble vist Seksjon¬†1.2.2.\nVelg Developer Settings i menyen til venstre.\nI menyen til venstre velger du Personal Access Token, og deretter Tokens (classic).\nVelg Generate new token og deretter Generate new token (classic).\nUnder Note kan du gi PAT‚Äôen et navn. Velg et navn som er intuitivt for deg. Hvis du skal bruke PAT til √• jobbe mot Dapla, s√• ville jeg ganske enkelt kalt den dapla. Hvis du skal bruke den mot bakkemilj√∏et ville jeg kalt den prodsone eller noe annet som gj√∏r det lett for det skj√∏nne innholdet i ettertid.\nUnder Expiration velger du hvor lang tid som skal g√• f√∏r PAT blir ugyldig. Dette er en avvening mellom sikkerhet og hva som er praktisk. Det anbefales at du velger 365 dager. N√•r PAT utl√∏per m√• du gjenta stegene i dette kapittelet.\nUnder Select scopes velger du Repo slik som vist i Figur¬†10.\n\n\n\n\nFigur¬†10: Gi token et kort og beskrivende navn\n\n\n\nTrykk p√• Generate token nederst p√• siden og du f√•r noe lignende det du ser i Figur¬†11.\n\n\n\n\nFigur¬†11: Token som ble generert.\n\n\n\nKopier deretter PAT til en midlertidig fil. Grunnen er at du aldri vil se det igjen her etter at vi har gjennomf√∏rt neste steg.\nDeretter trykker du p√• Configure SSO og velger Authorize ved siden statisticsnorway, slik som vist i Figur¬†12. Svar deretter p√• sp√∏rsm√•lene som dukker opp.\n\n\n\n\nFigur¬†12: Autorisering av Token mot SSBs GiHub-organisasjon.\n\n\nVi har n√• opprettet en PAT som er godkjent for bruk mot SSB sin kode p√• GitHub. Det betyr at hvis vi vil jobbe med Git p√• SSB sine maskiner i sky eller p√• bakken, s√• m√• vi sendte med dette tokenet for √• f√• lov til √• jobbe med koden som ligger p√• statisticsnorway p√• GitHub.\n\n\n\nDet er ganske upraktisk √• m√•tte sende med tokenet hver gang vi skal jobbe med GitHub. Vi b√∏r derfor lagre det lokalt der vi jobber, slik at Git automatisk finner det. Det finnes mange m√•ter √• gj√∏re dette p√• og det er ikke bestemt hva som skal v√¶re beste-praksis i SSB. Men en m√•te √• gj√∏re det er via en .netrc-fil. Vi oppretter da en fil som heter .netrc p√• v√•rt hjemmeomr√•de, og legger f√∏lgende informasjon p√• en (hvilken som helst) linje i filen:\nmachine github.com login &lt;github-bruker&gt; password &lt;Personal Access Token&gt;\nGitHub-bruker er da din personlige bruker og IKKE brukernavnet ditt i SSB. Personal Access Token er det vi lagde i forrige kapittelet.\nEn veldig enkel m√•te √• lagre dette er som f√∏lger. Anta at min personlige GitHub-bruker er SSB-Chad og at min Personal Access Token er blablabla. Da kan jeg gj√∏re f√∏lgende for √• lagre det i .netrc:\n\nG√• inn i Jupyterlab og √•pne en Python-notebook.\nI den f√∏rste kodecellen skriver du:\n\n!echo \"machine github.com login SSB-Chad password blablabla\" &gt;&gt; ~/.netrc\nAlternativt kan du droppe det utropstegnet og kj√∏re det direkte i en terminal. Det vil gi samme resultat. Koden over legger til en linje med teksten\nmachine github.com login SSB-Chad password blablabla\ni en .netrc-fil p√• din hjemmeomr√•det, uanvhengig av om du har en fra f√∏r eller ikke. Hvis du har en fil fra f√∏r som allerede har et token fra GitHub, ville jeg nok slettet det f√∏r jeg legger en et nytt token.\nHver gang du jobber mot GitHub vil Git sjekke om informasjon om autentisering ligger i denne filen, og bruke den hvis den ligger der.\n\n\n\nI eksempelet over lagde vi en PAT som var gyldig i 90 dager. Dermed vil du ikke kunne jobbe mot GitHub med dette tokenet etter 90 dager. For √• oppdatere tokenet gj√∏r du f√∏lgende:\n\nLag et nytt PAT ved √• repetere Seksjon¬†1.2.4.1.\nI milj√∏et der du skal jobbe med Git og GitHub g√•r du inn i din .netrc og bytter ut token med det nye.\n\nOg med det er du klar til √• jobbe mot statisticsnorway p√• GitHub."
  },
  {
    "objectID": "git-og-github.html#git-fa-brands-git-alt",
    "href": "git-og-github.html#git-fa-brands-git-alt",
    "title": "Git og Github",
    "section": "",
    "text": "Git er terminalprogram som installert p√• maskinen du jobber. Hvis man ikke liker √• bruke terminalen finnes det mange pek-og-klikk versjoner av Git, blant annet i Jupyterlab, SAS EG og RStudio. Men typisk vil det en eller annen gang oppst√• situasjoner der det ikke finnes l√∏sninger i pek-og-klikk versjonen, og man m√• ordne opp i terminalen. Av den grunn velger vi her √• fokusere p√• hvordan Git fungerer fra terminalen. Vi vil ogs√• fokusere p√• hvordan Git fungerer fra terminalen i Jupyterlab p√• Dapla.\n\n\nGit er en programvare for distribuert versjonsh√•ndtering av filer. Det vil si at den tar vare p√• historien til koden din. At Git er distribuert betyr at alle som jobber med koden har en kopi av koden hos seg selv. N√•r man √∏nsker √• dele koden med andre, s√• laster man opp koden til et felles kodelager som GitHub. Typisk vil Git brukes til √• passe p√• historien til rene tekstfiler, f.eks. kode-script, hvor det gir mening √• se p√• forskjeller mellom filen p√• ulike tidspunkter. Men n√•r det er sagt, s√• kan Git ogs√• brukes til √• f√∏lge med p√• endringer i andre filtyper, f.eks. bin√¶re filer som bilder, PDF-filer, etc.. Men bin√¶re filer er ikke s√• vanlig inkludere i Git-prosjekter siden forskjellene i historikken til filene er vanskelig √• forst√• for mennesker.\nMan aktiverer Git p√• en mappe i filsystemet sitt med kommandoen git init n√•r man st√•r i mappen som skal versjonsh√•nderes. Da vil Git versjonsh√•ndtere alle filer som er i den mappen og i eventuelle undermapper. N√•r du s√• gj√∏r endringer p√• en fil i mappen, s√• vil Git registrere endringer. √ònsker du at endringen skal bli et punkt i historikken til prosjektet, s√• m√• du f√∏rst legge til filen i Git med kommandoen git add filnavn. N√•r du har gjort dette, s√• kan du lagre endringen med kommandoen git commit -m \"Din melding her\". N√•r du har gjort dette, s√• vil endringen v√¶re lagret i Git. N√•r du har gjort mange endringer, s√• kan du sende endringene til GitHub med kommandoen git push. N√•r du har gjort dette, s√• vil endringene v√¶re synlige for alle som har tilgang til GitHub-prosjektet.\nI SSB anbefaler vi at du starter et nytt Git-prosjekt ved √• benytte ssb-project. Da vil du ikke bare aktivere Git, men du kan ogs√• f√• implementert en del andre gode praksiser for √• holde koden din ryddig, oversiktlig og sikker.\nMen f√∏r vi kan begynne √• bruke Git m√• vi konfigurere v√•r egen bruker for Git, som er forklart i neste seksjon.\nLes mer om Git p√• https://git-scm.com/.\n\n\n\n\n\n\n\n\n\nBruk SSB sin konfigurasjon!\n\n\n\nAlle som bruker GitHub i SSB b√∏r bruke SSB sin Git-konfigurasjon. Hvis ikke kan man bl.a. risikere at output fra Notebooks blir pushet til GitHub. Du kan bruke SSBs konfigurasjon ved √• kj√∏re ssb-gitconfig.py i terminalen og svare p√• sp√∏rsm√•lene som dukker opp.\n\n\nFor √• jobbe med Git s√• m√• man konfigurere brukeren sin slik at Git vet hvem som gj√∏r endringer i koden. I praksis betyr det at du m√• ha filen .gitconfig p√• hjemmeomr√•det ditt (f.eks. /home/jovyan/.gitconfig p√• Dapla) med noe grunnleggende informasjon:\n# /home/jovyan/.gitconfig p√• Dapla\n[user]\n        name = Ola Nordmann\n        email = &lt;ini&gt;@ssb.no\nMed denne konfigurasjonen s√• kan man bruke Git lokalt. Men skal man ogs√• bruke GitHub i SSB, dvs. dele kode med andre, s√• m√• man ogs√• legge til mer informasjon i .gitconfig. Heldigvis er det skrevet et script som gj√∏r dette for deg. For √• f√• anbefalt konfigurasjon for Git s√• kan du kj√∏re f√∏lgende kommando i terminalen:\nssb-gitconfig.py\nDette scriptet vil sp√∏rre deg om ditt brukernavn i SSB, og s√• vil det opprette en fil som heter .gitconfig i hjemmeomr√•det ditt. Denne filen vil inneholde informasjon om brukernavn, e-post og tilgang til GitHub. I tillegg vil den s√∏rge for at ditt ssb-project ikke sender output fra notebooks til GitHub.\n\n\n\nP√• Dapla er det Jupyterlab som er utviklingsmilj√∏et for kode. I Jupyterlab vil man ofte jobbe i notebooks, og det har noen konsekvenser for versjonsh√•ndtering. En notebook er en ipynb-fil som inneholder b√•de tekst og kode. √Öpner vi disse filene i Jupyterlab s√• ser vi det kjente notebook-grensesnittet med kode- og markdown-celler. Men egentlig er det bare et grensesnitt bygd over en json-fil1. Denne json-filen inneholder metadata og masse annet som gj√∏r det vanskelig √• se forskjellen p√• en fil over tid. Dette er derfor noe som √• fikses f√∏r Git blir et nyttig verkt√∏y for disse filene.\nI SSB sin anbefalte konfigurasjon setter opp nbdime for √• f√• leselige differ mellom notebooks. Har du satt opp Git med ssb-gitconfig.py s√• vil dette v√¶re automatisk satt opp for deg.\nDet finnes ogs√• alternativer til √• bruke nbdime. P√• Dapla er Jupytext installert for de som ikke √∏nsker √• versjonsh√•ndtere ipynb-filer, men heller vil bruke rene script-filer som .py- eller .R-filer. M√•ten Jupytext gj√∏r dette p√• er konvertere f.eks. en ipynb-fil py-fil for deg, Den kan gj√∏re det automatisk n√•r du lagrer, eller du kan gj√∏re det manuelt. Med denne tiln√¶rmingen s√• kan du be Git ignorere alle ipynb-filer og bare versjonsh√•ndtere de rene script.filene. Oppsett er Jupytext er ikke en del av SSB sin standard-konfigurasjon og er noe du m√• sett opp selv.\n\n\n\nGit er veldig sterkt verkt√∏y med mange muligheter. Det er ikke tenkt at alt skal adresseres i dette kapitlet. Men det er noen vanlig operasjoner som er s√• vanlige at alle som jobber med kode i SSB b√∏r kjenne dem.\nVi har tidligere nevnt at kommandoen for √• aktivere versjonsh√•ndtering med Git p√• en mappe, er git init. Dette gj√∏res ogs√• automatisk n√•r man oppretter et nytt ssb-project.\nHva skjer hvis man gj√∏r en endring i en fil i mappa? For det f√∏rste kan du kj√∏re kommandoen git status for √• se hvilke endringer Git har oppdaget. Men det betyr ikke at endringene er under versjonskontroll enda. For √• fortelle Git om at disse endringene skal registreres s√• m√• du kj√∏re git add &lt;filnavn&gt;. Men fortsatt er ikke endringene et punkt i historien til koden din. For √• gj√∏re det m√• du kj√∏re git commit -m \"Din melding her\". Ved √• gj√∏re det s√• har du laget et unikt punkt i historien til koden din, med en egen ID, som du kan g√• tilbake til senere hvis du √∏nsker.\nN√•r man utvikler kode s√• gj√∏r man det fra s√•kalte branches2. Hvis vi tenker oss at din eksisterende kodebase er stammen p√• et tre (ofte kalt master eller main), s√• legger Git opp til at man gj√∏r endringer p√• denne koden via branches eller grener av treet. Med andre ord holder vi stammen/master ur√∏rt helt til vi vet at endringen fungerer som den skal. Til slutt merger vi den inn i master. Vi kan opprette en ny branch og g√• inn i den ved √• skrive git checkout -b &lt;branch navn&gt;. Da st√•r du i en branch og kan bruke kommandoer som git add og git commit som vist tidligere.\nVi kan merge endringene fra v√•r branch inn i main ved √• f√∏rst g√• inn i main, git switch main, og deretter skrive git merge &lt;branch navn&gt;. Men typisk vil ikke dette v√¶re fremgangsm√•ten i SSB. N√•r man er forn√∏yd med endringene i en branch, s√• vil man pushe den opp til GitHub, slik at en kollega kan vurdere om den skal merges inn i main. Dermed gj√∏res selve mergen i GitHub-grensenittet. Vi skal se n√¶rmere p√• GitHub i neste kapittel."
  },
  {
    "objectID": "git-og-github.html#github-fa-brands-github",
    "href": "git-og-github.html#github-fa-brands-github",
    "title": "Git og Github",
    "section": "",
    "text": "GitHub er et nettsted som bl.a. fungerer som v√•rt felles mappesystem for deling av kode. SSB har sin egen organisasjonskonto med navn statisticsnorway. Men selv om SSB har en organisasjonskonto p√• GitHub m√• alle ansatte opprette sin egen brukerprofil, og knytte den mot SSB sin organisasjonskonto. Under forklarer vi hvordan du gj√∏r dette.\n\n\nDette kapittelet er bare relevant hvis man ikke har en GitHub-brukerkonto fra f√∏r. For √• bruke ssb-project-programmet til √• generere et remote repo p√• GitHub m√• du ha en konto. Derfor starter vi med √• gj√∏re dette. Det er en engangsjobb og du trenger aldri gj√∏re det igjen.\n\n\n\n\n\n\nSSB har valgt √• ikke sette opp SSB-brukerne til de ansatte som GitHub-brukere. En viktig √•rsak er at er en GitHub-konto ofte regnes som en del av den ansattes CV. For de som aldri har brukt GitHub f√∏r kan det virke fremmed, men det er nok en fordel p√• sikt n√•r alle blir godt kjent med denne arbeidsformen.\n\n\n\nSlik gj√∏r du det:\n\nG√• til https://github.com/\nTrykk Sign up √∏verst i h√∏yre hj√∏rne\nI dialogboksen som √•pnes, se Figur¬†1, skriver du inn din e-postkonto og lager et passord. Dette trenger ikke v√¶re din SSB-bruker og e-post. Vi anbefaler at du bruker en personlig e-postkonto og velger ditt eget passord. Det samme gjelder brukernavn ogs√•.\n\n\n\n\nFigur¬†1: Dialogboks for opprettelse av GitHub-bruker.\n\n\nDu har n√• laget en egen GitHub-bruker. I neste steg skal vi knytte denne kontoen til din SSB-bruker.\n\n\n\nHvis du har fullf√∏rt forrige steg s√• har du n√• en GitHub-konto. Hvis du st√•r p√• din profil-side s√• ser den ut som i Figur¬†2.\n\n\n\nFigur¬†2: Et eksempel p√• hjemmeomr√•det til en GitHub-bruker\n\n\nDet neste vi m√• gj√∏re er √• aktivere 2-faktor autentisering, siden det er dette som benyttes i SSB. Hvis du st√•r p√• siden i bildet over, s√• gj√∏r du f√∏lgende for √• aktivere 2-faktor autentisering mot GitHub:\n\n\n\nTrykk p√• den lille pilen √∏verst til h√∏yre og velg Settings(se Figur¬†3).\nDeretter velger du Password and authentification i menyen til venstre.\nUnder Two-factor authentication trykker du p√• Enable.\n\n\n\n\n\n\nFigur¬†3: √Öpne settings for din GitHub-bruker.\n\n\n\n\n\nFigur¬†4 viser dialogboksen som vises. Velg Enable two-factor authentification.\n\n\n\n\nFigur¬†4: Dialogboks som √•pnes n√•r 2FA skrus p√• f√∏rste gang.\n\n\n\nFigur¬†5 viser dialogboksen som vises for √• velge hvordan man skal autentisere seg. Her anbefales det √• velge Set up using an app, slik at du kan bruke Microsoft Authenticator-appen p√• din mobil.\n\n\n\n\nFigur¬†5: Dialogboks for √• velge hvordan man skal autentisere seg med 2FA.\n\n\nFigur¬†6 viser QR-koden som vises. Denne skal vi bruke i neste steg.\n\n\n\nFigur¬†6: QR-kode som skannes av Microsoft Authenticator.\n\n\n\n\n\nStrekkoden over skal skannes i din Microsoft Authenticator-app p√• mobilen, som vist i Figur¬†7. √Öpne appen, trykk p√• Bekreftede ID-er, og til slutt trykk p√• Skann QR-kode. Deretter skanner du QR-koden fra punkt 5.\nN√•r koden er skannet har du f√•tt opp f√∏lgende bilde p√• appens hovedside (se bilde til h√∏yre). Skriv inn den 6-siffer koden p√• GitHub-siden med QR-koden.\nTil slutt lagrer du Recovery-codes et trygt sted som bare du har tilgang til.\n\n\n\n\n\n\nFigur¬†7: Mobilappen Microsoft authenticator\n\n\n\n\nN√• har vi aktivert 2-faktor autentisering for GitHub og er klare til √• knytte v√•r personlige konto til v√•r SSB-bruker p√• SSBs ‚ÄúGithub organisation‚Äù statisticsnorway.\n\n\n\nI forrige steg aktiverte vi 2-faktor autentisering for GitHub. Det neste vi m√• gj√∏re er √• koble oss til Single Sign On (SSO) for SSB sin organisasjon p√• GitHub:\n\nTrykk p√• lenken https://github.com/orgs/statisticsnorway/sso\nI dialogboksen som dukker opp trykker du p√• Continue, slik som vist i Figur¬†8.\n\n\n\n\nFigur¬†8: Single Sign on (SSO) for SSB sin organisasjon p√• GitHub\n\n\nN√•r du har gjennomf√∏rt dette s√• har du tilgang til statisticsnorway p√• GitHub. G√•r du inn p√• denne lenken s√• skal du n√• kunne lese b√•de Public, Private og Internal repoer, slik som vist i Figur¬†9.\n\n\n\nFigur¬†9: Medlemsvisning for SSB sin GitHub-organisasjon.\n\n\n\n\n\nN√•r vi skal jobbe med SSB-kode som ligger lagret hos statistcsnorway p√• GitHub, s√• m√• vi autentisere oss. M√•ten vi gj√∏re det p√• er ved √• generere et Personal Access Token (ofte forkortet PAT) som vi oppgir n√•r vi vil hente eller oppdatere kode p√• GitHub. Da sender vi med PAT for √• autentisere oss for GitHub.\n\n\nFor √• lage en PAT som er godkjent mot statisticsnorway s√• gj√∏r man f√∏lgende:\n\nG√• til din profilside p√• GitHub og √•pne Settings slik som ble vist Seksjon¬†1.2.2.\nVelg Developer Settings i menyen til venstre.\nI menyen til venstre velger du Personal Access Token, og deretter Tokens (classic).\nVelg Generate new token og deretter Generate new token (classic).\nUnder Note kan du gi PAT‚Äôen et navn. Velg et navn som er intuitivt for deg. Hvis du skal bruke PAT til √• jobbe mot Dapla, s√• ville jeg ganske enkelt kalt den dapla. Hvis du skal bruke den mot bakkemilj√∏et ville jeg kalt den prodsone eller noe annet som gj√∏r det lett for det skj√∏nne innholdet i ettertid.\nUnder Expiration velger du hvor lang tid som skal g√• f√∏r PAT blir ugyldig. Dette er en avvening mellom sikkerhet og hva som er praktisk. Det anbefales at du velger 365 dager. N√•r PAT utl√∏per m√• du gjenta stegene i dette kapittelet.\nUnder Select scopes velger du Repo slik som vist i Figur¬†10.\n\n\n\n\nFigur¬†10: Gi token et kort og beskrivende navn\n\n\n\nTrykk p√• Generate token nederst p√• siden og du f√•r noe lignende det du ser i Figur¬†11.\n\n\n\n\nFigur¬†11: Token som ble generert.\n\n\n\nKopier deretter PAT til en midlertidig fil. Grunnen er at du aldri vil se det igjen her etter at vi har gjennomf√∏rt neste steg.\nDeretter trykker du p√• Configure SSO og velger Authorize ved siden statisticsnorway, slik som vist i Figur¬†12. Svar deretter p√• sp√∏rsm√•lene som dukker opp.\n\n\n\n\nFigur¬†12: Autorisering av Token mot SSBs GiHub-organisasjon.\n\n\nVi har n√• opprettet en PAT som er godkjent for bruk mot SSB sin kode p√• GitHub. Det betyr at hvis vi vil jobbe med Git p√• SSB sine maskiner i sky eller p√• bakken, s√• m√• vi sendte med dette tokenet for √• f√• lov til √• jobbe med koden som ligger p√• statisticsnorway p√• GitHub.\n\n\n\nDet er ganske upraktisk √• m√•tte sende med tokenet hver gang vi skal jobbe med GitHub. Vi b√∏r derfor lagre det lokalt der vi jobber, slik at Git automatisk finner det. Det finnes mange m√•ter √• gj√∏re dette p√• og det er ikke bestemt hva som skal v√¶re beste-praksis i SSB. Men en m√•te √• gj√∏re det er via en .netrc-fil. Vi oppretter da en fil som heter .netrc p√• v√•rt hjemmeomr√•de, og legger f√∏lgende informasjon p√• en (hvilken som helst) linje i filen:\nmachine github.com login &lt;github-bruker&gt; password &lt;Personal Access Token&gt;\nGitHub-bruker er da din personlige bruker og IKKE brukernavnet ditt i SSB. Personal Access Token er det vi lagde i forrige kapittelet.\nEn veldig enkel m√•te √• lagre dette er som f√∏lger. Anta at min personlige GitHub-bruker er SSB-Chad og at min Personal Access Token er blablabla. Da kan jeg gj√∏re f√∏lgende for √• lagre det i .netrc:\n\nG√• inn i Jupyterlab og √•pne en Python-notebook.\nI den f√∏rste kodecellen skriver du:\n\n!echo \"machine github.com login SSB-Chad password blablabla\" &gt;&gt; ~/.netrc\nAlternativt kan du droppe det utropstegnet og kj√∏re det direkte i en terminal. Det vil gi samme resultat. Koden over legger til en linje med teksten\nmachine github.com login SSB-Chad password blablabla\ni en .netrc-fil p√• din hjemmeomr√•det, uanvhengig av om du har en fra f√∏r eller ikke. Hvis du har en fil fra f√∏r som allerede har et token fra GitHub, ville jeg nok slettet det f√∏r jeg legger en et nytt token.\nHver gang du jobber mot GitHub vil Git sjekke om informasjon om autentisering ligger i denne filen, og bruke den hvis den ligger der.\n\n\n\nI eksempelet over lagde vi en PAT som var gyldig i 90 dager. Dermed vil du ikke kunne jobbe mot GitHub med dette tokenet etter 90 dager. For √• oppdatere tokenet gj√∏r du f√∏lgende:\n\nLag et nytt PAT ved √• repetere Seksjon¬†1.2.4.1.\nI milj√∏et der du skal jobbe med Git og GitHub g√•r du inn i din .netrc og bytter ut token med det nye.\n\nOg med det er du klar til √• jobbe mot statisticsnorway p√• GitHub."
  },
  {
    "objectID": "git-og-github.html#footnotes",
    "href": "git-og-github.html#footnotes",
    "title": "Git og Github",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPr√∏v selv √• √•pne en ipynb som json ved h√∏reklikke p√• fila i Jupyterlab, velge Open with, og velg json. Da vil du se den underliggende json-filen‚Ü©Ô∏é\nBranches kan oversettes til grener p√• norsk. Men i denne boken velger vi √• bruke det engelske ordet branches. Grunnen er at det erfaringsmessig er lettere forholde seg til det engelske ordet n√•r man skal s√∏ke etter informasjon i annen dokumentasjon‚Ü©Ô∏é"
  },
  {
    "objectID": "kildedata-prosessering.html",
    "href": "kildedata-prosessering.html",
    "title": "Kildedata prosessering",
    "section": "",
    "text": "Denne tjenesten er under utvikling og kan ikke anses som klar for produksjon.\n\n\n\nFor √• minske aksessering av PII1, oppfordres alle team p√• Dapla √• benytte seg av automatisering av kildedata prosessering. Automatisering av kildedata er en tjeneste som er tilgjengelig for team √• ta i bruk 100% selv-betjent. Kildedata (Standardutvalget 2021, 5) prosesseres til inndata gjennom et bestemt utvalg av operasjoner. Kildedata prosesseres som individuelle filer for √• holde oppsettet enkelt og m√•lrettet mot de definerte operasjoner. Mer kompleks operasjoner som g√•r p√• tvers av flere filer burde utf√∏res p√• inndata eller senere datatilstander.\n\n\n\n\n\n\nDet er kun teamets kildedataansvarlige som skal aksessere kildedata.\n\n\n\n\n\n\n\n\n\nTeamets kildedataansvarlige tar ansvar for √• prosessere kildedata til inndata p√• en forsvarlig m√•te.\n\n\n\n\n\n\n\n\nFigur¬†1: Operasjoner som inng√•r i kildedata prosessering\n\n\n\nInndata er kildedata som er transformert til SSBs standard lagringsformat. Variabelnavn og -innhold er uendret bortsett fra at\n\ndirekte identifiserende variabler (f.eks. f√∏dselsnummer) er pseudonymisert\ntegnsett, datoformat, adresse mm er endret til SSBs standardformat\ndet benyttes standard kodeverk (Klass) der det er mulig (f.eks. kj√∏nn)\ndataene er minimert slik at kun variablene som er n√∏dvendige i den videre produksjonsprosessen, inng√•r.\n\n\n(Standardutvalget 2021, 8)\nDet er ikke anbefalt √• gjennomf√∏re operasjoner som:\n\nG√•r p√• tvers av flere filer\nLegge til nye felt\nEndre navn p√• felt\nAggregerer data\nosv.\n\n\n\n\n\n\nF√∏lg instruksjonene her for √• koble prosjektet ditt til Github.\n\n\n\n\n\nKilder konfigureres i et teams Infrastructure as Code (IaC) repo p√• Github. Det kan finnes basert p√• f√∏lgende formulering: github.com/statisticsnorway/&lt;teamnavn&gt;-iac. Kilder konfigureres under stien automation/source_data p√• repoet.\n\n\n\nHver kilde konfigureres ved hjelp av to filer:\n\nconfig.yaml som blant annet konfigurer hvilke stier i teamets kildedatab√∏tte prosesseres.\nprocess_source_data.py som kj√∏res n√•r en kildedatafil prosesseres. Her m√• man skrive en python funksjon p√• en viss format.\n\nDisse filene er lagt til en mappe per kilde, under automation/source_data i IaC repoet. Se eksemplet under for en detaljert forklaring.\n\n\n\n\n\n\nMappenavnet for kilder i IaC repoene er brukt som navn for ressurser. Dette i praksis betyr at det enesete tillatte tegnene i mappenavnet er bokstaver, tall og bindestrek. Det er ikke tillatt med mellomrom eller andre spesialtegn. Mappenavnet kan best√• av opptil 20 tegn.\n\n\n\n\n\n\nDette g√•r ut p√• om prosesseringsscriptet kan enkelt h√•ndtere variasjonen i filene som samles inn.\nGrunn til √• opprette en ny kilde kan v√¶re: - Kildedatafilen har en annen format (f.eks xml eller json) - Kildedataen har ulike felter - Kildedataen inneholder PII2 eller ikke\n\n\n\n\n\n\n\n\n\nDisse instruksjoner forutsetter at prosjektet ditt er koblet til Github allerede.\n\n\n\n\nSkrive skriptet process_source_data.py som prosesserer kildedatafilen til inndata. Dette kan testes av kildedataansvarlige manuelt p√• Jupyter for √• verifisere at dataene blir prosessert som √∏nsket.\nI en branch i teamets IaC repo, legge til config.yaml og process_source_data.py i en mappe under automation/source_data. Se eksemplet under for en detaljert forklaring av formatet.\nLag en PR p√• grenen og f√• den godkjent av kildedataansvarlige.\nVent til alle tester er ferdige. Det skal st√• ‚ÄúAll checks have passed‚Äù f√∏r man g√•r videre, hvis testene feiler f√∏lg stegene her. \nSkrive atlantis apply i en kommentar p√• PRen for √• opprette det n√∏dvendige infrastruktur for √• prosessere kilden.\nMerge PRen.\nSjekk resultatet av det automatiske bygget.\nVerifisere at nye filer lagt i kildedatab√∏tten blir prosessert til inndata som forventet.\n\n\n\n\n\nLa oss si at et team (sm√•bakst) har to datakilder levert av ulik dataeiere p√• ulik formater. Den ene er om boller og er p√• csv format og den andre er om rundstykker og er p√• json format. Kildedataansvarlige i teamet bestemmer seg for at filene i boller/ er like nok √• prosesseres som en kilde, og at filene i rundstykker/ kan prosesseres som en annen kilde.\n\n\nssb-prod-smaabakst-data-kilde\n‚îú‚îÄ‚îÄ boller\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ hvetebolle\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 2018-salg.csv\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 2019-salg.csv\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ...\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ kanelbolle\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 2018-salg.csv\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 2019-salg.csv\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ...\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ skolebolle\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 2018-salg.csv\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 2019-salg.csv\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ ...\n‚îî‚îÄ‚îÄ rundstykker\n    ‚îú‚îÄ‚îÄ haandverker\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ apr-2022-resultater.json\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ aug-2022-resultater.json\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ...\n    ‚îî‚îÄ‚îÄ havre\n        ‚îú‚îÄ‚îÄ apr-2022-resultater.json\n        ‚îú‚îÄ‚îÄ aug-2022-resultater.json\n        ‚îú‚îÄ‚îÄ ...\n\n\n\nsmaabakst-iac\n‚îî‚îÄ‚îÄ automation\n    ‚îî‚îÄ‚îÄ source_data\n        ‚îú‚îÄ‚îÄ boller\n        ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ config.yaml\n        ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ process_source_data.py\n        ‚îî‚îÄ‚îÄ rundstykker\n            ‚îú‚îÄ‚îÄ config.yaml\n            ‚îî‚îÄ‚îÄ process_source_data.py\n\n\n\nfolder_prefix: boller\nVerdien for folder_prefix tilsvarer en ‚Äúfil sti‚Äù i kildedatab√∏tte. I dette tilfellet vil nye filer lagt til under boller/ trigge en prosessering.\n\n\n\nfolder_prefix: rundstykker\n\n\n\n\n\nMed prosesseringsscriptet mener vi filen process_source_data.py som ligger i en mappe per kilde under automation/source_data. Dette scriptet vil bli kalt hver gang det blir lagt til en ny fil i kildedatab√∏tten. Metodesignaturen ser slik ut:\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\nDette betyr at hvis f.eks. 10 filer blir lagt til i kildedatab√∏tten samtidig s√• vil det startes opp 10 individuelle Python-prosesser som kaller denne main-metoden med forskjellig file_path. Parameteren file_path vil inneholde hele filstien inkl. filnavn. S√• en enkel flytteoperasjon fra kildedatab√∏tten til inndateb√∏tten (uten noen form for konvertering) vil kunne uttrykkes slik:\nimport dapla as dp\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n    source_bucket_name = \"ssb-prod-my-project-data-kilde\"\n    destination_bucket_name = \"ssb-prod-my-project-data-produkt\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    df = dp.read_pandas(file_path)\n    dp.write_pandas(df, destination_bucket_path)\nAlternativt‚Ä¶\nfrom dapla import FileClient\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n    source_bucket_name = \"ssb-prod-my-project-data-kilde\"\n    destination_bucket_name = \"ssb-prod-my-project-data-produkt\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    fs = FileClient.get_gcs_file_system()\n    fs.copy(file_path, destination_path)\n\n\nDet anbefales √• bruke Pythons logging modul for √• logge, og ikke bruke print eller skrive til stdout/stderr. Det er satt opp en standard logger-konfigurasjon som skriver informasjonsmeldinger (log level info) til stdout og feilmeldinger (log level warning eller error) til stderr. Feil som ikke blir h√•ndtert blir automatisk fanget opp og logget av automatiseringsl√∏sningen. Eksemplet nedenfor logger en informasjonsmelding, en advarsel og en feilmelding:\nimport logging\nlogging.info('Til info')\nlogging.warning('Advarsel!')\nlogging.error('En feil oppstod!')"
  },
  {
    "objectID": "kildedata-prosessering.html#operasjoner-som-inng√•r-i-kildedata-prosessering",
    "href": "kildedata-prosessering.html#operasjoner-som-inng√•r-i-kildedata-prosessering",
    "title": "Kildedata prosessering",
    "section": "",
    "text": "Figur¬†1: Operasjoner som inng√•r i kildedata prosessering\n\n\n\nInndata er kildedata som er transformert til SSBs standard lagringsformat. Variabelnavn og -innhold er uendret bortsett fra at\n\ndirekte identifiserende variabler (f.eks. f√∏dselsnummer) er pseudonymisert\ntegnsett, datoformat, adresse mm er endret til SSBs standardformat\ndet benyttes standard kodeverk (Klass) der det er mulig (f.eks. kj√∏nn)\ndataene er minimert slik at kun variablene som er n√∏dvendige i den videre produksjonsprosessen, inng√•r.\n\n\n(Standardutvalget 2021, 8)\nDet er ikke anbefalt √• gjennomf√∏re operasjoner som:\n\nG√•r p√• tvers av flere filer\nLegge til nye felt\nEndre navn p√• felt\nAggregerer data\nosv."
  },
  {
    "objectID": "kildedata-prosessering.html#ta-tjenesten-i-bruk",
    "href": "kildedata-prosessering.html#ta-tjenesten-i-bruk",
    "title": "Kildedata prosessering",
    "section": "",
    "text": "F√∏lg instruksjonene her for √• koble prosjektet ditt til Github.\n\n\n\n\n\nKilder konfigureres i et teams Infrastructure as Code (IaC) repo p√• Github. Det kan finnes basert p√• f√∏lgende formulering: github.com/statisticsnorway/&lt;teamnavn&gt;-iac. Kilder konfigureres under stien automation/source_data p√• repoet.\n\n\n\nHver kilde konfigureres ved hjelp av to filer:\n\nconfig.yaml som blant annet konfigurer hvilke stier i teamets kildedatab√∏tte prosesseres.\nprocess_source_data.py som kj√∏res n√•r en kildedatafil prosesseres. Her m√• man skrive en python funksjon p√• en viss format.\n\nDisse filene er lagt til en mappe per kilde, under automation/source_data i IaC repoet. Se eksemplet under for en detaljert forklaring.\n\n\n\n\n\n\nMappenavnet for kilder i IaC repoene er brukt som navn for ressurser. Dette i praksis betyr at det enesete tillatte tegnene i mappenavnet er bokstaver, tall og bindestrek. Det er ikke tillatt med mellomrom eller andre spesialtegn. Mappenavnet kan best√• av opptil 20 tegn.\n\n\n\n\n\n\nDette g√•r ut p√• om prosesseringsscriptet kan enkelt h√•ndtere variasjonen i filene som samles inn.\nGrunn til √• opprette en ny kilde kan v√¶re: - Kildedatafilen har en annen format (f.eks xml eller json) - Kildedataen har ulike felter - Kildedataen inneholder PII2 eller ikke\n\n\n\n\n\n\n\n\n\nDisse instruksjoner forutsetter at prosjektet ditt er koblet til Github allerede.\n\n\n\n\nSkrive skriptet process_source_data.py som prosesserer kildedatafilen til inndata. Dette kan testes av kildedataansvarlige manuelt p√• Jupyter for √• verifisere at dataene blir prosessert som √∏nsket.\nI en branch i teamets IaC repo, legge til config.yaml og process_source_data.py i en mappe under automation/source_data. Se eksemplet under for en detaljert forklaring av formatet.\nLag en PR p√• grenen og f√• den godkjent av kildedataansvarlige.\nVent til alle tester er ferdige. Det skal st√• ‚ÄúAll checks have passed‚Äù f√∏r man g√•r videre, hvis testene feiler f√∏lg stegene her. \nSkrive atlantis apply i en kommentar p√• PRen for √• opprette det n√∏dvendige infrastruktur for √• prosessere kilden.\nMerge PRen.\nSjekk resultatet av det automatiske bygget.\nVerifisere at nye filer lagt i kildedatab√∏tten blir prosessert til inndata som forventet.\n\n\n\n\n\nLa oss si at et team (sm√•bakst) har to datakilder levert av ulik dataeiere p√• ulik formater. Den ene er om boller og er p√• csv format og den andre er om rundstykker og er p√• json format. Kildedataansvarlige i teamet bestemmer seg for at filene i boller/ er like nok √• prosesseres som en kilde, og at filene i rundstykker/ kan prosesseres som en annen kilde.\n\n\nssb-prod-smaabakst-data-kilde\n‚îú‚îÄ‚îÄ boller\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ hvetebolle\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 2018-salg.csv\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 2019-salg.csv\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ...\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ kanelbolle\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 2018-salg.csv\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 2019-salg.csv\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ...\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ skolebolle\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 2018-salg.csv\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 2019-salg.csv\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ ...\n‚îî‚îÄ‚îÄ rundstykker\n    ‚îú‚îÄ‚îÄ haandverker\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ apr-2022-resultater.json\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ aug-2022-resultater.json\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ...\n    ‚îî‚îÄ‚îÄ havre\n        ‚îú‚îÄ‚îÄ apr-2022-resultater.json\n        ‚îú‚îÄ‚îÄ aug-2022-resultater.json\n        ‚îú‚îÄ‚îÄ ...\n\n\n\nsmaabakst-iac\n‚îî‚îÄ‚îÄ automation\n    ‚îî‚îÄ‚îÄ source_data\n        ‚îú‚îÄ‚îÄ boller\n        ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ config.yaml\n        ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ process_source_data.py\n        ‚îî‚îÄ‚îÄ rundstykker\n            ‚îú‚îÄ‚îÄ config.yaml\n            ‚îî‚îÄ‚îÄ process_source_data.py\n\n\n\nfolder_prefix: boller\nVerdien for folder_prefix tilsvarer en ‚Äúfil sti‚Äù i kildedatab√∏tte. I dette tilfellet vil nye filer lagt til under boller/ trigge en prosessering.\n\n\n\nfolder_prefix: rundstykker"
  },
  {
    "objectID": "kildedata-prosessering.html#skrive-prosesseringsscriptet",
    "href": "kildedata-prosessering.html#skrive-prosesseringsscriptet",
    "title": "Kildedata prosessering",
    "section": "",
    "text": "Med prosesseringsscriptet mener vi filen process_source_data.py som ligger i en mappe per kilde under automation/source_data. Dette scriptet vil bli kalt hver gang det blir lagt til en ny fil i kildedatab√∏tten. Metodesignaturen ser slik ut:\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\nDette betyr at hvis f.eks. 10 filer blir lagt til i kildedatab√∏tten samtidig s√• vil det startes opp 10 individuelle Python-prosesser som kaller denne main-metoden med forskjellig file_path. Parameteren file_path vil inneholde hele filstien inkl. filnavn. S√• en enkel flytteoperasjon fra kildedatab√∏tten til inndateb√∏tten (uten noen form for konvertering) vil kunne uttrykkes slik:\nimport dapla as dp\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n    source_bucket_name = \"ssb-prod-my-project-data-kilde\"\n    destination_bucket_name = \"ssb-prod-my-project-data-produkt\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    df = dp.read_pandas(file_path)\n    dp.write_pandas(df, destination_bucket_path)\nAlternativt‚Ä¶\nfrom dapla import FileClient\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n    source_bucket_name = \"ssb-prod-my-project-data-kilde\"\n    destination_bucket_name = \"ssb-prod-my-project-data-produkt\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    fs = FileClient.get_gcs_file_system()\n    fs.copy(file_path, destination_path)\n\n\nDet anbefales √• bruke Pythons logging modul for √• logge, og ikke bruke print eller skrive til stdout/stderr. Det er satt opp en standard logger-konfigurasjon som skriver informasjonsmeldinger (log level info) til stdout og feilmeldinger (log level warning eller error) til stderr. Feil som ikke blir h√•ndtert blir automatisk fanget opp og logget av automatiseringsl√∏sningen. Eksemplet nedenfor logger en informasjonsmelding, en advarsel og en feilmelding:\nimport logging\nlogging.info('Til info')\nlogging.warning('Advarsel!')\nlogging.error('En feil oppstod!')"
  },
  {
    "objectID": "kildedata-prosessering.html#footnotes",
    "href": "kildedata-prosessering.html#footnotes",
    "title": "Kildedata prosessering",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPersonidentifiserende Informasjon‚Ü©Ô∏é\nPersonidentifiserende Informasjon‚Ü©Ô∏é"
  },
  {
    "objectID": "dashboard.html",
    "href": "dashboard.html",
    "title": "Dash og dashboard",
    "section": "",
    "text": "Hvis en √∏nsker √• lage ett dashbord som et brukergrensesnitt, s√• kan pakken Dash v√¶re et godt alternativ. Dash er ett rammeverk hvor man selv kan bygge opp applikasjoner i form av dashbord p√• en enklere m√•te, og det bygges opp√• javascript pakker som plotly.js og react.js. Det er et produkt ved siden av og helintegrert med plotly, som ogs√• er en annen pakke i Python som gir oss interaktive grafer. Dash er et godt verkt√∏y hvis en √∏nsker et dashbord som brukergrensesnitt for interaktiv visualisering av data. Dash kan kodes i Python og R, men ogs√• Julia og F#.\n\n\nI SSB kan man lage dashbord i virtuelle milj√∏er, gjerne satt opp med ssb-project, men man trenger helt spesifikke pakker for √• f√• det oppe √• g√•. Mer info om √• sette opp et eget milj√∏ med ssb-project finner du her. Tabell under viser navn p√• pakkene, og gjeldende versjoner som er benyttet i skrivende stund av eksempel i neste avsnitt. Vel og merke kan nyere versjoner ogs√• fungere fint, noe man m√• pr√∏ve ut selv, men f√∏lgende versjoner fungerer fint.\n\n\n\nPakke\nVersjon (i skrivende stund)\n\n\n\n\ndash\n2.8.1\n\n\njupyter-dash\n0.4.2\n\n\njupyter-server-proxy\n3.2.2\n\n\njupyterlab-dash\n0.1.0a3\n\n\nipykernel\n6.21.3\n\n\ndash-bootstrap-components\n1.3.0 (hvis √∏nskelig)\n\n\n\nFor mer om h√•ndtering av pakker i ett virtuelt milj√∏ satt opp med ssb-project kan man se n√¶rmere her. For √• legge til disse pakkene kan man gj√∏re f√∏lgende i terminalen:\npoetry add dash\npoetry add jupyter-dash\npoetry add jupyter-server-proxy\npoetry add jupyterlab-dash\npoetry add ipykernel\nOg hvis en √∏nsker Dash-Bootstrap-Components:\npoetry add dash-bootstrap-components\nVel og merke s√• vil ikke denne pakken fungere uten at tilh√∏rende filer er med i selve repoet. Dette fordi i hovedsak leter denne etter filer den trenger p√• internett. Pakken i seg selv har en fordel i at det er lettere √• bygge opp utseende(layout) i dashbordet ditt, samt andre komponenter som ikke ligger i standard dash pakken.\n\n\n\nNoen ting er viktig √• huske p√• at kommer i korrekt rekkef√∏lge n√•r en koder opp i JupyterLab. Her kommer ett veldig enkelt eksempel.\nF√∏rste celle importerer vi alle n√∏dvendige pakker\nfrom dash import html\nfrom jupyter_dash import JupyterDash\nfrom jupyter_dash.comms import _send_jupyter_config_comm_request\n_send_jupyter_config_comm_request()\nI Andre celle m√• f√∏lgende kj√∏res, men her er det VELDIG VIKTIG at man avventer 2-3 sekunder etter at forrige celle ble kj√∏rt.\nJupyterDash.infer_jupyter_proxy_config()\nDeretter s√• er vi klare for √• bygge opp selve dashbordet. s√• i Tredje celle kan en enkel kode for eksempel se slik ut:\napp = JupyterDash(__name__)\nporten = 8642 # Valgfritt fire sifret nummer\n\napp.layout = html.Div([\n    html.H1(\"Eget dashbord med Dash i SSB\")\n])\n\nif __name__ == \"__main__\":\n    app.run_server(debug=True, port=porten, mode=\"external\")\nDenne koden vil starte opp dashbordet i eget vindu i browseren ettersom mode=‚Äúexternal‚Äù. Dersom man vil ha et eget vindu i JupyterLab kan man sette denne til ‚Äújupyterlab‚Äù, eller hvis du vil at dashbordet skal dukke opp under cellen i notebooken din, s√• kan man sette denne til ‚Äúinline‚Äù.\n\n\n\nDiverse som er verdt √• se n√¶rmere p√• n√•r en bygger dashbord applikasjon med Dash. Det f√∏lger med mange gode eksempler for bruk av diverse komponenter i dokumentasjonene under her.\n\nStartsiden til Dash\nLage interaktive grafer i Python med Plotly\nDash Core Components\nDash HTML Components\nDash Bootstrap Components\n\nVel og merke henter denne diverse materialer fra internett, og vil ikke fungere uten tilgang. Det er heller anbefalt √• ha de n√∏dvendige filene lagret lokalt for bruk av denne pakken."
  },
  {
    "objectID": "dashboard.html#anbefalte-n√∏dvendige-pakker",
    "href": "dashboard.html#anbefalte-n√∏dvendige-pakker",
    "title": "Dash og dashboard",
    "section": "",
    "text": "I SSB kan man lage dashbord i virtuelle milj√∏er, gjerne satt opp med ssb-project, men man trenger helt spesifikke pakker for √• f√• det oppe √• g√•. Mer info om √• sette opp et eget milj√∏ med ssb-project finner du her. Tabell under viser navn p√• pakkene, og gjeldende versjoner som er benyttet i skrivende stund av eksempel i neste avsnitt. Vel og merke kan nyere versjoner ogs√• fungere fint, noe man m√• pr√∏ve ut selv, men f√∏lgende versjoner fungerer fint.\n\n\n\nPakke\nVersjon (i skrivende stund)\n\n\n\n\ndash\n2.8.1\n\n\njupyter-dash\n0.4.2\n\n\njupyter-server-proxy\n3.2.2\n\n\njupyterlab-dash\n0.1.0a3\n\n\nipykernel\n6.21.3\n\n\ndash-bootstrap-components\n1.3.0 (hvis √∏nskelig)\n\n\n\nFor mer om h√•ndtering av pakker i ett virtuelt milj√∏ satt opp med ssb-project kan man se n√¶rmere her. For √• legge til disse pakkene kan man gj√∏re f√∏lgende i terminalen:\npoetry add dash\npoetry add jupyter-dash\npoetry add jupyter-server-proxy\npoetry add jupyterlab-dash\npoetry add ipykernel\nOg hvis en √∏nsker Dash-Bootstrap-Components:\npoetry add dash-bootstrap-components\nVel og merke s√• vil ikke denne pakken fungere uten at tilh√∏rende filer er med i selve repoet. Dette fordi i hovedsak leter denne etter filer den trenger p√• internett. Pakken i seg selv har en fordel i at det er lettere √• bygge opp utseende(layout) i dashbordet ditt, samt andre komponenter som ikke ligger i standard dash pakken."
  },
  {
    "objectID": "dashboard.html#eksempel-kode-i-jupyterlab",
    "href": "dashboard.html#eksempel-kode-i-jupyterlab",
    "title": "Dash og dashboard",
    "section": "",
    "text": "Noen ting er viktig √• huske p√• at kommer i korrekt rekkef√∏lge n√•r en koder opp i JupyterLab. Her kommer ett veldig enkelt eksempel.\nF√∏rste celle importerer vi alle n√∏dvendige pakker\nfrom dash import html\nfrom jupyter_dash import JupyterDash\nfrom jupyter_dash.comms import _send_jupyter_config_comm_request\n_send_jupyter_config_comm_request()\nI Andre celle m√• f√∏lgende kj√∏res, men her er det VELDIG VIKTIG at man avventer 2-3 sekunder etter at forrige celle ble kj√∏rt.\nJupyterDash.infer_jupyter_proxy_config()\nDeretter s√• er vi klare for √• bygge opp selve dashbordet. s√• i Tredje celle kan en enkel kode for eksempel se slik ut:\napp = JupyterDash(__name__)\nporten = 8642 # Valgfritt fire sifret nummer\n\napp.layout = html.Div([\n    html.H1(\"Eget dashbord med Dash i SSB\")\n])\n\nif __name__ == \"__main__\":\n    app.run_server(debug=True, port=porten, mode=\"external\")\nDenne koden vil starte opp dashbordet i eget vindu i browseren ettersom mode=‚Äúexternal‚Äù. Dersom man vil ha et eget vindu i JupyterLab kan man sette denne til ‚Äújupyterlab‚Äù, eller hvis du vil at dashbordet skal dukke opp under cellen i notebooken din, s√• kan man sette denne til ‚Äúinline‚Äù."
  },
  {
    "objectID": "dashboard.html#aktuell-dokumentasjon",
    "href": "dashboard.html#aktuell-dokumentasjon",
    "title": "Dash og dashboard",
    "section": "",
    "text": "Diverse som er verdt √• se n√¶rmere p√• n√•r en bygger dashbord applikasjon med Dash. Det f√∏lger med mange gode eksempler for bruk av diverse komponenter i dokumentasjonene under her.\n\nStartsiden til Dash\nLage interaktive grafer i Python med Plotly\nDash Core Components\nDash HTML Components\nDash Bootstrap Components\n\nVel og merke henter denne diverse materialer fra internett, og vil ikke fungere uten tilgang. Det er heller anbefalt √• ha de n√∏dvendige filene lagret lokalt for bruk av denne pakken."
  },
  {
    "objectID": "arkivering.html",
    "href": "arkivering.html",
    "title": "Arkivering",
    "section": "",
    "text": "Arkivering"
  },
  {
    "objectID": "dapla-team.html",
    "href": "dapla-team.html",
    "title": "Dapla Team",
    "section": "",
    "text": "Dapla Team\nI forrige del beskrev vi noen grunnleggende ting rundt Dapla. I denne delen tar vi for oss hvordan du kan begynne √• jobbe med skarpe data p√• plattformen.\nKapittelet som beskriver hvordan man logger seg inn p√• Dapla vil fungere uten at du m√• gj√∏re noen forberedelser. Er man koblet p√• SSB sitt nettverk s√• vil alle SSB-ansatte kunne g√• inn p√• plattformen og kode i Python og R. Men du f√•r ikke tilgang til SSBs omr√•de for datalagring p√• plattformen. I praksis vil det si at man kan generere data med kode, men man kan ikke jobbe med skarpe data.\nFor √• f√• muligheten til √• jobbe med skarpe data M√Ö du f√∏rst opprette et dapla-team. Dette er det f√∏rste naturlige steget √• ta n√•r man skal begynne √• jobbe med statistikkproduksjon p√• Dapla. I dette kapittelet vil vi forklare det du trenger √• vite om det √• opprette og jobbe innenfor et team."
  },
  {
    "objectID": "systemoversikt.html",
    "href": "systemoversikt.html",
    "title": "Systemoversikt",
    "section": "",
    "text": "Systemoversikt\nHvilke komponenter er plattformen bygd opp p√•? Forklart p√• lettest mulig m√•te."
  },
  {
    "objectID": "gjenopprette-data.html",
    "href": "gjenopprette-data.html",
    "title": "Gjenopprette data fra b√∏tter",
    "section": "",
    "text": "Alle b√∏tter har automatisk versjonering. Dette gj√∏r det mulig √• tilbakef√∏re filer til en tidligere versjon eller gjenopprette filer som er slettet ved et uhell.\nLogg inn p√• Google Cloud Console og s√∏k opp ‚ÄúCloud Storage‚Äù i s√∏kefeltet. Klikk p√• den b√∏tten hvor filen er lagret under ‚ÄúBuckets‚Äù.\n\n\nFra Cloud Storage skjermbildet kan man navigere seg frem til den mappen hvor filen tidligere er lagret og skru p√• radioknappen ‚ÄúShow deleted data‚Äù (Figur¬†1)\n\n\n\nFigur¬†1: Skru p√• visning av slettede filer\n\n\nN√• vil man kunne se slettede filer i kursiv med teksten (Deleted) p√• slutten. Kolonnen ‚ÄúVersion history‚Äù vil ogs√• vise hvor mange tidligere versjoner som finnes av denne filen. Trykk p√• filnavnet du √∏nsker √• gjenopprette og velg deretter fanen ‚ÄúVersion history‚Äù. I listen av versjoner til denne filen har man mulighet til √• gjenopprette til en tidligere versjon ved √• klikke p√• ‚ÄúRestore‚Äù (Figur¬†2).\n\n\n\nFigur¬†2: Gjenoppretting av en slettet fil\n\n\n\n\n\nFra Cloud Storage skjermbildet kan man navigere seg frem til den mappen hvor filen er lagret, og trykke p√• filnavnet. Velg deretter fanen ‚ÄúVersion history‚Äù. I listen av versjoner til denne filen har man mulighet til √• gjenopprette til en tidligere versjon ved √• klikke p√• ‚ÄúRestore‚Äù (Figur¬†3).\n\n\n\nFigur¬†3: Versjonshistorikk til en fil"
  },
  {
    "objectID": "gjenopprette-data.html#gjenopprette-en-slettet-fil",
    "href": "gjenopprette-data.html#gjenopprette-en-slettet-fil",
    "title": "Gjenopprette data fra b√∏tter",
    "section": "",
    "text": "Fra Cloud Storage skjermbildet kan man navigere seg frem til den mappen hvor filen tidligere er lagret og skru p√• radioknappen ‚ÄúShow deleted data‚Äù (Figur¬†1)\n\n\n\nFigur¬†1: Skru p√• visning av slettede filer\n\n\nN√• vil man kunne se slettede filer i kursiv med teksten (Deleted) p√• slutten. Kolonnen ‚ÄúVersion history‚Äù vil ogs√• vise hvor mange tidligere versjoner som finnes av denne filen. Trykk p√• filnavnet du √∏nsker √• gjenopprette og velg deretter fanen ‚ÄúVersion history‚Äù. I listen av versjoner til denne filen har man mulighet til √• gjenopprette til en tidligere versjon ved √• klikke p√• ‚ÄúRestore‚Äù (Figur¬†2).\n\n\n\nFigur¬†2: Gjenoppretting av en slettet fil"
  },
  {
    "objectID": "gjenopprette-data.html#gjenopprette-en-fil-til-en-tidligere-versjon",
    "href": "gjenopprette-data.html#gjenopprette-en-fil-til-en-tidligere-versjon",
    "title": "Gjenopprette data fra b√∏tter",
    "section": "",
    "text": "Fra Cloud Storage skjermbildet kan man navigere seg frem til den mappen hvor filen er lagret, og trykke p√• filnavnet. Velg deretter fanen ‚ÄúVersion history‚Äù. I listen av versjoner til denne filen har man mulighet til √• gjenopprette til en tidligere versjon ved √• klikke p√• ‚ÄúRestore‚Äù (Figur¬†3).\n\n\n\nFigur¬†3: Versjonshistorikk til en fil"
  },
  {
    "objectID": "bakkesystemer.html",
    "href": "bakkesystemer.html",
    "title": "Bakkesystemer",
    "section": "",
    "text": "Bakkesystemer"
  },
  {
    "objectID": "contribution.html",
    "href": "contribution.html",
    "title": "Bidra til Dapla-manualen",
    "section": "",
    "text": "Alle i SSB kan bidra til denne manualen. Endringer m√• godkjennes av noen i Team Statistikktjenester, si gjerne i fra at det ligger en PR √• se p√•.\n\n\n\n\n\n\nWarning\n\n\n\nDenne nettsiden er √•pen og hvem som helst kan lese det som er skrevet her. Hold det i tankene n√•r du skriver.\n\n\n\n\n\nMan trenger basis git kompetanse, det ligger en fin beskrivelse av det p√• Beste Praksis siden fra KVAKK.\nMan trenger en konto p√• Github, det kan man opprette ved √• f√∏lge instruksjonene her.\nMan kan l√¶re seg Markdown (enkel tekst formatering), en fin oversikt finnes her.\nVerkt√∏yet Quarto burde installeres for √• kunne se endringene slik som de ser ut p√• nettsiden. Installasjon instruksjoner finnes her.\n\n\n\n\n\nKlone repoet git clone https://github.com/statisticsnorway/dapla-manual.git\nLage en ny gren\nGj√∏re endringen\nKj√∏r quarto preview dapla-manual og f√∏lge lenken for √• sjekke at alt ser bra ut p√• nettsiden\n√Öpne en PR\nBe noen √• gjennomg√• endringen og eventuelt godkjenne den\nMerge PRen\nEtter et par minutter skal din endring v√¶re synlig!\n\n\n\nQuarto tilbyr √• legge ved (embed) notebooks inn i nettsiden. Dette er en fin m√•te √• dele kode og output p√•. Men det krever at vi tenker gjennom hvor output`en genereres. Siden Dapla-manualen renderes med GitHub-action, s√• √∏nsker vi ikke √• introdusere kompleksiteten det inneb√¶rer √• generere output fra kode her. I tillegg er det mange milj√∏-spesifikke konfigurasjoner som bestemmer hvordan output kan genereres. Derfor anbefaler vi f√∏lgende tiln√¶rming n√•r man skal legge ved en notebook til et kapittel:\n\nLogg deg inn i milj√∏et du √∏nsker √• bruke.\nKlone ned repoet til Dapla-manualen.\nOpprett en notebook i mappen ./dapla-manual/notebooks/notebook-navn.ipynb\nSkriv kode og generer output som du √∏nsker. Husk √• bare bruke data som alle har tilgang til eller de kan generere selv.\nEmbed output i kapitlet du √∏nsker iht til denne beskrivelsen\nP√• toppen av notebooken lager du en raw-celle der du skriver:\n\n---\nfreeze: true\n---\n\nKj√∏r denne notebooken fra terminalen med kommandoen:\n\nquarto render ./notebooks/&lt;notebook-navn.ipynb&gt; --execute\nDette sikrer at vi kan se output etter at boken bygges i GitHub Actions. Husk at notebooks ikke blir strippet for output p√• vanlig m√•te, slik at kun √•pne data skal benyttes.\nSp√∏r Team Statistikktjenester om du lurer p√• noe."
  },
  {
    "objectID": "contribution.html#forutsetninger",
    "href": "contribution.html#forutsetninger",
    "title": "Bidra til Dapla-manualen",
    "section": "",
    "text": "Man trenger basis git kompetanse, det ligger en fin beskrivelse av det p√• Beste Praksis siden fra KVAKK.\nMan trenger en konto p√• Github, det kan man opprette ved √• f√∏lge instruksjonene her.\nMan kan l√¶re seg Markdown (enkel tekst formatering), en fin oversikt finnes her.\nVerkt√∏yet Quarto burde installeres for √• kunne se endringene slik som de ser ut p√• nettsiden. Installasjon instruksjoner finnes her."
  },
  {
    "objectID": "contribution.html#fremgangsm√•ten",
    "href": "contribution.html#fremgangsm√•ten",
    "title": "Bidra til Dapla-manualen",
    "section": "",
    "text": "Klone repoet git clone https://github.com/statisticsnorway/dapla-manual.git\nLage en ny gren\nGj√∏re endringen\nKj√∏r quarto preview dapla-manual og f√∏lge lenken for √• sjekke at alt ser bra ut p√• nettsiden\n√Öpne en PR\nBe noen √• gjennomg√• endringen og eventuelt godkjenne den\nMerge PRen\nEtter et par minutter skal din endring v√¶re synlig!\n\n\n\nQuarto tilbyr √• legge ved (embed) notebooks inn i nettsiden. Dette er en fin m√•te √• dele kode og output p√•. Men det krever at vi tenker gjennom hvor output`en genereres. Siden Dapla-manualen renderes med GitHub-action, s√• √∏nsker vi ikke √• introdusere kompleksiteten det inneb√¶rer √• generere output fra kode her. I tillegg er det mange milj√∏-spesifikke konfigurasjoner som bestemmer hvordan output kan genereres. Derfor anbefaler vi f√∏lgende tiln√¶rming n√•r man skal legge ved en notebook til et kapittel:\n\nLogg deg inn i milj√∏et du √∏nsker √• bruke.\nKlone ned repoet til Dapla-manualen.\nOpprett en notebook i mappen ./dapla-manual/notebooks/notebook-navn.ipynb\nSkriv kode og generer output som du √∏nsker. Husk √• bare bruke data som alle har tilgang til eller de kan generere selv.\nEmbed output i kapitlet du √∏nsker iht til denne beskrivelsen\nP√• toppen av notebooken lager du en raw-celle der du skriver:\n\n---\nfreeze: true\n---\n\nKj√∏r denne notebooken fra terminalen med kommandoen:\n\nquarto render ./notebooks/&lt;notebook-navn.ipynb&gt; --execute\nDette sikrer at vi kan se output etter at boken bygges i GitHub Actions. Husk at notebooks ikke blir strippet for output p√• vanlig m√•te, slik at kun √•pne data skal benyttes.\nSp√∏r Team Statistikktjenester om du lurer p√• noe."
  },
  {
    "objectID": "bakke-til-sky.html",
    "href": "bakke-til-sky.html",
    "title": "Fra bakke til sky",
    "section": "",
    "text": "Fra bakke til sky"
  },
  {
    "objectID": "automatisering.html",
    "href": "automatisering.html",
    "title": "Automatisering",
    "section": "",
    "text": "For √• redusere tilgang til PII1 oppfordres alle Dapla-team til √• ha en automatisert prosessering av kildedata. Dapla tilbyr dette som en 100% selvbetjent l√∏sning. Kildedata (Standardutvalget 2021, 5) prosesseres til inndata gjennom et gitt sett av operasjoner(se Figur¬†1). Kildedataprosesseringsl√∏sningen som tilbys p√• Dapla er bygget p√• at hver kildedatafil behandles individuelt. Mer komplekse operasjoner som g√•r p√• tvers av flere filer b√∏r utf√∏res p√• inndata eller senere datatilstander.\n\n\n\n\n\n\nDet er kun teamets kildedataansvarlige som skal aksessere kildedata.\n\n\n\n\n\n\n\n\n\nTeamets kildedataansvarlige tar ansvar for √• prosessere kildedata til inndata p√• en forsvarlig m√•te.\n\n\n\n\n\n\n\n\nFigur¬†1: Operasjoner som inng√•r i kildedataprosessering\n\n\n\nInndata er kildedata som er transformert til SSBs standard lagringsformat. Variabelnavn og -innhold er uendret bortsett fra at:\n\nDirekte identifiserende variabler (f.eks. f√∏dselsnummer) er pseudonymisert\nTegnsett, datoformat, adresse m.m. er endret til SSBs standardformat\nDet benyttes standard kodeverk (Klass) der det er mulig (f.eks. kj√∏nn)\nDataene er minimert slik at kun variablene som er n√∏dvendige i den videre produksjonsprosessen inng√•r.\n\n\n(Standardutvalget 2021, 8)\nVed transformasjon fra kildedata til inndata er det ikke anbefalt √• gjennomf√∏re operasjoner som:\n\nG√•r p√• tvers av flere filer\nLegger til nye felt\nEndrer navn p√• felt\nAggregerer data\n\n\n\n\n\n\nAutomatiseringsl√∏sningen krever at teamets Google-prosjekt kan lese fra teamets Infrastructure as Code (IaC) repo p√• Github. F√∏lg instruksjonene her for √• sette opp dette. Dette er en engangsjobb som m√• gj√∏res av en som har administratortilgang til IaC-repoet.\n\n\n\n\n\nKilder konfigureres i teamets Infrastructure as Code (IaC) repo p√• Github. Det kan finnes basert p√• f√∏lgende formulering: github.com/statisticsnorway/&lt;teamnavn&gt;-iac. Kilder konfigureres under stien automation/source_data/prod p√• repoet.\n\n\n\n\n\n\nHer referer vi til filstiene som automation/source_data/prod, men under testing burde man alltid jobbe i staging-milj√∏et med automation/source_data/staging.\n\n\n\n\n\n\nHver kilde konfigureres ved hjelp av to filer:\n\nconfig.yaml som konfigurer hvilken filsti som skal prosesseres i teamets kildedatab√∏tte. Eksempel:\n\nfolder_prefix: source-folder/2022\n\nprocess_source_data.py som kj√∏res n√•r en kildedatafil blir prosessert. Her m√• man skrive en python-funksjon med en bestemt metodesignatur som ser slik ut:\n\n def main(source_file):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         source_file: fully qualified file name of the source data file.\n         Example: gs://ssb-prod-my-project-data-kilde/source-folder/2022/data.xml\n     \"\"\"\nDisse filene m√• legges til i en mappe per kilde under automation/source_data/prod i IaC repoet. Se eksemplet under for en detaljert forklaring.\n\n\n\n\n\n\nMappenavnet for kilder i IaC repoene blir brukt som navn p√• ressurser. Dette betyr at de enesete tillatte tegnene i mappenavnet er bokstaver, tall, bindestrek og underscore. Det er ikke tillatt med mellomrom eller andre spesialtegn. Mappenavnet kan best√• av opptil 20 tegn.\n\n\n\n\n\n\nDette avhenger av om prosesseringsscriptet kan behandle alle kildefilene p√• samme m√•te, eller om det vil v√¶re variasjoner som gj√∏r at prosesseringen b√∏r splittes opp i uavhengige prosesseringsscript.\nGrunner til √• differensiere mellom kilder kan v√¶re:\n\nKildedatafilene har forskjellig filformat (f.eks xml eller json)\nKildedataene har ulike felter\nKildedataene inneholder PII2 eller ikke\n\n\n\n\n\n\n\n\n\n\nDisse instruksjonene forutsetter at ditt Google-prosjekt er koblet til Github.\n\n\n\n\nOpprette skriptet process_source_data.py som prosesserer kildedatafilen til inndata. Dette kan testes av kildedataansvarlige manuelt p√• Jupyter for √• verifisere at dataene blir prosessert som √∏nsket.\nI en branch i teamets IaC repo, legg til filene config.yaml og process_source_data.py i en ny mappe (valgfritt navn) under automation/source_data/prod. Se eksemplet under for en detaljert forklaring av formatet.\nLag en PR p√• branchen og f√• den godkjent av kildedataansvarlige.\nVent til alle tester er ferdige. Det skal st√• ‚ÄúAll checks have passed‚Äù f√∏r man g√•r videre, hvis testene feiler f√∏lg stegene her. \nSkrive atlantis apply i en kommentar p√• PRen for √• opprette det n√∏dvendige infrastruktur for √• prosessere kilden.\nMerge PRen.\nSjekk resultatet av det automatiske bygget.\nVerifisere at nye filer lagt i kildedatab√∏tten blir prosessert til inndata som forventet.\n\n\n\n\n\nLa oss si at et team (sm√•bakst) har to datakilder levert av ulik dataeiere p√• ulik formater. Den ene er om boller og er p√• csv format og den andre er om rundstykker og er p√• json format. Kildedataansvarlige i teamet bestemmer seg for at filene i boller/ er like nok √• prosesseres som en kilde, og at filene i rundstykker/ kan prosesseres som en annen kilde.\n\n\nssb-prod-smaabakst-data-kilde\n‚îú‚îÄ‚îÄ boller\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ hvetebolle\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 2018-salg.csv\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 2019-salg.csv\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ...\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ kanelbolle\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 2018-salg.csv\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 2019-salg.csv\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ...\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ skolebolle\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 2018-salg.csv\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 2019-salg.csv\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ ...\n‚îî‚îÄ‚îÄ rundstykker\n    ‚îú‚îÄ‚îÄ haandverker\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ apr-2022-resultater.json\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ aug-2022-resultater.json\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ...\n    ‚îî‚îÄ‚îÄ havre\n        ‚îú‚îÄ‚îÄ apr-2022-resultater.json\n        ‚îú‚îÄ‚îÄ aug-2022-resultater.json\n        ‚îú‚îÄ‚îÄ ...\n\n\n\nsmaabakst-iac\n‚îî‚îÄ‚îÄ automation\n    ‚îî‚îÄ‚îÄ source_data\n        ‚îî‚îÄ‚îÄ prod\n            ‚îú‚îÄ‚îÄ boller\n            ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ config.yaml\n            ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ process_source_data.py\n            ‚îî‚îÄ‚îÄ rundstykker\n                ‚îú‚îÄ‚îÄ config.yaml\n                ‚îî‚îÄ‚îÄ process_source_data.py\n\n\n\nfolder_prefix: boller\nVerdien for folder_prefix tilsvarer en ‚Äúfil sti‚Äù i kildedatab√∏tte. I dette tilfellet vil nye filer lagt til under boller/ trigge en prosessering.\n\n\n\nfolder_prefix: rundstykker\n\n\n\n\n\nMed prosesseringsscriptet mener vi filen process_source_data.py som ligger i en mappe per kilde under automation/source_data/prod. Dette scriptet vil bli kalt hver gang det blir lagt til en ny fil i kildedatab√∏tten.\nDette betyr at hvis f.eks. 10 filer blir lagt til i kildedatab√∏tten samtidig s√• vil det startes opp 10 individuelle Python-prosesser som kaller denne main-metoden med forskjellig source_file. Parameteren source_file vil inneholde hele filstien inkl. filnavn. S√• en enkel flytteoperasjon fra kildedatab√∏tten til inndateb√∏tten (uten noen form for konvertering) vil kunne uttrykkes slik:\nimport dapla as dp\n\ndestination_bucket_name = \"ssb-prod-smaabakst-data-produkt\"\n\n def main(source_file):\n    # source_file er f.eks: gs://ssb-prod-smaabakst-data-kilde/boller/hveteboller/2018-salg.csv\n    df = dp.read_pandas(source_file,  file_format=\"csv\")\n\n    # Eksempel p√• konvertering fra xml til parquet-format\n    dp.write_pandas(df, f\"gs://{destination_bucket_name}/inndata/boller/hveteboller/2018-salg.parquet\")\nAlternativt (uten noen form for konvertering)‚Ä¶\nfrom dapla import FileClient\n\nsource_bucket_name = \"ssb-prod-smaabakst-data-kilde\"\ndestination_bucket_name = \"ssb-prod-smaabakst-data-produkt\"\n\n def main(source_file):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         source_file: fully qualified file name of the source data file.\n     \"\"\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    fs = FileClient.get_gcs_file_system()\n    fs.copy(file_path, destination_path)\n\n\nDet anbefales √• bruke Pythons logging modul for √• logge, og ikke bruke print eller skrive til stdout/stderr. Det er satt opp en standard logger-konfigurasjon som skriver informasjonsmeldinger (log level info) til stdout og feilmeldinger (log level warning eller error) til stderr. Feil som ikke blir h√•ndtert blir automatisk fanget opp og logget av automatiseringsl√∏sningen. Eksemplet nedenfor logger en informasjonsmelding, en advarsel og en feilmelding:\nimport logging\nlogging.info('Til info')\nlogging.warning('Advarsel!')\nlogging.error('En feil oppstod!')"
  },
  {
    "objectID": "automatisering.html#operasjoner-som-inng√•r-i-kildedataprosessering",
    "href": "automatisering.html#operasjoner-som-inng√•r-i-kildedataprosessering",
    "title": "Automatisering",
    "section": "",
    "text": "Figur¬†1: Operasjoner som inng√•r i kildedataprosessering\n\n\n\nInndata er kildedata som er transformert til SSBs standard lagringsformat. Variabelnavn og -innhold er uendret bortsett fra at:\n\nDirekte identifiserende variabler (f.eks. f√∏dselsnummer) er pseudonymisert\nTegnsett, datoformat, adresse m.m. er endret til SSBs standardformat\nDet benyttes standard kodeverk (Klass) der det er mulig (f.eks. kj√∏nn)\nDataene er minimert slik at kun variablene som er n√∏dvendige i den videre produksjonsprosessen inng√•r.\n\n\n(Standardutvalget 2021, 8)\nVed transformasjon fra kildedata til inndata er det ikke anbefalt √• gjennomf√∏re operasjoner som:\n\nG√•r p√• tvers av flere filer\nLegger til nye felt\nEndrer navn p√• felt\nAggregerer data"
  },
  {
    "objectID": "automatisering.html#ta-tjenesten-i-bruk",
    "href": "automatisering.html#ta-tjenesten-i-bruk",
    "title": "Automatisering",
    "section": "",
    "text": "Automatiseringsl√∏sningen krever at teamets Google-prosjekt kan lese fra teamets Infrastructure as Code (IaC) repo p√• Github. F√∏lg instruksjonene her for √• sette opp dette. Dette er en engangsjobb som m√• gj√∏res av en som har administratortilgang til IaC-repoet.\n\n\n\n\n\nKilder konfigureres i teamets Infrastructure as Code (IaC) repo p√• Github. Det kan finnes basert p√• f√∏lgende formulering: github.com/statisticsnorway/&lt;teamnavn&gt;-iac. Kilder konfigureres under stien automation/source_data/prod p√• repoet.\n\n\n\n\n\n\nHer referer vi til filstiene som automation/source_data/prod, men under testing burde man alltid jobbe i staging-milj√∏et med automation/source_data/staging.\n\n\n\n\n\n\nHver kilde konfigureres ved hjelp av to filer:\n\nconfig.yaml som konfigurer hvilken filsti som skal prosesseres i teamets kildedatab√∏tte. Eksempel:\n\nfolder_prefix: source-folder/2022\n\nprocess_source_data.py som kj√∏res n√•r en kildedatafil blir prosessert. Her m√• man skrive en python-funksjon med en bestemt metodesignatur som ser slik ut:\n\n def main(source_file):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         source_file: fully qualified file name of the source data file.\n         Example: gs://ssb-prod-my-project-data-kilde/source-folder/2022/data.xml\n     \"\"\"\nDisse filene m√• legges til i en mappe per kilde under automation/source_data/prod i IaC repoet. Se eksemplet under for en detaljert forklaring.\n\n\n\n\n\n\nMappenavnet for kilder i IaC repoene blir brukt som navn p√• ressurser. Dette betyr at de enesete tillatte tegnene i mappenavnet er bokstaver, tall, bindestrek og underscore. Det er ikke tillatt med mellomrom eller andre spesialtegn. Mappenavnet kan best√• av opptil 20 tegn.\n\n\n\n\n\n\nDette avhenger av om prosesseringsscriptet kan behandle alle kildefilene p√• samme m√•te, eller om det vil v√¶re variasjoner som gj√∏r at prosesseringen b√∏r splittes opp i uavhengige prosesseringsscript.\nGrunner til √• differensiere mellom kilder kan v√¶re:\n\nKildedatafilene har forskjellig filformat (f.eks xml eller json)\nKildedataene har ulike felter\nKildedataene inneholder PII2 eller ikke\n\n\n\n\n\n\n\n\n\n\nDisse instruksjonene forutsetter at ditt Google-prosjekt er koblet til Github.\n\n\n\n\nOpprette skriptet process_source_data.py som prosesserer kildedatafilen til inndata. Dette kan testes av kildedataansvarlige manuelt p√• Jupyter for √• verifisere at dataene blir prosessert som √∏nsket.\nI en branch i teamets IaC repo, legg til filene config.yaml og process_source_data.py i en ny mappe (valgfritt navn) under automation/source_data/prod. Se eksemplet under for en detaljert forklaring av formatet.\nLag en PR p√• branchen og f√• den godkjent av kildedataansvarlige.\nVent til alle tester er ferdige. Det skal st√• ‚ÄúAll checks have passed‚Äù f√∏r man g√•r videre, hvis testene feiler f√∏lg stegene her. \nSkrive atlantis apply i en kommentar p√• PRen for √• opprette det n√∏dvendige infrastruktur for √• prosessere kilden.\nMerge PRen.\nSjekk resultatet av det automatiske bygget.\nVerifisere at nye filer lagt i kildedatab√∏tten blir prosessert til inndata som forventet.\n\n\n\n\n\nLa oss si at et team (sm√•bakst) har to datakilder levert av ulik dataeiere p√• ulik formater. Den ene er om boller og er p√• csv format og den andre er om rundstykker og er p√• json format. Kildedataansvarlige i teamet bestemmer seg for at filene i boller/ er like nok √• prosesseres som en kilde, og at filene i rundstykker/ kan prosesseres som en annen kilde.\n\n\nssb-prod-smaabakst-data-kilde\n‚îú‚îÄ‚îÄ boller\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ hvetebolle\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 2018-salg.csv\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 2019-salg.csv\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ...\n‚îÇ¬†¬† ‚îú‚îÄ‚îÄ kanelbolle\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 2018-salg.csv\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ 2019-salg.csv\n‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ...\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ skolebolle\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 2018-salg.csv\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ 2019-salg.csv\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ ...\n‚îî‚îÄ‚îÄ rundstykker\n    ‚îú‚îÄ‚îÄ haandverker\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ apr-2022-resultater.json\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ aug-2022-resultater.json\n    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ ...\n    ‚îî‚îÄ‚îÄ havre\n        ‚îú‚îÄ‚îÄ apr-2022-resultater.json\n        ‚îú‚îÄ‚îÄ aug-2022-resultater.json\n        ‚îú‚îÄ‚îÄ ...\n\n\n\nsmaabakst-iac\n‚îî‚îÄ‚îÄ automation\n    ‚îî‚îÄ‚îÄ source_data\n        ‚îî‚îÄ‚îÄ prod\n            ‚îú‚îÄ‚îÄ boller\n            ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ config.yaml\n            ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ process_source_data.py\n            ‚îî‚îÄ‚îÄ rundstykker\n                ‚îú‚îÄ‚îÄ config.yaml\n                ‚îî‚îÄ‚îÄ process_source_data.py\n\n\n\nfolder_prefix: boller\nVerdien for folder_prefix tilsvarer en ‚Äúfil sti‚Äù i kildedatab√∏tte. I dette tilfellet vil nye filer lagt til under boller/ trigge en prosessering.\n\n\n\nfolder_prefix: rundstykker"
  },
  {
    "objectID": "automatisering.html#skrive-prosesseringsscriptet",
    "href": "automatisering.html#skrive-prosesseringsscriptet",
    "title": "Automatisering",
    "section": "",
    "text": "Med prosesseringsscriptet mener vi filen process_source_data.py som ligger i en mappe per kilde under automation/source_data/prod. Dette scriptet vil bli kalt hver gang det blir lagt til en ny fil i kildedatab√∏tten.\nDette betyr at hvis f.eks. 10 filer blir lagt til i kildedatab√∏tten samtidig s√• vil det startes opp 10 individuelle Python-prosesser som kaller denne main-metoden med forskjellig source_file. Parameteren source_file vil inneholde hele filstien inkl. filnavn. S√• en enkel flytteoperasjon fra kildedatab√∏tten til inndateb√∏tten (uten noen form for konvertering) vil kunne uttrykkes slik:\nimport dapla as dp\n\ndestination_bucket_name = \"ssb-prod-smaabakst-data-produkt\"\n\n def main(source_file):\n    # source_file er f.eks: gs://ssb-prod-smaabakst-data-kilde/boller/hveteboller/2018-salg.csv\n    df = dp.read_pandas(source_file,  file_format=\"csv\")\n\n    # Eksempel p√• konvertering fra xml til parquet-format\n    dp.write_pandas(df, f\"gs://{destination_bucket_name}/inndata/boller/hveteboller/2018-salg.parquet\")\nAlternativt (uten noen form for konvertering)‚Ä¶\nfrom dapla import FileClient\n\nsource_bucket_name = \"ssb-prod-smaabakst-data-kilde\"\ndestination_bucket_name = \"ssb-prod-smaabakst-data-produkt\"\n\n def main(source_file):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         source_file: fully qualified file name of the source data file.\n     \"\"\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    fs = FileClient.get_gcs_file_system()\n    fs.copy(file_path, destination_path)\n\n\nDet anbefales √• bruke Pythons logging modul for √• logge, og ikke bruke print eller skrive til stdout/stderr. Det er satt opp en standard logger-konfigurasjon som skriver informasjonsmeldinger (log level info) til stdout og feilmeldinger (log level warning eller error) til stderr. Feil som ikke blir h√•ndtert blir automatisk fanget opp og logget av automatiseringsl√∏sningen. Eksemplet nedenfor logger en informasjonsmelding, en advarsel og en feilmelding:\nimport logging\nlogging.info('Til info')\nlogging.warning('Advarsel!')\nlogging.error('En feil oppstod!')"
  },
  {
    "objectID": "automatisering.html#footnotes",
    "href": "automatisering.html#footnotes",
    "title": "Automatisering",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPersonidentifiserende Informasjon‚Ü©Ô∏é\nPersonidentifiserende Informasjon‚Ü©Ô∏é"
  },
  {
    "objectID": "jupyterlab.html",
    "href": "jupyterlab.html",
    "title": "Jupyterlab",
    "section": "",
    "text": "Mer kommer.\n\n\n\nM√• nevne operativsystemet og at noe programvare ligger installert her (git, jwsacruncher, quarto, ++)\n\n\n\nNoe er i base-image, noe b√∏r gj√∏res i virtuelle mil√∏er. Hvordan liste ut pakker som er pre-installert?\n\n\n\nJupyterlab er en samling extension. Kan bare installeres av admin. Sikkerhet. Hvilke extension har vi tilgjengeliggjort?\n\n\n\nSane defaults for Jupyterlab."
  },
  {
    "objectID": "jupyterlab.html#hva-er-jupyterlab",
    "href": "jupyterlab.html#hva-er-jupyterlab",
    "title": "Jupyterlab",
    "section": "",
    "text": "Mer kommer."
  },
  {
    "objectID": "jupyterlab.html#terminalen",
    "href": "jupyterlab.html#terminalen",
    "title": "Jupyterlab",
    "section": "",
    "text": "M√• nevne operativsystemet og at noe programvare ligger installert her (git, jwsacruncher, quarto, ++)"
  },
  {
    "objectID": "jupyterlab.html#pakkeinstallasjoner",
    "href": "jupyterlab.html#pakkeinstallasjoner",
    "title": "Jupyterlab",
    "section": "",
    "text": "Noe er i base-image, noe b√∏r gj√∏res i virtuelle mil√∏er. Hvordan liste ut pakker som er pre-installert?"
  },
  {
    "objectID": "jupyterlab.html#extensions",
    "href": "jupyterlab.html#extensions",
    "title": "Jupyterlab",
    "section": "",
    "text": "Jupyterlab er en samling extension. Kan bare installeres av admin. Sikkerhet. Hvilke extension har vi tilgjengeliggjort?"
  },
  {
    "objectID": "jupyterlab.html#tips-triks",
    "href": "jupyterlab.html#tips-triks",
    "title": "Jupyterlab",
    "section": "",
    "text": "Sane defaults for Jupyterlab."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html",
    "href": "notebooks/spark/pyspark-intro.html",
    "title": "Introduksjon til PySpark",
    "section": "",
    "text": "Apache Spark er et sterkt verkt√∏y som utvider mulighetsrommet for databehandling med R og Python. Kjernefunksjonaliteten ligger i at den lar deg kj√∏re en jobb p√• flere maskiner samtidig, noe som ikke er mulig med klassiske rammeverk som Pandas og Tidyverse. F√∏lgelig er det et rammeverk som blant annet er veldig egnet for √• prosessere store datamengder eller gj√∏re store beregninger. Les om mer om Apache Spark i Dapla-manualen\nI denne notebooken vises noen enkle eksempler p√• hvordan du kan jobbe med data med PySpark, et Python-grensesnitt mot Spark."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#oppsett",
    "href": "notebooks/spark/pyspark-intro.html#oppsett",
    "title": "Introduksjon til PySpark",
    "section": "Oppsett",
    "text": "Oppsett\nN√•r du logger deg inn p√• Dapla kan du velge mellom 2 ferdigoppsatte kernels for √• jobbe med PySpark:\n\nPyspark (local)\nPyspark (k8s cluster)\n\nDen f√∏rste lar deg bruke Spark p√• en enkeltmaskin, mens den andre lar deg distribuere kj√∏ringen p√• mange maskiner avhengig av hvor store jobbene er. I eksemplene under brukes Pyspark (local).\n\n# Importer biblioteker\nimport numpy as np\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import date_format, explode, expr, sequence, sum, avg\nfrom pyspark.sql.types import DateType, DoubleType, StructField, StructType\n\n# Initialierer en SparkSession\nspark = (\n    SparkSession.builder.master(\"local[2]\")\n    .appName(\"Dapla-manual-example\")\n    .getOrCreate()\n)\n\nI koden over importerer vi de bibliotekene vi skal bruke under. Grunnen til at vi importerer pyspark.sql er at dette er at Spark SQL er Apache Spark sin modul for √• jobbe med strukturerte data. Og som navnet tilsier vil det si at vi kan blande Python og SQL i koden v√•r. Vi skal n√¶rmere p√• hvordan man bruke SQL fra PySpark-notebook senere.\nSpark tilbyr ogs√• et eget grensesnitt, Spark UI, for √• monitorere hva som skjer under en SparkSession. Vi kan bruke f√∏lgende kommando for √• f√• opp en lenke til Spark UI i notebooken v√•r:\n\nspark.sparkContext\n\n\n        \n            SparkContext\n\n            Spark UI\n\n            \n              Version\n                v3.3.1\n              Master\n                local[*]\n              AppName\n                pyspark-shell\n            \n        \n        \n\n\nKlikker du p√• Spark UI-lenken s√• tar den deg til dashboard som lar deg monitorere, debugge, optimalisere og forst√• kj√∏ringene dine. Det kan v√¶re et sv√¶rt nyttig verkt√∏y i mange tilfeller."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#generere-data",
    "href": "notebooks/spark/pyspark-intro.html#generere-data",
    "title": "Introduksjon til PySpark",
    "section": "Generere data",
    "text": "Generere data\nVi kan begynne med √• generere en Spark DataFrame med en kolonne som inneholder m√•nedlige datoer for perioden 2000M1-2023M8.\n\n# Genererer m√•nedlige data\ndates_df = spark.range(1).select(\n    explode(\n        sequence(\n            start=expr(\"date '2000-01-01'\"),\n            stop=expr(\"date '2023-08-01'\"),\n            step=expr(\"interval 1 month\"),\n        )\n    ).alias(\"Date\")\n)\ndates_df.show(5)\n\n+----------+\n|      Date|\n+----------+\n|2000-01-01|\n|2000-02-01|\n|2000-03-01|\n|2000-04-01|\n|2000-05-01|\n+----------+\nonly showing top 5 rows\n\n\n\nEn Spark DataFrame er en distribuert samling av data som er organisert inn i kolonner. Siden Spark lar deg distribuere kj√∏ringer p√• flere maskiner, er DataFrames optimalisert for √• kunne splittes opp slik at de kan brukes p√• flere maskiner. Med andre er dette ikke det samme som en Pandas dataframe mange kjenner fra f√∏r.\nOver genererte vi en datokolonne. For √• f√• litt mer data kan vi ogs√• generere 100 kolonner med tidsseriedata og s√• printer vi de 2 f√∏rste av disse:\n\n# Genererer random walk data\nschema = StructType(\n    [StructField(f\"serie{i:02d}\", DoubleType(), True) for i in range(100)]\n)\n\ndata = [\n    tuple((10 + np.random.normal(0, 1, 100)).cumsum().tolist())\n    for _ in range(284)  # 284 months from 2000-01 to 2023-08\n]\n\ndata_df = spark.createDataFrame(data, schema=schema)\n\ndata_df.select(\"serie00\", \"serie01\").show(5)\n\n+------------------+------------------+\n|           serie00|           serie01|\n+------------------+------------------+\n|10.410703377184355| 21.06318801110079|\n|10.509249410154466|  19.5674295298024|\n| 9.618310122060274|17.635805093465642|\n| 9.691112692298294|18.593842915949082|\n| 9.903675228685067|20.012215769058564|\n+------------------+------------------+\nonly showing top 5 rows\n\n\n\nTil slutt kan vi joine de to datasettene sammen og lage noen kolonner som viser √•r, kvartal og m√•ned. Deretter printer vi ut noen av kolonnene med kommandoen show().\n\n\nCode\n# Legger til row index til DataFrame f√∏r join med dates_df\ndata_df = data_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n\n# Joiner de to datasettene\ndf = (\n    dates_df.withColumn(\"row_index\", expr(\"monotonically_increasing_id()\"))\n    .join(data_df, \"row_index\")\n    .drop(\"row_index\")\n)\n\n# Legger til √•r, kvartal og mnd\ndf = df.withColumn(\"Year\", date_format(df.Date, \"yyyy\"))\ndf = df.withColumn(\"Quarter\", expr(\"quarter(Date)\"))\ndf = df.withColumn(\"Month\", date_format(df.Date, \"MM\"))\n\ndf.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows\n\n\n\nOg med det har vi noe data vi kan jobbe med i resten av notebooken."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#skrive-til-parquet",
    "href": "notebooks/spark/pyspark-intro.html#skrive-til-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Skrive til Parquet",
    "text": "Skrive til Parquet\nPySpark tilbyr mange opsjoner ved skriving til parquet-filer som vi kanskje ikke er vant til √• forholde oss til med enklere rammeverk som Pandas. Den enkleste m√•ten √• skrive ut en fil er som f√∏lger:\ndf.write.parquet(\n    \"gs://ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet\"\n)\nDette vil fungere hvis filen ikke finnes fra f√∏r. Hvis den finnes fra f√∏r s√• vil den feile. Grunnen er at vi ikke har spesifisert hva vi √∏nsker at den skal gj√∏re. Vi kan velge mellom overwrite, append, ignore eller errorifexists. Sistnevnte er ogs√• default-oppf√∏rsel hvis du ikke ber den gj√∏re noe annet.\nUnder bruker vi opsjonen overwrite, det vil si at den skriver over en evt eksisterende fil med samme navn.\n\ndf.write.mode(\"overwrite\").parquet(\n    \"gs://ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet\"\n)\n\nVi kan inspisere hva som ble skrevet ved √• liste ut innholder i b√∏tta.\n\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nfs.glob(\"gs://ssb-prod-dapla-felles-data-delt/temp/**\")\n\n['ssb-prod-dapla-felles-data-delt/temp/',\n 'ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet',\n 'ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet/',\n 'ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet/_SUCCESS',\n 'ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet/part-00000-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet',\n 'ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet/part-00001-b32e7299-0590-4b31-bcc2-dc3d58725529-c000.snappy.parquet']\n\n\nHvis denne parquet-filen hadde v√¶rt partisjonert etter en kolonne, s√• ville det v√¶rt egne undermapper med navnestruktur column_name=value som indikerte hva filen er partisjonert p√•. Siden vi her bruker en maskin og har et lite datasett, valgte Spark √• ikke partisjonere."
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#lese-inn-parquet",
    "href": "notebooks/spark/pyspark-intro.html#lese-inn-parquet",
    "title": "Introduksjon til PySpark",
    "section": "Lese inn Parquet",
    "text": "Lese inn Parquet\nApache Spark kan lese inn flere parquet-filer samtidig. Syntaxen er like enkel som den for √• skrive ut.\n\ndf_ts = spark.read.parquet(\n    \"gs://ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet\"\n)\ndf_ts.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie66\", \"serie55\").show(5)\n\n+----------+----+-------+-----+-----------------+-----------------+\n|      Date|Year|Quarter|Month|          serie66|          serie55|\n+----------+----+-------+-----+-----------------+-----------------+\n|2000-01-01|2000|      1|   01|670.2679830025959|562.4312808525777|\n|2000-02-01|2000|      1|   02|675.4233411662802|562.5168447360121|\n|2000-03-01|2000|      1|   03|687.3412458214908|568.6203957584232|\n|2000-04-01|2000|      2|   04|673.1128047244955|557.4633871253379|\n|2000-05-01|2000|      2|   05| 667.513406101114|561.7766450346327|\n+----------+----+-------+-----+-----------------+-----------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#pyspark-og-sql",
    "href": "notebooks/spark/pyspark-intro.html#pyspark-og-sql",
    "title": "Introduksjon til PySpark",
    "section": "PySpark og SQL",
    "text": "PySpark og SQL\nDu kan ogs√• skrive SQL med Spark. For √• skrive SQL m√• vi f√∏rst lage et temporary view. Under kaller vi viewt for tidsserie.\n\ndf.createOrReplaceTempView(\"tidsserie\")\n\nVi kan deretter skrive en SQL-statement som vi √∏nsker √• kj√∏re p√• viewet:\n\nquery = \"SELECT * FROM tidsserie WHERE Year = 2010\"\n\nDeretter kan vi bruke det til √• filtrere datasettet:\n\nresult_df = spark.sql(query)\nresult_df.select(\"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\").show(5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2010-01-01|2010|      1|   01| 11.26910423907778|21.730128215168268|\n|2010-02-01|2010|      1|   02| 8.722783282690738| 17.46851086792347|\n|2010-03-01|2010|      1|   03|10.376873608348605|20.109386343182802|\n|2010-04-01|2010|      2|   04|11.459959305590926|21.995141825523866|\n|2010-05-01|2010|      2|   05|10.441456792180572| 21.25096473981906|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "notebooks/spark/pyspark-intro.html#aggregering",
    "href": "notebooks/spark/pyspark-intro.html#aggregering",
    "title": "Introduksjon til PySpark",
    "section": "Aggregering",
    "text": "Aggregering\nvi kan aggregere opp enten med PySpark-syntax eller SQL-syntax. Under viser vi begge:\n\nfrom pyspark.sql import functions as F\n\n# Assuming df_ts is your DataFrame\naggregated_df = df_ts.groupBy(\"Quarter\").agg(F.sum(\"serie00\").alias(\"Sum\"),\n                                             F.avg(\"serie00\").alias(\"Average\"),\n                                             F.max(\"serie00\").alias(\"Maximum\"))\n\n# Show the result\naggregated_df.show()\n\n+-------+------------------+------------------+------------------+\n|Quarter|               Sum|           Average|           Maximum|\n+-------+------------------+------------------+------------------+\n|      1|363.95869885234185|10.109963857009497|11.829453550532005|\n|      3|365.68324879453405| 10.15786802207039|12.233378837422391|\n|      4| 342.2334082209804|10.065688477087658|12.210138970053695|\n|      2|  361.991445506568|10.055317930738001|12.276030776082463|\n+-------+------------------+------------------+------------------+\n\n\n\nLa oss gj√∏re det samme med SQL, men grupperer etter to variabler og sorterer output etterp√•.\n\n# Assuming df_ts is your DataFrame\ndf_ts.createOrReplaceTempView(\"temp_table\")\n\n# Now you can run a SQL query\nquery = \"\"\"\n    SELECT\n        Year,\n        Quarter,\n        SUM(serie00) AS Sum,\n        AVG(serie00) AS Average,\n        MAX(serie00) AS Maximum\n    FROM \n        temp_table\n    GROUP BY \n        Year, Quarter\n    ORDER BY\n        YEAR, QUARTER\n\"\"\"\n\naggregated_df_sql = spark.sql(query)\n\n# Show the result\naggregated_df_sql.show(10)\n\n+----+-------+------------------+------------------+------------------+\n|Year|Quarter|               Sum|           Average|           Maximum|\n+----+-------+------------------+------------------+------------------+\n|2000|      1|31.323050432219453|10.441016810739818|11.118293927071951|\n|2000|      2|28.911192473027377| 9.637064157675793| 9.900977410685329|\n|2000|      3|33.670797229797415|11.223599076599138|12.233378837422391|\n|2000|      4|28.094793356286914| 9.364931118762305| 10.32000478359274|\n|2001|      1|31.636678535169537|10.545559511723178|11.367822302191831|\n|2001|      2|29.629770128521507| 9.876590042840503|11.135215930381191|\n|2001|      3| 30.75408440118315| 10.25136146706105|10.723803326978505|\n|2001|      4|30.361048932627902|  10.1203496442093|10.368365984482093|\n|2002|      1|31.184163218551227|10.394721072850409|10.550579652234951|\n|2002|      2|29.128978392451202|   9.7096594641504|10.186365745367246|\n+----+-------+------------------+------------------+------------------+\nonly showing top 10 rows"
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Forord",
    "section": "",
    "text": "Denne boken tar sikte p√• √• gi SSB-ansatte mulighet til √• ta i bruk grunnleggende funksjonalitet p√• DAPLA uten hjelp fra eksperter. Boken er bygget opp som den reisen vi mener en statistikker skal gjennom n√•r de flytter sin produksjon fra bakke til sky1. F√∏rste del inneholder en del grunnleggende kunnskap som vi mener er viktig √• ha f√∏r man skal starte √• jobbe i skyen. Andre del forklarer hvordan man s√∏ker om √• opprette et Dapla-team, en forutsetning for √• drive databehandling p√• plattformen. Det vil ofte v√¶re f√∏rste steget i ta i bruk plattformen, siden det er slik man f√•r et sted √• lagre data. Her forklarer vi hvilke tjenester som inkluderes i et statistikkteam og hvordan man bruker og administerer dem. Den tredje delen tar utgangspunkt i at man skal starte √• kode opp sin statistikkproduksjon eller kj√∏re eksisterende kode. ssb-project er et verkt√∏y som er utviklet i SSB for √• gj√∏re denne prosessen s√• enkel som mulig. Da kan brukerne implementere det som anses som god praksis i SSB med noen f√• tastetrykk, samtidig som vi ogs√• forklarer mer detaljert hva som skjer under panseret.\nDet er tilrettelagt for en treningsarena i bakkemilj√∏et. Dette milj√∏et er nesten identisk med det som m√∏ter deg p√• Dapla, med unntak av at du her har tilgang til mange av de gamle systemene og mye mindre hestekrefter i maskinene. Ideen er at SSB-ere ofte vil √∏nske √• l√¶re seg de nye verkt√∏yene2 i kjente og kj√¶re omgivelser f√∏rst, og deretter flytte et ferdig skrevet produksjonsl√∏p til Dapla. Del 4 av denne boken beskriver mer utfyllende hvordan dette milj√∏et skiller seg fra Dapla, og hvordan man gj√∏r en del vanlige operasjoner mot de gamle bakkesystemene.\nSiste delen av boken kaller vi Avansert og tar for seg ulike emner som mer avanserte brukere typisk trenger informasjon om. Her finner man blant annet informasjon om hvilke databaser man kan bruke og hvilke form√•l de er egnet for. Her beskrives ogs√• hvordan man kan bruke andre IDE-er enn Jupyterlab hvis man √∏nsker det. Tjenester for schedulerte kj√∏ringer av Notebooks blir ogs√• diskutert.\nForh√•pentligvis senker denne boken terskelen for √• ta i bruk Dapla. Kommentarer og √∏nsker vedr√∏rende boken tas imot med √•pne armer.\nGod forn√∏yelseüòÅ"
  },
  {
    "objectID": "preface.html#footnotes",
    "href": "preface.html#footnotes",
    "title": "Forord",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI denne boken omtaler vi den gamle produksjonssonen, ofte kalt prodsonen, som bakke, og det nye skymilj√∏et Google Cloud som sky. Det er ikke helt presist men duger for form√•lene i denne boken.‚Ü©Ô∏é\nDet som omtales som nye verkt√∏y er vil som regel bety R, Python, Git, GitHub og Jupyterlab.‚Ü©Ô∏é"
  },
  {
    "objectID": "visualisering.html",
    "href": "visualisering.html",
    "title": "Visualisering",
    "section": "",
    "text": "Visualisering"
  },
  {
    "objectID": "slette-data.html",
    "href": "slette-data.html",
    "title": "Slette data fra b√∏tter",
    "section": "",
    "text": "Slette data fra b√∏tter\nSletting av filer og mapper fra b√∏tter kan gj√∏res fra Google Cloud Console. S√∏k opp ‚ÄúCloud Storage‚Äù i s√∏kefeltet og klikk p√• den b√∏tten hvor filen er lagret under ‚ÄúBuckets‚Äù.\nKryss av filen/katalogen som du √∏nsker √• slette og trykk ‚ÄúDelete‚Äù (Figur¬†1)\n\n\n\nFigur¬†1: Sletting av en fil\n\n\nSiden b√∏tter p√• Dapla har versjonering f√•r man opp en dialogboks som informerer om at objektet (dvs. filen) er versjonert (Figur¬†2). Trykk p√• ‚ÄúDelete‚Äù.\n\n\n\nFigur¬†2: Bekreft sletting av fil\n\n\nSlettingen kan ta noe tid. N√•r denne er ferdig vil filen v√¶re slettet, men den kan fortsatt gjenopprettes. Hvis du √∏nsker at filen skal slettes permanent, gj√∏r f√∏lgende:\n\nSkru p√• visning av slettede filer med √• bruke radioknappen ‚ÄúShow deleted data‚Äù (Figur¬†3)\n\n\n\n\nFigur¬†3: Skru p√• visning av slettede filer\n\n\n\nFinn frem til den slettede filen og trykk p√• linken ‚Äú1 noncurrent version‚Äù eller tilsvarende (Figur¬†4). Dette vil ta deg direkte til en side som viser filens versjonshistorikk.\n\n\n\n\nFigur¬†4: Velg versjonshistorikk\n\n\n\nVelg alle versjoner som vist p√• Figur¬†5 og trykk ‚ÄúDelete‚Äù\n\n\n\n\nFigur¬†5: Slett alle versioner\n\n\n\nTil slutt m√• man bekrefte at man √∏nsker √• slette alle versioner (Figur¬†6) med √• skrive inn DELETE og trykke p√• den bl√• ‚ÄúDelete‚Äù-knappen:\n\n\n\n\nFigur¬†6: Bekreft sletting av alle versjoner"
  },
  {
    "objectID": "orkestrering.html",
    "href": "orkestrering.html",
    "title": "Orkestrering",
    "section": "",
    "text": "Orkestrering"
  },
  {
    "objectID": "nytt-ssbproject.html",
    "href": "nytt-ssbproject.html",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "I dette kapittelet forklarer vi hvordan du oppretter et ssb-project og hva det inneb√¶rer. ssb-project er et CLI1 for √• raskt komme i gang med koding p√• Dapla, hvor en del SSB-spesifikke beste-prakiser er ivaretatt. Kode som naturlig h√∏rer sammen, f.eks. koden til et produksjonsl√∏p for en statistikk, er m√•lgruppen for dette programmet. Kort fortalt kan du kj√∏re denne kommandoen i en terminal\nssb-project create stat-testprod\nog du vil f√• en mappe som heter stat-testprod med f√∏lgende innhold:\n\nStandard mappestruktur En standard mappestruktur gj√∏r det lettere √• dele og samarbeide om kode, som igjen reduserer s√•rbarheten knyttet til at f√• personer kjenner koden.\nVirtuelt milj√∏ Virtuelle milj√∏er isolerer og lagrer informasjon knyttet til kode. For eksempel hvilken versjon av Python du bruker og tilh√∏rende pakkeversjoner. Det er viktig for at publiserte tall skal v√¶re reproduserbare. Verkt√∏yet for √• lage virtuelt milj√∏ er Poetry.\nVersjonsh√•ndtering med Git Initierer versjonsh√•ndtering med Git og legger til SSBs anbefalte .gitignore og .gitattributes. Det sikrer at du ikke versjonh√•ndterer filer/informasjon som ikke skal versjonsh√•ndteres.\n\nI tillegg lar ssb-project deg opprette et GitHub-repo hvis du √∏nsker. Les mer om hvordan du kan ta i bruk dette verkt√∏yet under.\n\n\n\n\n\n\nNote\n\n\n\nDokumentasjonen for ssb-project finnes her: https://statisticsnorway.github.io/ssb-project-cli/. Det oppdateres hver gang en ny versjon av ssb-project slippes.\n\n\n\n\nF√∏r du kan ta i bruk ssb-project s√• er det et par ting som m√• v√¶re p√• plass:\n\nDu m√• ha opprettet en git-bruker og git-epost lokalt der du skal kalle p√• programmet (les mer om hvordan her).\nHvis du √∏nsker at ssb-project ogs√• skal opprette et GitHub-repo for deg m√• du ogs√• f√∏lgende v√¶re p√• plass:\n\nDu m√• ha en GitHub-bruker (les hvordan her)\nSkru p√• 2-faktor autentisering for GitHub-brukeren din (les hvordan her)\nV√¶re koblet mot SSBs organisasjon statisticsnorway p√• GitHub (les hvordan her)\nOpprette Personal Access Token (PAT) og godkjenne det for bruk mot statisticsnorway (les hvordan her)\n\n\nDet er ogs√• √• anbefale at du lagrer PAT lokalt slik at du ikke trenger √• forholde deg til det n√•r jobber med Git og GitHub. Hvis du har alt dette p√• plass s√• kan du bare fortsette √• f√∏lge de neste kapitlene.\n\n\n\n\n\n\n\n\n\nHar du Github bruker? Noe funksjonalitet i ssb-project krever det. Finn ut hvordan ved √• lese forrige kapittel.\n\n\n\nssb-project lar deg opprette en prosjekt-mappe med og uten GitHub-repo. La oss ta for oss hver av alternativene.\n\n\nFor √• opprette et nytt ssb-project uten GitHub-repo gj√∏r du f√∏lgende:\n\n√Öpne en terminal. De fleste vil gj√∏re dette i Jupyterlab p√• bakke eller sky og da kan de bare trykke p√• det bl√• ‚ûï-tegnet i Jupyterlab og velge Terminal.\nF√∏r vi kj√∏rer programmet m√• vi v√¶re obs p√• at ssb-project vil opprette en ny mappe der vi st√•r. G√• derfor til den mappen du √∏nsker √• ha den nye prosjektmappen. For √• opprette et prosjekt som heter stat-testprod s√• skriver du f√∏lgende i terminalen:\n\nssb-project create stat-testprod\n\n\nHvis du stod i hjemmemappen din p√• n√•r du skrev inn kommandoen over i terminalen, s√• har du f√•tt mappestrukturen som vises i Figur¬†1. 2. Den inneholder f√∏lgende :\n\n.git-mappe som blir opprettet for √• versjonsh√•ndtere med Git.\nsrc-mappe som skal inneholde all koden som utgj√∏r produksjonsl√∏pet.\ntests-mappe som inneholder tester du skriver for koden din.\nLICENCE-fil som skal benyttes for public-repos i SSB.\npoetry.lock-fil som inneholder alle versjoner av Python-pakker som blir brukt.\nREADME.md-fil som brukes for tekstlig innhold p√• GitHub-siden for prosjektet.\n\n\n\n\n\n\nFigur¬†1: Mappen som ble opprettet av ssb-project.\n\n\n\n\n\n\n\nOver s√• opprettet vi et ssb-project uten √• opprette et GitHub-repo. Hvis du √∏nsker √• opprette et GitHub-repo ogs√• m√• du endre kommandoen over til:\nssb-project create stat-testprod --github --github-token='blablabla'\nKommandoen over oppretter en mappestruktur slik vi s√• tidligere, men ogs√• et ssb-project som heter stat-testprod med et GitHub-repo med samme navn. Som du ser s√• m√• vi da sende med opsjonen --github og PAT med opsjonen --github-token='blablabla'. Repoet i GitHub ser da ut som i Figur¬†2.\n\n\n\nFigur¬†2: GitHub-repo som er opprettet av ssb-project\n\n\n\n\n\n\n\n\nN√•r du har opprettet et nytt ssb-project, eller bygget et eksisterende prosjekt, s√• kan det ta rundt 30 sekunder f√∏r kernelen viser seg i Jupterlab-launcher. V√¶r t√•lmodig!\n\n\n\n\n\n\n\nN√•r du har opprettet et ssb-project s√• kan du installere de python-pakkene du trenger fra PyPI. Hvis du for eksempel √∏nsker √• installere Pandas, et popul√¶rt data wrangling bibliotek, s√• kan du gj√∏re f√∏lgende:\n\n√Öpne en terminal i Jupyterlab.\nG√• inn i prosjektmappen din ved √• skrive\n\ncd &lt;sti til prosjektmappe&gt;\n\nLag en branch/utviklingsg som f.eks. heter install-pandas:\n\ngit checkout -b install-pandas\n\nInstaller Pandas ved √• skrive f√∏lgende\n\npoetry add pandas\n\n\n\nFigur¬†3: Installasjon av Pandas med ssb-project\n\n\nFigur¬†3 viser hvordan dette vil se ut i en Jupyterlab-terminal. Kommandoen for √• installere noe er poetry add etterfulgt av pakkenavnet. Vi ser ogs√• at den automatisk legger til Pandas-versjonen i filen poetry.lock. Les mer om hvordan man installerer pakker her.\n\n\n\nN√•r du n√• har installert en pakke s√• har filen poetry.lock endret seg. La oss for eksempelets skyld anta at du √∏nsker √• bruke Git til √• dokumentere denne hendelsen, og dele det med en kollega via GitHub. Hvis vi har opprettet et ssb-project med et GitHub-repo s√• kan vi gj√∏re akkurat dette:\n\nVi kan stage alle endringer med f√∏lgende kommando i terminalen n√•r vi st√•r i prosjektmappen:\n\ngit add -A\n\nVidere kan commit en endring, dvs. ta et stillbilde av koden i dette √∏yeblikket, ved √• skrive f√∏lgende:\n\ngit commit -m \"Installert pandas\"\n\nPush det opp til GitHub3. Anta at vi gjorde dette i branchen install-pandas som ble opprettet tidligere. Da kan vi skrive f√∏lgende:\n\ngit push --set-upstream origin install-pandas\nMer kommer her.\n\n\n\nN√•r vi skal samarbeide med andre om kode s√• gj√∏r vi dette via GitHub. N√•r du pusher koden din til GitHub, s√• kan samarbeidspartnere pulle ned koden og jobbe videre med den. Men n√•r de henter ned koden s√• vil de bare hente ned selve koden, ikke pakker og Python-versjonen som ble brukt. De m√• installere alt som du hadde installert. I tillegg trenger de en kernel hvis de skal jobbe i Jupyterlab. ssb-project gj√∏r det sv√¶rt enkelt √• bygge opp det du trenger, siden det virtuelle milj√∏et har all informasjon om hva som trengs.\nFor at samarbeidspartneren din skal kunne bygge milj√∏et p√• nytt, m√• de ha gjort en minimal konfigurering av Git. Les mer om hvordan du frem for √• gj√∏re dette her.\nFor √• bygge opp et eksisterende milj√∏ gj√∏r du f√∏lgende:\n\nF√∏rst m√• du kopiere prosjektet ned lokalt, eller klone repoet med git-terminologi\n\ngit clone https://github.com/statisticsnorway/&lt;prosjektnavn&gt;\n\nG√• inn i mappen du klonet\n\ncd &lt;prosjektnavn&gt;\n\nSkape et virtuelt milj√∏ og installere en tilsvarende Jupyter kernel med\n\nssb-project build\n\n\n\nDet vil v√¶re tilfeller hvor man √∏nsker √• slette et ssb-project, enten fordi man ikke trenger koden lenger eller fordi man bare testet litt.\n\n\nHvis man jobber med flere prosjekter s√• kan det fort bli mange Jupyter kerneler hengende igjen. Derfor er det ogs√• mulighet √• kj√∏re\nssb-project clean stat-testprod\nsom sletter Jupyter-kernelen og de installerte pakkene i prosjektet. Hvis du ogs√• √∏nsker √• slette selve mappen med kode m√• du gj√∏re det manuelt4:\nrm -rf ~/stat-testprod/\nProsjektmappen over l√• direkte i hjemmemappen min og hjemmemappen p√• Linux kan alltid referes til med et tilda-tegn ~.\n\n\n\nGitHub-repoer som er opprettet under SSB sin organinasjons statisticsnorway p√• GitHub kan ikke slettes, bare arkiveres. Grunnen er at hvis man oppdager en s√•rbarhet senere s√• er det viktig √• kunne se repoet for √• forst√• hva som har skjedd.\nHvis du ikke trenger et GitHub-repo lenger kan man arkivere repoet. Det gj√∏r du p√• f√∏lgende m√•te:\n\nGi inn i repoet Settings slik som vist med r√∏d pil i Figur¬†4.\n\n\n\n\nFigur¬†4: Settings for repoet.\n\n\n\nUnder General scroller du deg ned til delen som heter Danger Zone og velger Archive this repository, slik som vist p√• Figur¬†5.\n\n\n\n\nFigur¬†5: Arkivering av et repo.\n\n\n\nI dialogboksen som dukker opp fyller du inn reponavnet som beskrevet og trykker p√• I understand the consequences, archive this repository, som vist i Figur¬†6.\n\n\n\n\nFigur¬†6: Bekreftelse av arkiveringen.\n\n\nN√•r det er gjort s√• er repoet lesbart, men man kan ikke jobbe med det. Men som vi ser av @#fig-github-repo-settings-archive-warning kan man omgj√∏re arkiveringen senere hvis det skulle v√¶re √∏nskelig.\n\n\n\n\nVi har forel√∏pig ikke integret R i ssb-project. Grunnen er at det mest popul√¶re virtuelle milj√∏-verkt√∏et for R, renv, kun tilbyr √• passe p√• versjoner av R-pakker og ikke selve R-installasjonen. Det er en svakhet som trolig gj√∏r det vanskeligere enn n√∏dvendig √• gjenskape tidligere publiserte resultater med ssb-project. I tillegg klarer den ikke √• gjenkjenne pakker som blir brukt i ipynb-filer.\nPlanen er √• finne et annet verkt√∏y enn renv som kan ogs√• reprodusere R-versjonen. Team Statistikktjenester ser n√¶rmere p√• hvilke alternativer som finnes og vil tilby noe i fremtiden.\nI mellomtiden kan man bruke renv slik det er beskrevet her for skymilj√∏et, og med denne modifiseringen for bakkemilj√∏et."
  },
  {
    "objectID": "nytt-ssbproject.html#forberedelser",
    "href": "nytt-ssbproject.html#forberedelser",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "F√∏r du kan ta i bruk ssb-project s√• er det et par ting som m√• v√¶re p√• plass:\n\nDu m√• ha opprettet en git-bruker og git-epost lokalt der du skal kalle p√• programmet (les mer om hvordan her).\nHvis du √∏nsker at ssb-project ogs√• skal opprette et GitHub-repo for deg m√• du ogs√• f√∏lgende v√¶re p√• plass:\n\nDu m√• ha en GitHub-bruker (les hvordan her)\nSkru p√• 2-faktor autentisering for GitHub-brukeren din (les hvordan her)\nV√¶re koblet mot SSBs organisasjon statisticsnorway p√• GitHub (les hvordan her)\nOpprette Personal Access Token (PAT) og godkjenne det for bruk mot statisticsnorway (les hvordan her)\n\n\nDet er ogs√• √• anbefale at du lagrer PAT lokalt slik at du ikke trenger √• forholde deg til det n√•r jobber med Git og GitHub. Hvis du har alt dette p√• plass s√• kan du bare fortsette √• f√∏lge de neste kapitlene."
  },
  {
    "objectID": "nytt-ssbproject.html#opprett-ssb-project",
    "href": "nytt-ssbproject.html#opprett-ssb-project",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "Har du Github bruker? Noe funksjonalitet i ssb-project krever det. Finn ut hvordan ved √• lese forrige kapittel.\n\n\n\nssb-project lar deg opprette en prosjekt-mappe med og uten GitHub-repo. La oss ta for oss hver av alternativene.\n\n\nFor √• opprette et nytt ssb-project uten GitHub-repo gj√∏r du f√∏lgende:\n\n√Öpne en terminal. De fleste vil gj√∏re dette i Jupyterlab p√• bakke eller sky og da kan de bare trykke p√• det bl√• ‚ûï-tegnet i Jupyterlab og velge Terminal.\nF√∏r vi kj√∏rer programmet m√• vi v√¶re obs p√• at ssb-project vil opprette en ny mappe der vi st√•r. G√• derfor til den mappen du √∏nsker √• ha den nye prosjektmappen. For √• opprette et prosjekt som heter stat-testprod s√• skriver du f√∏lgende i terminalen:\n\nssb-project create stat-testprod\n\n\nHvis du stod i hjemmemappen din p√• n√•r du skrev inn kommandoen over i terminalen, s√• har du f√•tt mappestrukturen som vises i Figur¬†1. 2. Den inneholder f√∏lgende :\n\n.git-mappe som blir opprettet for √• versjonsh√•ndtere med Git.\nsrc-mappe som skal inneholde all koden som utgj√∏r produksjonsl√∏pet.\ntests-mappe som inneholder tester du skriver for koden din.\nLICENCE-fil som skal benyttes for public-repos i SSB.\npoetry.lock-fil som inneholder alle versjoner av Python-pakker som blir brukt.\nREADME.md-fil som brukes for tekstlig innhold p√• GitHub-siden for prosjektet.\n\n\n\n\n\n\nFigur¬†1: Mappen som ble opprettet av ssb-project.\n\n\n\n\n\n\n\nOver s√• opprettet vi et ssb-project uten √• opprette et GitHub-repo. Hvis du √∏nsker √• opprette et GitHub-repo ogs√• m√• du endre kommandoen over til:\nssb-project create stat-testprod --github --github-token='blablabla'\nKommandoen over oppretter en mappestruktur slik vi s√• tidligere, men ogs√• et ssb-project som heter stat-testprod med et GitHub-repo med samme navn. Som du ser s√• m√• vi da sende med opsjonen --github og PAT med opsjonen --github-token='blablabla'. Repoet i GitHub ser da ut som i Figur¬†2.\n\n\n\nFigur¬†2: GitHub-repo som er opprettet av ssb-project\n\n\n\n\n\n\n\n\nN√•r du har opprettet et nytt ssb-project, eller bygget et eksisterende prosjekt, s√• kan det ta rundt 30 sekunder f√∏r kernelen viser seg i Jupterlab-launcher. V√¶r t√•lmodig!"
  },
  {
    "objectID": "nytt-ssbproject.html#installere-pakker",
    "href": "nytt-ssbproject.html#installere-pakker",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "N√•r du har opprettet et ssb-project s√• kan du installere de python-pakkene du trenger fra PyPI. Hvis du for eksempel √∏nsker √• installere Pandas, et popul√¶rt data wrangling bibliotek, s√• kan du gj√∏re f√∏lgende:\n\n√Öpne en terminal i Jupyterlab.\nG√• inn i prosjektmappen din ved √• skrive\n\ncd &lt;sti til prosjektmappe&gt;\n\nLag en branch/utviklingsg som f.eks. heter install-pandas:\n\ngit checkout -b install-pandas\n\nInstaller Pandas ved √• skrive f√∏lgende\n\npoetry add pandas\n\n\n\nFigur¬†3: Installasjon av Pandas med ssb-project\n\n\nFigur¬†3 viser hvordan dette vil se ut i en Jupyterlab-terminal. Kommandoen for √• installere noe er poetry add etterfulgt av pakkenavnet. Vi ser ogs√• at den automatisk legger til Pandas-versjonen i filen poetry.lock. Les mer om hvordan man installerer pakker her."
  },
  {
    "objectID": "nytt-ssbproject.html#push-til-github",
    "href": "nytt-ssbproject.html#push-til-github",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "N√•r du n√• har installert en pakke s√• har filen poetry.lock endret seg. La oss for eksempelets skyld anta at du √∏nsker √• bruke Git til √• dokumentere denne hendelsen, og dele det med en kollega via GitHub. Hvis vi har opprettet et ssb-project med et GitHub-repo s√• kan vi gj√∏re akkurat dette:\n\nVi kan stage alle endringer med f√∏lgende kommando i terminalen n√•r vi st√•r i prosjektmappen:\n\ngit add -A\n\nVidere kan commit en endring, dvs. ta et stillbilde av koden i dette √∏yeblikket, ved √• skrive f√∏lgende:\n\ngit commit -m \"Installert pandas\"\n\nPush det opp til GitHub3. Anta at vi gjorde dette i branchen install-pandas som ble opprettet tidligere. Da kan vi skrive f√∏lgende:\n\ngit push --set-upstream origin install-pandas\nMer kommer her."
  },
  {
    "objectID": "nytt-ssbproject.html#bygg-eksisterende-ssb-project",
    "href": "nytt-ssbproject.html#bygg-eksisterende-ssb-project",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "N√•r vi skal samarbeide med andre om kode s√• gj√∏r vi dette via GitHub. N√•r du pusher koden din til GitHub, s√• kan samarbeidspartnere pulle ned koden og jobbe videre med den. Men n√•r de henter ned koden s√• vil de bare hente ned selve koden, ikke pakker og Python-versjonen som ble brukt. De m√• installere alt som du hadde installert. I tillegg trenger de en kernel hvis de skal jobbe i Jupyterlab. ssb-project gj√∏r det sv√¶rt enkelt √• bygge opp det du trenger, siden det virtuelle milj√∏et har all informasjon om hva som trengs.\nFor at samarbeidspartneren din skal kunne bygge milj√∏et p√• nytt, m√• de ha gjort en minimal konfigurering av Git. Les mer om hvordan du frem for √• gj√∏re dette her.\nFor √• bygge opp et eksisterende milj√∏ gj√∏r du f√∏lgende:\n\nF√∏rst m√• du kopiere prosjektet ned lokalt, eller klone repoet med git-terminologi\n\ngit clone https://github.com/statisticsnorway/&lt;prosjektnavn&gt;\n\nG√• inn i mappen du klonet\n\ncd &lt;prosjektnavn&gt;\n\nSkape et virtuelt milj√∏ og installere en tilsvarende Jupyter kernel med\n\nssb-project build"
  },
  {
    "objectID": "nytt-ssbproject.html#rydd-opp-etter-deg",
    "href": "nytt-ssbproject.html#rydd-opp-etter-deg",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "Det vil v√¶re tilfeller hvor man √∏nsker √• slette et ssb-project, enten fordi man ikke trenger koden lenger eller fordi man bare testet litt.\n\n\nHvis man jobber med flere prosjekter s√• kan det fort bli mange Jupyter kerneler hengende igjen. Derfor er det ogs√• mulighet √• kj√∏re\nssb-project clean stat-testprod\nsom sletter Jupyter-kernelen og de installerte pakkene i prosjektet. Hvis du ogs√• √∏nsker √• slette selve mappen med kode m√• du gj√∏re det manuelt4:\nrm -rf ~/stat-testprod/\nProsjektmappen over l√• direkte i hjemmemappen min og hjemmemappen p√• Linux kan alltid referes til med et tilda-tegn ~.\n\n\n\nGitHub-repoer som er opprettet under SSB sin organinasjons statisticsnorway p√• GitHub kan ikke slettes, bare arkiveres. Grunnen er at hvis man oppdager en s√•rbarhet senere s√• er det viktig √• kunne se repoet for √• forst√• hva som har skjedd.\nHvis du ikke trenger et GitHub-repo lenger kan man arkivere repoet. Det gj√∏r du p√• f√∏lgende m√•te:\n\nGi inn i repoet Settings slik som vist med r√∏d pil i Figur¬†4.\n\n\n\n\nFigur¬†4: Settings for repoet.\n\n\n\nUnder General scroller du deg ned til delen som heter Danger Zone og velger Archive this repository, slik som vist p√• Figur¬†5.\n\n\n\n\nFigur¬†5: Arkivering av et repo.\n\n\n\nI dialogboksen som dukker opp fyller du inn reponavnet som beskrevet og trykker p√• I understand the consequences, archive this repository, som vist i Figur¬†6.\n\n\n\n\nFigur¬†6: Bekreftelse av arkiveringen.\n\n\nN√•r det er gjort s√• er repoet lesbart, men man kan ikke jobbe med det. Men som vi ser av @#fig-github-repo-settings-archive-warning kan man omgj√∏re arkiveringen senere hvis det skulle v√¶re √∏nskelig."
  },
  {
    "objectID": "nytt-ssbproject.html#hva-med-r",
    "href": "nytt-ssbproject.html#hva-med-r",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "Vi har forel√∏pig ikke integret R i ssb-project. Grunnen er at det mest popul√¶re virtuelle milj√∏-verkt√∏et for R, renv, kun tilbyr √• passe p√• versjoner av R-pakker og ikke selve R-installasjonen. Det er en svakhet som trolig gj√∏r det vanskeligere enn n√∏dvendig √• gjenskape tidligere publiserte resultater med ssb-project. I tillegg klarer den ikke √• gjenkjenne pakker som blir brukt i ipynb-filer.\nPlanen er √• finne et annet verkt√∏y enn renv som kan ogs√• reprodusere R-versjonen. Team Statistikktjenester ser n√¶rmere p√• hvilke alternativer som finnes og vil tilby noe i fremtiden.\nI mellomtiden kan man bruke renv slik det er beskrevet her for skymilj√∏et, og med denne modifiseringen for bakkemilj√∏et."
  },
  {
    "objectID": "nytt-ssbproject.html#footnotes",
    "href": "nytt-ssbproject.html#footnotes",
    "title": "Nytt ssb-project",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nCLI = Command-Line-Interface. Dvs. et program som er skrevet for √• brukes terminalen ved hjelp av enkle kommandoer.‚Ü©Ô∏é\nFiler og mapper som starter med punktum er skjulte med mindre man ber om √• se dem. I Jupyterlab kan disse vises i filutforskeren ved √• velge View fra menylinjen, og deretter velge Show hidden files. I en terminal skriver man ls -a for √• se de.‚Ü©Ô∏é\n√Ö pushe til GitHub uten √• sende ved Personal Access Token fordrer at du har lagret det lokalt s√• Git kan finne det. Her et eksempel p√• hvordan det kan gj√∏res.‚Ü©Ô∏é\nDette kan ogs√• gj√∏res ved √• h√∏yreklikke p√• mappen i Jupyterlab sin filutforsker og velge Delete.‚Ü©Ô∏é"
  },
  {
    "objectID": "lage-nettsider.html",
    "href": "lage-nettsider.html",
    "title": "Lage nettsider",
    "section": "",
    "text": "Lage nettsider"
  },
  {
    "objectID": "koble-data.html",
    "href": "koble-data.html",
    "title": "Koble data",
    "section": "",
    "text": "Koble data"
  },
  {
    "objectID": "jobbe-med-data.html",
    "href": "jobbe-med-data.html",
    "title": "Jobbe med data",
    "section": "",
    "text": "N√•r vi oppretter et dapla-team s√• f√•r vi tildelt et eget omr√•det for lagring av data. For √• kunne lese og skrive data fra Jupyter til disse omr√•dene m√• vi autentisere oss, siden Jupyter og lagringsomr√•det er to separate sikkerhetsoner.\nFigur¬†1 viser dette klarer skillet mellom hvor vi koder og hvor dataene ligger p√• Dapla1. I dette kapitlet beskriver vi n√¶rmere hvordan du kan jobbe med dataene dine p√• Dapla.\n\n\n\nFigur¬†1: Tydelig skille mellom kodemilj√∏ og datalagring p√• Dapla.\n\n\n\n\nFor √• gj√∏re det enklere √• jobbe data p√• tvers av Jupyter og lagringsomr√•det er det laget noen egne SSB-utviklede biblioteker for √• gj√∏re vanlige operasjoner mot lagringsomr√•det. Siden b√•de R og Python skal brukes p√• Dapla, s√• er det laget to biblioteker, en for hver av disse spr√•kene. fellesr er biblioteket for R, og dapla-toolbelt er biblioteket for Python.\n\n\ndapla-toolbelt er en en pakke som lar deg enkelt lese og skrive til lagringsomr√•det uten √• m√•tte autentifisere deg manuelt. Den har en Pandas-aktig syntaks som forh√•pentlig er gjenkjennbar for de fleste. Pakken er installert i alle Python-kernels p√• Dapla, s√• du trenger ikke √• installere den selv hvis du √•pner en notebook med Python3 for eksempel. For √• importere hele biblioteket i en notebook skriver du bare\nimport dapla as dp\ndapla-toolbelt bruker en pakke som heter gcsfs for √• kommunisere med lagringsomr√•det. gcsfs er en pakke som lar deg bruke Google Cloud Storage (GCS) som om det var en filsystem. Det betyr at du kan bruke samme syntaks som du bruker for √• lese og skrive til filer p√• din egen maskin. Du kan lese mulighetene i gcsfs her. Et eksempel p√• hvordan de to pakkene kan brukes sammen ser du her:\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Example of how you can use gcsfs and dapla-toolbelt together\nfs.touch(\"gs://my-bucket/my-folder/\")\nI koden over brukte jeg kommandoen touch fra gcsfs og FileClient fra dapla-toolbelt for √• opprette en mappe i lagringsomr√•det.\nI kapitlene under finner du konkrete eksempler p√• hvordan du kan bruke dapla-toolbelt til √• jobbe med data i SSBs lagringsomr√•det.\n\n\n\nR-pakken fellesr er under utvikling og gir mye av den samme funksjonaliteten som dapla-toolbelt gir for Python. I tillegg til √• kunne lese og skrive til lagringsomr√•det p√• Dapla, s√• har fellesr ogs√• funksjoner for √• jobbe med metadata p√• Dapla.\nfellesr er installert p√• Dapla og funksjoner kan benyttes ved:\nlibrary(fellesr)\nHvis du benytte en renv milj√∏, m√• pakken installeres en gang. Dette kan gj√∏res ved:\nrenv::install(\"statisticsnorway/fellesr\")\n\n\n\n\nI denne delen viser vi hvordan man gj√∏r veldig vanlige operasjoner n√•r man koder et produksonsl√∏p for en statistikk. Flere eksempler p√• nyttige systemkommandoer finner du her.\n\n\n\n\n\n\n\n\nEksempeldata\n\n\n\nDet finnes et omr√•de som alle SSB-ansatte har lese- og skrivetilgang til. Det er\ngs://ssb-prod-dapla-felles-data-delt/ i prod-milj√∏et p√• Dapla, og\ngs://ssb-staging-dapla-felles-data-delt/ i staging-milj√∏et. Eksemplene under bruker f√∏rstnevnte i koden, slik at alle kan kj√∏re koden selv.\nKode-eksemplene finnes for b√•de R og Python, og du kan velge hvilken du skal se ved √• trykke p√• den arkfanen du er interessert i.\n\n\n√Ö liste ut innhold i et gitt mappe p√• Dapla er ganske enkelt. Under ser du hvordan du kan liste ut innholdet i f√∏lgende mappe:\ngs://ssb-prod-dapla-felles-data-delt/felles/veiledning/python/eksempler/purchases\n\nPython \n\n\nVi bruker modulen FileClient fra dapla-toolbelt for √• liste ut innholdet i en mappe.\nfrom dapla import FileClient\n\n# Set path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\nFileClient.ls(f\"{bucket}/{folder}\")\nMed kommandoen over f√•r du listet ut alle filnavn i mappen. Hvis du vil ha mer informasjon om filene s√• kan du bruke ls-kommandoen med detail = True, som under:\nFileClient.ls(f\"{bucket}/{folder}\", detail = True)\nSyntaksen med ls er veldig lik det man kjenner fra en Linux-terminal. Men n√•r vi bruker detail = True blir metadata om filene returnert som en Python-liste med dictionaries. Det kan v√¶re sv√¶rt nyttig n√•r du f.eks. trenger √• vite dato og tidspunkt for n√•r en fil ble opprettet, eller n√•r den sist ble oppdatert.\n\n\n# Loading functions into notebook\nlibrary(fellesr)\n\n# Path to folder\nbucket &lt;- \"ssb-prod-dapla-felles-data-delt/\"\nfolder &lt;- \"felles/veiledning/python/eksempler/purchases\"\n\n# List files in folder \nlist.files(paste0(bucket, folder))\nMerknad: N√•r du spesifisere b√∏tter i R, trenger du ikke ‚Äúgs://‚Äù foran.\n\n\n\n\n\n\n√Ö skrive filer til et lagringsomr√•de p√• Dapla er ogs√• ganske enkelt. Det ligner mye p√• den syntaksen vi er kjent med fra vanlige R- og Python-pakker, med noen sm√• unntak.\n\n\nUnder lager vi en dataframe i en notebook og skriver den ut til en parquet-fil.\n\nPython \n\n\nN√•r vi leser en Parquet-fil med dapla-toolbelt s√• bruker den pyarrow i bakgrunnen. Dette er en av de raskeste m√•tene √• lese og skrive Parquet-filer p√•.\n\nimport dapla as dp\nimport pandas as pd\nimport numpy as np\n\n# Set path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Create pandas dataframe\npurchases = pd.DataFrame(np.random.randn(10, 5),\n                        columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Write pandas dataframe as parquet to GCS bucket\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{bucket}/{folder}/data.parquet\",\n                file_format = \"parquet\",)\nN√•r vi kalte write_pandas over s√• spesifiserte vi at filformatet skulle v√¶re parquet. Dette er default, s√• vi kunne ogs√• ha skrevet det slik:\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{bucket}/{folder}/data.parquet\")\nMen for de andre filformatene m√• vi alts√• spesifisere dette.\n\n\nN√•r vi jobber med Parquet-fil i R, bruker vi pakken arrow. Dette er en del av fellesr pakken s√• du trenger kun √• kalle inn dette. Pakken inneholder funksjonen write_SSB som kan brukes til √• skrive data til b√∏tte p√• Dapla.\nlibrary(fellesr)\n\n# Set stien til hvor data skal lagres\nbucket &lt;- \"ssb-prod-dapla-felles-data-delt\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\n# Lage en eksempel dataframe\npurchases = data.frame(A = runif(10), B= runif(10), C=runif(10))\n\n# Skrive data til b√∏ttet som en parquet\nwrite_SSB(purchases, file.path(bucket, folder, \"purchases.parquet\"))\nMerknad: N√•r du spesifisere b√∏tter i R, trenger du ikke ‚Äúgs://‚Äù foran.\n\n\n\n\n\n\nNoen ganger √∏nsker vi √• lagre data i andre formatter slik som CSV, JSON og XML.\n\nPython \n\n\ndapla-toolbelt kan skrive ut json, csv og posisjonsfiler (fixed-width-files/fwf) til lagringsomr√•det. M√•ten den gj√∏r det p√• er √• bruke Pandas sine funksjoner read_json, read_csv, read_fwf under panseret. Dette kan v√¶re nyttig √• vite for skj√∏nne hvordan dapla-toolbelt h√•ndterer ulike strukturer i (spesielt hierarkiske) tekstfiler. Under ser du hvordan du kan skrive ut en dataframe til en json-fil.\nimport numpy as np\nimport pandas as pd\nfrom dapla import FileClient\n\n# Set path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Create a dataframe with Pandas\ndf = pd.DataFrame(np.random.randn(10, 5), columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Save dataframe as json with dapla-toolbelt\ndp.write_pandas(df = df,\n                gcs_path = f\"{bucket}/{folder}/test.json\",\n                file_format = \"json\")\nSom vi ser at syntaksen over s√• kunne vi skrevet ut til noe annet enn json ved √• endre verdien i argumentet file_format.\n\n\nPakken fellesr kan ogs√• brukes til √• skrive andre type filer, for eksempel csv, til b√∏tter. Dette gj√∏res med funksjonen write_SSB og spesifisere √∏nsket filtype i filnavn.\nF√∏rst kaller vi biblioteket og lage noe test data ved:\nlibrary(fellesr)\n\n# Set stien til hvor data skal lagres\nbucket &lt;- \"ssb-prod-dapla-felles-data-delt\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\n# Lage en eksempel dataframe\npurchases = data.frame(A = runif(10), B= runif(10), C=runif(10))\n\n# Skrive til csv\nwrite_SSB(purchases, file.path(bucket, folder, \"purchases.csv\")\n\n\n\n\n\n\nDet er ikke anbefalt √• bruke xlsx-formatet, men her ser du hvordan det kan skrives ut. Mer kommer.\n\nPython \n\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Henter token for √• kunne lese fra Dapla\ntoken = AuthClient.fetch_google_credentials()\n\n# Filsti\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\ndf.to_excel(f\"{bucket}/{folder}/test.xlsx\",\n           storage_options={\"token\": token})\n\n\nKommer snart\n\n\n\n\n\n\n\nUnder finner du eksempler p√• hvordan du kan lese inn data til en Jupyter Notebooks p√• Dapla.\n\n\n\nPython \n\n\nimport dapla as dp\n\n# Set path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Read path into pandas dataframe \ndp.read_pandas(gcs_path= f\"{bucket}/{folder}/data.parquet\",\n               file_format = \"parquet\",\n               columns = None,)\nSom vi s√• med write_pandas s√• er file_format default satt til parquet, og default for columns = None, s√• vi kunne ogs√• ha skrevet det slik:\ndp.read_pandas(gcs_path= f\"{bucket}/{folder}/data.parquet\")\ncolumns-argumentet er en liste med kolonnenavn som vi √∏nsker √• lese inn. Hvis vi ikke spesifiserer noen kolonner s√• vil alle kolonnene leses inn.\n\n\nPakken fellesr kan brukes til √• lese inn data. Funksjonen read_SSB() kan lese inn filer i flere format inkluderende parquet.\nHer er et eksempel av √• lese inn parquet fil ‚Äú1987‚Äù.\nlibrary(fellesr)\n\nbucket &lt;- \"ssb-prod-dapla-felles-data-delt\"\nfolder &lt;- \"R_smoke_test\"\n\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.parquet\"))\nVi kan ogs√• filtrere hvilke variabel vi √∏nsker √• lese inn ved √• spesifisere parameter col_select. For eksempel:\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.parquet\"),\n                    col_select = c(\"Year\", \"Month\"))\nInnlesning av parquet som er kartdata finner du her: Lese kartdata\n\n\n\n\n\n\nKommer mer snart. Python-koden under bygger p√• eksempelet over.\n\nPython \n\n\nimport dapla as dp\n\n# Path to write to\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Read in json-file from dapla-storage\ndf = dp.read_pandas(gcs_path = f\"{bucket}/{folder}/test3.json\",\n               file_format = \"json\")\n\n\nFunksjonen read_SSB() kan lese inn flere type av fil-format, slik som csv og json. Du trenger ikke √• endre koden, kun spesifisere hele filnavn.\nF√∏rst kaller vi inn biblioteket fellesr og spesifisere b√∏tte/mappen:\nlibrary(fellesr)\n\n# Filsti\nbucket &lt;- \"ssb-prod-dapla-felles-data-delt\"\nfolder &lt;- \"R_smoke_test\"\n\n# Lese inn CSV-fil\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.csv\"))\nFor √• lese inn en json-fil kan skrive f√∏lgende:\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.json\"))\n\n\n\n\n\n\n\nPython \n\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Hent token\ntoken = AuthClient.fetch_google_credentials()\n\n# Les inn fil\ndf = pd.read_excel(\"gs://ssb-prod-arbmark-skjema-data-produkt/test_gcp.xlsx\",\n    storage_options={\"token\": token})\n\n\nKommer snart\n\n\n\n\n\n\nHer er et eksempel p√• hvordan man leser inn en sas7bdat-fil p√• Dapla som har blitt generert i prodsonen.\n\nPython \n\n\nimport pandas as pd\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nsti = \"gs://ssb-prod-dapla-felles-data-delt/felles/veiledning/sas/statbank_ledstill.sas7bdat\"\n\nwith fs.open(sti) as sas:\n    df = pd.read_sas(sas, format=\"sas7bdat\", encoding=\"latin1\")\n\n\nI produksjon sone (p√• bakken) kan R-pakken haven benyttes for √• lese inn .sas7bdat filer. Dette er ikke implementert i fellesr enda for lesing fra Dapla b√∏tte.\nMer om dette kommer.\n\n\n\n\n\n\n\n√Ö slette filer fra lagringsomr√•det kan gj√∏res p√• flere m√•ter. I kapitlet om sletting av data viste vi hvordan man gj√∏r det med pek-og-klikk i Google Cloud Console. Under ser du hvordan du kan slette filer med dapla-toolbelt og gcsfs eller fellesr.\n\nPython \n\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\nfs.rm(f\"{bucket}/{from_folder}/df.json\")\n\n\nFunksjonen gc_delete_object kan brukes til √• slette data p√• lagringsomr√•det.\nlibrary(fellesr)\n\nbucket &lt;- \"ssb-prod-dapla-felles-data-delt\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\ngcs_delete_object(file.path(bucket, folder, \"purchases.parquet\"))\n\n\n\n\n\n\n√Ö kopiere filer mellom mapper p√• et Linux-filsystem inneb√¶rer som regel bruke cp-kommandoen. P√• Dapla er det ikke s√• mye forskjell. Vi bruker en ligende tiln√¶rming n√• vi skal kopiere mellom b√∏tter eller mapper p√• lagringsomr√•det til SSB. Under ser du hvordan du kan kopiere en fil fra en mappe til en annen.\n\nPython \n\n\nLa oss begynne med et eksempel der vi kopierer en fil fra en mappe til en annen i samme b√∏tte.\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Path to folders\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\n# Copy file\nfs.cp(f\"{bucket}/{from_folder}/data.parquet\",\n      f\"{bucket}/{to_folder}/data_copy.parquet\")\nDet ogs√• fungere for √• kopiere filer mellom b√∏tter.\nEt annet scenario vi ofte vil st√∏te p√• er at vi √∏nsker √• kopiere en fil fra v√•rt Jupyter-filsystem til en mappe p√• lagringsomr√•det. Her kan vi bruke fs.put-metoden.\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Create a new file in your home directory called test.txt\nwith open('/home/jovyan/test.txt', 'w') as f:\n    f.write('Create a new text file!')\n\n#Path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Copy file from local to remote file system\nfs.put(lpath=f\"/home/jovyan/test.txt\", rpath=f\"{bucket}/{folder}/test.txt\")\n√ònsker vi √• kopiere en hel mappe fra lagringsomr√•det til Jupyter-filsystemet, kan vi bruke fs.get-metoden, med opsjonen recursive=True.\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Copy file\nfs.get(&lt;from_bucket&gt;,\n      \"/home/jovyan/sesongjustering/\",\n      recursive=True)\n\n\nKommer snart\n\n\n\n\n\n\n\nPython \n\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\nfs.mv(f\"{bucket}/{from_folder}/data.parquet\", f\"{bucket}/{to_folder}/data.parquet\")\n\n\nKommer snart\n\n\n\n\n\n\nSelv om b√∏tter ikke har mapper med en hierarkisk struktur slik man er kjent med fra klassike filsystemer, s√• kan man opprette det som ser ut som mapper i objektnavnet. I realiteten blir bare / oppfattet som en del av navnet p√• objektet. Skulle du likevel √∏nske √• opprette dette s√• kan du gj√∏re det f√∏lgende m√•te:\n\nPython \n\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n#Path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Create folder\nfs.touch(f\"{bucket}/{folder}/testmappe/\")\n\n\nKommer snart"
  },
  {
    "objectID": "jobbe-med-data.html#ssb-biblioteker",
    "href": "jobbe-med-data.html#ssb-biblioteker",
    "title": "Jobbe med data",
    "section": "",
    "text": "For √• gj√∏re det enklere √• jobbe data p√• tvers av Jupyter og lagringsomr√•det er det laget noen egne SSB-utviklede biblioteker for √• gj√∏re vanlige operasjoner mot lagringsomr√•det. Siden b√•de R og Python skal brukes p√• Dapla, s√• er det laget to biblioteker, en for hver av disse spr√•kene. fellesr er biblioteket for R, og dapla-toolbelt er biblioteket for Python.\n\n\ndapla-toolbelt er en en pakke som lar deg enkelt lese og skrive til lagringsomr√•det uten √• m√•tte autentifisere deg manuelt. Den har en Pandas-aktig syntaks som forh√•pentlig er gjenkjennbar for de fleste. Pakken er installert i alle Python-kernels p√• Dapla, s√• du trenger ikke √• installere den selv hvis du √•pner en notebook med Python3 for eksempel. For √• importere hele biblioteket i en notebook skriver du bare\nimport dapla as dp\ndapla-toolbelt bruker en pakke som heter gcsfs for √• kommunisere med lagringsomr√•det. gcsfs er en pakke som lar deg bruke Google Cloud Storage (GCS) som om det var en filsystem. Det betyr at du kan bruke samme syntaks som du bruker for √• lese og skrive til filer p√• din egen maskin. Du kan lese mulighetene i gcsfs her. Et eksempel p√• hvordan de to pakkene kan brukes sammen ser du her:\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Example of how you can use gcsfs and dapla-toolbelt together\nfs.touch(\"gs://my-bucket/my-folder/\")\nI koden over brukte jeg kommandoen touch fra gcsfs og FileClient fra dapla-toolbelt for √• opprette en mappe i lagringsomr√•det.\nI kapitlene under finner du konkrete eksempler p√• hvordan du kan bruke dapla-toolbelt til √• jobbe med data i SSBs lagringsomr√•det.\n\n\n\nR-pakken fellesr er under utvikling og gir mye av den samme funksjonaliteten som dapla-toolbelt gir for Python. I tillegg til √• kunne lese og skrive til lagringsomr√•det p√• Dapla, s√• har fellesr ogs√• funksjoner for √• jobbe med metadata p√• Dapla.\nfellesr er installert p√• Dapla og funksjoner kan benyttes ved:\nlibrary(fellesr)\nHvis du benytte en renv milj√∏, m√• pakken installeres en gang. Dette kan gj√∏res ved:\nrenv::install(\"statisticsnorway/fellesr\")"
  },
  {
    "objectID": "jobbe-med-data.html#vanlige-operasjoner",
    "href": "jobbe-med-data.html#vanlige-operasjoner",
    "title": "Jobbe med data",
    "section": "",
    "text": "I denne delen viser vi hvordan man gj√∏r veldig vanlige operasjoner n√•r man koder et produksonsl√∏p for en statistikk. Flere eksempler p√• nyttige systemkommandoer finner du her.\n\n\n\n\n\n\n\n\nEksempeldata\n\n\n\nDet finnes et omr√•de som alle SSB-ansatte har lese- og skrivetilgang til. Det er\ngs://ssb-prod-dapla-felles-data-delt/ i prod-milj√∏et p√• Dapla, og\ngs://ssb-staging-dapla-felles-data-delt/ i staging-milj√∏et. Eksemplene under bruker f√∏rstnevnte i koden, slik at alle kan kj√∏re koden selv.\nKode-eksemplene finnes for b√•de R og Python, og du kan velge hvilken du skal se ved √• trykke p√• den arkfanen du er interessert i.\n\n\n√Ö liste ut innhold i et gitt mappe p√• Dapla er ganske enkelt. Under ser du hvordan du kan liste ut innholdet i f√∏lgende mappe:\ngs://ssb-prod-dapla-felles-data-delt/felles/veiledning/python/eksempler/purchases\n\nPython \n\n\nVi bruker modulen FileClient fra dapla-toolbelt for √• liste ut innholdet i en mappe.\nfrom dapla import FileClient\n\n# Set path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\nFileClient.ls(f\"{bucket}/{folder}\")\nMed kommandoen over f√•r du listet ut alle filnavn i mappen. Hvis du vil ha mer informasjon om filene s√• kan du bruke ls-kommandoen med detail = True, som under:\nFileClient.ls(f\"{bucket}/{folder}\", detail = True)\nSyntaksen med ls er veldig lik det man kjenner fra en Linux-terminal. Men n√•r vi bruker detail = True blir metadata om filene returnert som en Python-liste med dictionaries. Det kan v√¶re sv√¶rt nyttig n√•r du f.eks. trenger √• vite dato og tidspunkt for n√•r en fil ble opprettet, eller n√•r den sist ble oppdatert.\n\n\n# Loading functions into notebook\nlibrary(fellesr)\n\n# Path to folder\nbucket &lt;- \"ssb-prod-dapla-felles-data-delt/\"\nfolder &lt;- \"felles/veiledning/python/eksempler/purchases\"\n\n# List files in folder \nlist.files(paste0(bucket, folder))\nMerknad: N√•r du spesifisere b√∏tter i R, trenger du ikke ‚Äúgs://‚Äù foran.\n\n\n\n\n\n\n√Ö skrive filer til et lagringsomr√•de p√• Dapla er ogs√• ganske enkelt. Det ligner mye p√• den syntaksen vi er kjent med fra vanlige R- og Python-pakker, med noen sm√• unntak.\n\n\nUnder lager vi en dataframe i en notebook og skriver den ut til en parquet-fil.\n\nPython \n\n\nN√•r vi leser en Parquet-fil med dapla-toolbelt s√• bruker den pyarrow i bakgrunnen. Dette er en av de raskeste m√•tene √• lese og skrive Parquet-filer p√•.\n\nimport dapla as dp\nimport pandas as pd\nimport numpy as np\n\n# Set path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Create pandas dataframe\npurchases = pd.DataFrame(np.random.randn(10, 5),\n                        columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Write pandas dataframe as parquet to GCS bucket\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{bucket}/{folder}/data.parquet\",\n                file_format = \"parquet\",)\nN√•r vi kalte write_pandas over s√• spesifiserte vi at filformatet skulle v√¶re parquet. Dette er default, s√• vi kunne ogs√• ha skrevet det slik:\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{bucket}/{folder}/data.parquet\")\nMen for de andre filformatene m√• vi alts√• spesifisere dette.\n\n\nN√•r vi jobber med Parquet-fil i R, bruker vi pakken arrow. Dette er en del av fellesr pakken s√• du trenger kun √• kalle inn dette. Pakken inneholder funksjonen write_SSB som kan brukes til √• skrive data til b√∏tte p√• Dapla.\nlibrary(fellesr)\n\n# Set stien til hvor data skal lagres\nbucket &lt;- \"ssb-prod-dapla-felles-data-delt\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\n# Lage en eksempel dataframe\npurchases = data.frame(A = runif(10), B= runif(10), C=runif(10))\n\n# Skrive data til b√∏ttet som en parquet\nwrite_SSB(purchases, file.path(bucket, folder, \"purchases.parquet\"))\nMerknad: N√•r du spesifisere b√∏tter i R, trenger du ikke ‚Äúgs://‚Äù foran.\n\n\n\n\n\n\nNoen ganger √∏nsker vi √• lagre data i andre formatter slik som CSV, JSON og XML.\n\nPython \n\n\ndapla-toolbelt kan skrive ut json, csv og posisjonsfiler (fixed-width-files/fwf) til lagringsomr√•det. M√•ten den gj√∏r det p√• er √• bruke Pandas sine funksjoner read_json, read_csv, read_fwf under panseret. Dette kan v√¶re nyttig √• vite for skj√∏nne hvordan dapla-toolbelt h√•ndterer ulike strukturer i (spesielt hierarkiske) tekstfiler. Under ser du hvordan du kan skrive ut en dataframe til en json-fil.\nimport numpy as np\nimport pandas as pd\nfrom dapla import FileClient\n\n# Set path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Create a dataframe with Pandas\ndf = pd.DataFrame(np.random.randn(10, 5), columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Save dataframe as json with dapla-toolbelt\ndp.write_pandas(df = df,\n                gcs_path = f\"{bucket}/{folder}/test.json\",\n                file_format = \"json\")\nSom vi ser at syntaksen over s√• kunne vi skrevet ut til noe annet enn json ved √• endre verdien i argumentet file_format.\n\n\nPakken fellesr kan ogs√• brukes til √• skrive andre type filer, for eksempel csv, til b√∏tter. Dette gj√∏res med funksjonen write_SSB og spesifisere √∏nsket filtype i filnavn.\nF√∏rst kaller vi biblioteket og lage noe test data ved:\nlibrary(fellesr)\n\n# Set stien til hvor data skal lagres\nbucket &lt;- \"ssb-prod-dapla-felles-data-delt\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\n# Lage en eksempel dataframe\npurchases = data.frame(A = runif(10), B= runif(10), C=runif(10))\n\n# Skrive til csv\nwrite_SSB(purchases, file.path(bucket, folder, \"purchases.csv\")\n\n\n\n\n\n\nDet er ikke anbefalt √• bruke xlsx-formatet, men her ser du hvordan det kan skrives ut. Mer kommer.\n\nPython \n\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Henter token for √• kunne lese fra Dapla\ntoken = AuthClient.fetch_google_credentials()\n\n# Filsti\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\ndf.to_excel(f\"{bucket}/{folder}/test.xlsx\",\n           storage_options={\"token\": token})\n\n\nKommer snart\n\n\n\n\n\n\n\nUnder finner du eksempler p√• hvordan du kan lese inn data til en Jupyter Notebooks p√• Dapla.\n\n\n\nPython \n\n\nimport dapla as dp\n\n# Set path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Read path into pandas dataframe \ndp.read_pandas(gcs_path= f\"{bucket}/{folder}/data.parquet\",\n               file_format = \"parquet\",\n               columns = None,)\nSom vi s√• med write_pandas s√• er file_format default satt til parquet, og default for columns = None, s√• vi kunne ogs√• ha skrevet det slik:\ndp.read_pandas(gcs_path= f\"{bucket}/{folder}/data.parquet\")\ncolumns-argumentet er en liste med kolonnenavn som vi √∏nsker √• lese inn. Hvis vi ikke spesifiserer noen kolonner s√• vil alle kolonnene leses inn.\n\n\nPakken fellesr kan brukes til √• lese inn data. Funksjonen read_SSB() kan lese inn filer i flere format inkluderende parquet.\nHer er et eksempel av √• lese inn parquet fil ‚Äú1987‚Äù.\nlibrary(fellesr)\n\nbucket &lt;- \"ssb-prod-dapla-felles-data-delt\"\nfolder &lt;- \"R_smoke_test\"\n\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.parquet\"))\nVi kan ogs√• filtrere hvilke variabel vi √∏nsker √• lese inn ved √• spesifisere parameter col_select. For eksempel:\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.parquet\"),\n                    col_select = c(\"Year\", \"Month\"))\nInnlesning av parquet som er kartdata finner du her: Lese kartdata\n\n\n\n\n\n\nKommer mer snart. Python-koden under bygger p√• eksempelet over.\n\nPython \n\n\nimport dapla as dp\n\n# Path to write to\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Read in json-file from dapla-storage\ndf = dp.read_pandas(gcs_path = f\"{bucket}/{folder}/test3.json\",\n               file_format = \"json\")\n\n\nFunksjonen read_SSB() kan lese inn flere type av fil-format, slik som csv og json. Du trenger ikke √• endre koden, kun spesifisere hele filnavn.\nF√∏rst kaller vi inn biblioteket fellesr og spesifisere b√∏tte/mappen:\nlibrary(fellesr)\n\n# Filsti\nbucket &lt;- \"ssb-prod-dapla-felles-data-delt\"\nfolder &lt;- \"R_smoke_test\"\n\n# Lese inn CSV-fil\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.csv\"))\nFor √• lese inn en json-fil kan skrive f√∏lgende:\ndt_1987 &lt;- read_SSB(file.path(bucket, folder, \"1987.json\"))\n\n\n\n\n\n\n\nPython \n\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Hent token\ntoken = AuthClient.fetch_google_credentials()\n\n# Les inn fil\ndf = pd.read_excel(\"gs://ssb-prod-arbmark-skjema-data-produkt/test_gcp.xlsx\",\n    storage_options={\"token\": token})\n\n\nKommer snart\n\n\n\n\n\n\nHer er et eksempel p√• hvordan man leser inn en sas7bdat-fil p√• Dapla som har blitt generert i prodsonen.\n\nPython \n\n\nimport pandas as pd\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\nsti = \"gs://ssb-prod-dapla-felles-data-delt/felles/veiledning/sas/statbank_ledstill.sas7bdat\"\n\nwith fs.open(sti) as sas:\n    df = pd.read_sas(sas, format=\"sas7bdat\", encoding=\"latin1\")\n\n\nI produksjon sone (p√• bakken) kan R-pakken haven benyttes for √• lese inn .sas7bdat filer. Dette er ikke implementert i fellesr enda for lesing fra Dapla b√∏tte.\nMer om dette kommer.\n\n\n\n\n\n\n\n√Ö slette filer fra lagringsomr√•det kan gj√∏res p√• flere m√•ter. I kapitlet om sletting av data viste vi hvordan man gj√∏r det med pek-og-klikk i Google Cloud Console. Under ser du hvordan du kan slette filer med dapla-toolbelt og gcsfs eller fellesr.\n\nPython \n\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\nfs.rm(f\"{bucket}/{from_folder}/df.json\")\n\n\nFunksjonen gc_delete_object kan brukes til √• slette data p√• lagringsomr√•det.\nlibrary(fellesr)\n\nbucket &lt;- \"ssb-prod-dapla-felles-data-delt\"\nfolder &lt;- \"felles/veiledning/r/eksempler/purchases\"\n\ngcs_delete_object(file.path(bucket, folder, \"purchases.parquet\"))\n\n\n\n\n\n\n√Ö kopiere filer mellom mapper p√• et Linux-filsystem inneb√¶rer som regel bruke cp-kommandoen. P√• Dapla er det ikke s√• mye forskjell. Vi bruker en ligende tiln√¶rming n√• vi skal kopiere mellom b√∏tter eller mapper p√• lagringsomr√•det til SSB. Under ser du hvordan du kan kopiere en fil fra en mappe til en annen.\n\nPython \n\n\nLa oss begynne med et eksempel der vi kopierer en fil fra en mappe til en annen i samme b√∏tte.\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Path to folders\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\n# Copy file\nfs.cp(f\"{bucket}/{from_folder}/data.parquet\",\n      f\"{bucket}/{to_folder}/data_copy.parquet\")\nDet ogs√• fungere for √• kopiere filer mellom b√∏tter.\nEt annet scenario vi ofte vil st√∏te p√• er at vi √∏nsker √• kopiere en fil fra v√•rt Jupyter-filsystem til en mappe p√• lagringsomr√•det. Her kan vi bruke fs.put-metoden.\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Create a new file in your home directory called test.txt\nwith open('/home/jovyan/test.txt', 'w') as f:\n    f.write('Create a new text file!')\n\n#Path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Copy file from local to remote file system\nfs.put(lpath=f\"/home/jovyan/test.txt\", rpath=f\"{bucket}/{folder}/test.txt\")\n√ònsker vi √• kopiere en hel mappe fra lagringsomr√•det til Jupyter-filsystemet, kan vi bruke fs.get-metoden, med opsjonen recursive=True.\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Copy file\nfs.get(&lt;from_bucket&gt;,\n      \"/home/jovyan/sesongjustering/\",\n      recursive=True)\n\n\nKommer snart\n\n\n\n\n\n\n\nPython \n\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\nfs.mv(f\"{bucket}/{from_folder}/data.parquet\", f\"{bucket}/{to_folder}/data.parquet\")\n\n\nKommer snart\n\n\n\n\n\n\nSelv om b√∏tter ikke har mapper med en hierarkisk struktur slik man er kjent med fra klassike filsystemer, s√• kan man opprette det som ser ut som mapper i objektnavnet. I realiteten blir bare / oppfattet som en del av navnet p√• objektet. Skulle du likevel √∏nske √• opprette dette s√• kan du gj√∏re det f√∏lgende m√•te:\n\nPython \n\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n#Path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Create folder\nfs.touch(f\"{bucket}/{folder}/testmappe/\")\n\n\nKommer snart"
  },
  {
    "objectID": "jobbe-med-data.html#footnotes",
    "href": "jobbe-med-data.html#footnotes",
    "title": "Jobbe med data",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI de tidligere systemene p√• bakken s√• var det ikke n√∏dvendig med autentisering mellom kodemilj√∏ og datalagringen‚Ü©Ô∏é"
  },
  {
    "objectID": "automatisering-avansert.html",
    "href": "automatisering-avansert.html",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Figur¬†1: Infrastruktur\n\n\n\n\n\nSom utgangspunkt, f√•r hver prosesseringsinstans standard ressurstildeling for Cloud Run, dvs. 1 CPU kjerne og 512MB minne. Dette kan v√¶re for lite i noen tilfeller, s√¶rlig for st√∏rre kildedatafiler. Hvis det oppleves at mer minne kreves, ta gjerne kontakt med Dapla Kundeservice for √• ordne dette.\n\n\n\nEn liste over Python pakker man kan benytte seg av i process_source_data.py finnes her. Ta gjerne kontakt med Dapla Kundeservice hvis man har behov for ytterligere.\n\n\n\nHver kilde kan skalere opp med parallele prosesseringsinstanser. Det gj√∏r det kjappere √• prosessere mange filer. I utgangspunktet er dette begrenset til 5 parallele instanser, men det kan √∏kes ved behov.\n\n\n\n√Ö benytte b√•de prod og staging milj√∏er er en god praksis for √• sikre at nye funksjoner og endringer fungerer som forventet f√∏r de rulles ut i produksjon. Staging-milj√∏et gir mulighet til √• validere endringer i en mer kontrollert og isolert setting f√∏r de blir lansert i produksjonsmilj√∏et med skarpe data.\nAutomatiseringsl√∏sningen st√∏tter oppsett av kilder i b√•de staging og prod milj√∏. For oppsett av kilder i staging-milj√∏et, m√• man legge til kilden i en egen undermappe kalt staging. Dette kan gj√∏res p√• samme m√•te som beskrevet her, bortsett fra at kilden legges i en undermappe som heter staging.\nHer er et eksempel p√• konfigurasjon av b√•de staging og produksjonsmilj√∏:\n...\n‚îú‚îÄ‚îÄ automation\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ source_data\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ prod\n‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ kilde1\n‚îÇ¬†¬†     ‚îÇ¬†¬†  ¬†¬† ‚îú‚îÄ‚îÄ config.yaml\n‚îÇ¬†¬†     ‚îÇ¬†¬†  ¬†¬† ‚îî‚îÄ‚îÄ process_source_data.py\n‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ staging\n‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ kilde1\n‚îÇ¬†¬†             ‚îú‚îÄ‚îÄ config.yaml\n‚îÇ¬†¬†             ‚îî‚îÄ‚îÄ process_source_data.py\n...\n\n\n\n\n\n\nInnholdet i config.yaml for b√•de staging og produksjonsmilj√∏ er som hovedregel identisk, men b√∏ttene som benyttes vil v√¶re forskjellige. Stagingmilj√∏et har en egen kildedatab√∏tte kalt gs://ssb-staging-my-project-data-kilde.\n\n\n\n\n\n\nFor √• gi raskt tilbakemelding p√• noen mulige feilsituasjoner, s√• kj√∏res det enkel validering p√• kilde config og process_source_data.py n√•r en PR er opprettet.\nHvis valideringen feiler, s√• m√• feilen rettes f√∏r PRen merges.\nTestene feiler hvis:\n\nKildemappen ikke har et python script kalt process_source_data.py med metodesignaturen, som beskrevet her.\nKildemappen ikke har en yaml fil og en gyldig folder_prefix definert, som i dette eksempelet.\nPython scriptet ikke kan importeres av tjenesten. Tjenesten st√∏tter kun disse tredjeparts pakkene.\nHvis Pyflakes finner feil med kildens Python script.\n\n\n\nHvis validerings testene feiler kan det v√¶re nyttig √• se p√• loggene for √• finne frem til feilen.\n\nFinn frem til testen som feiler, i bildet feiler valideringstestene for kilde1. Trykk s√• p√• lenken ‚ÄúDetails‚Äù som vist i bilde under. \nP√• siden du n√• har kommet til skal det v√¶re en tabell som heter ‚ÄúBuild Information‚Äù, trykk p√• lenken i Build kolonnen. \nDu har n√• kommet frem til loggene, se etter indikasjoner p√• feil. I eksemplet under ser vi at testen test_main_accepts_expected_number_of_args feiler fordi process_source_data.py mangler en main funksjon. \nFiks feilen og push endingen til samme branch, testen vil da starte p√• nytt.\n\n\n\n\n\nEndringer til process_source_data.py blir automatisk rullet ut n√•r en PR er merget til main branchen. Utrullingsprosessen tar noe tid, ca. 2-3 minutter fra branchen er merget til tjenesten er oppdatert, for √• bekrefte at tjenesten er rullet ut kan du f√∏lge stegene i neste avsnitt.\n\n\nStegene under viser hvordan man g√•r frem for √• finne resultat av utrullingen av kilden ‚Äúledstill‚Äù for teamet ‚Äúarbmark-skjema‚Äù. Og forutsetter at koden er pushet til main branchen.\n\nNaviger til GitHub.\nI s√∏kefeltet oppe i venstre hj√∏rne skriv arbmark-skjema og klikk ‚ÄúJump to‚Äù arbmark-skjema-iac. Som i bilde under. \nN√•r utrullingen er ferdig vil en av disse ikonene vises, gr√∏nn hake betyr at tjeneste er rullet ut med koden som ligger i main og at nye filer blir behandlet med koden som ligger der. . R√∏dt kryss indikerer at utrullig har feilet.  Se etter symbolene der hvor den r√∏de pilen i bilde under peker. I eksempel er utrulligen vellykket. \n\n\n\n\n\nMan f√•r en oversikt over kildene man har konfigurert prosessering for og statusen p√• dem ved hjelp av konsollet p√• GCP. Der navigerer man til siden for Cloud Run (se Figur¬†2) som er kj√∏remilj√∏et som kildedata prosessering benyttes av. Eksempel URl er: https://console.cloud.google.com/run?project=&lt;teamets-prosjekt-id&gt;\nHer f√•r man en oversikt av ressursbruk og loggene til prosesseringen.\n\n\n\nFigur¬†2: Cloud Run dashboard\n\n\n\n\nEtter du har valgt kilden kan du se logger ved √• velge fanen ‚ÄúLOGS‚Äù. Her ligger alle logger for den spesifikke kilden. For √• f√• bedre oversikt over eventulle feil kan man sette severity til error. Dette vil uten ekstra konfigurasjon gi oversikt over alle uh√•ndterte exceptions. \n\n\n\nHvis en fil blir mottatt av tjenesten, men ikke lar seg behandle blir det skrevet til loggen. Man kan f√• en oversik over hvilke filer som ikke har blitt prosessert ved √• s√∏ke etter: Could not process object. \n\n\n\n\nNoen ganger vil det v√¶re n√∏dvendig √• trigge kj√∏ring av en kilde uten at de tilh√∏rende filene i kildeb√∏tta er oppdatert f.eks. etter en endring i prosseseringsskriptet. For √• gj√∏re dette kan man benytte seg av dapla toolbelt.\nFor √• trigge en ny kj√∏ring m√• man v√¶re data-admin i teamet og ha denne informasjonen tilgjengelig:\n\nproject_id(prosjekt id) for kilden, den finner man ved √• f√∏lge beskrivelsen her.\nfolder_prefix beskriver stien til filene som skal behandles og fungerer p√• samme m√•te som i config.yaml\nsource_name finner man ved √• se p√• navnet til mappen hvor kilden konfigureres, i eksempelet her ser vi at team smaabakst har to kilder boller og rundstykker.\n\n\n\nDette eksemplet viser hvordan man g√•r frem for √• manuelt trigge kilden boller for team smaabakst.\nTeam smaabakst √∏nsker √• re-prosessere alle filer i kilden boller. Ved √• bruke samme folder_prefix som i config.yaml vil alle filer som tilh√∏rer kilden bli prosessert p√• nytt.\nfrom dapla import trigger_source_data_processing\n\nproject_id = \"prod-smaabakst-b69d\"\nsource_name = \"boller\"\nfolder_prefix = \"boller\"\n\ntrigger_source_data_processing(project_id, source_name, folder_prefix)"
  },
  {
    "objectID": "automatisering-avansert.html#infrastruktur-oversikt",
    "href": "automatisering-avansert.html#infrastruktur-oversikt",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Figur¬†1: Infrastruktur"
  },
  {
    "objectID": "automatisering-avansert.html#ressurser",
    "href": "automatisering-avansert.html#ressurser",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Som utgangspunkt, f√•r hver prosesseringsinstans standard ressurstildeling for Cloud Run, dvs. 1 CPU kjerne og 512MB minne. Dette kan v√¶re for lite i noen tilfeller, s√¶rlig for st√∏rre kildedatafiler. Hvis det oppleves at mer minne kreves, ta gjerne kontakt med Dapla Kundeservice for √• ordne dette."
  },
  {
    "objectID": "automatisering-avansert.html#tilgjengelige-pakker",
    "href": "automatisering-avansert.html#tilgjengelige-pakker",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "En liste over Python pakker man kan benytte seg av i process_source_data.py finnes her. Ta gjerne kontakt med Dapla Kundeservice hvis man har behov for ytterligere."
  },
  {
    "objectID": "automatisering-avansert.html#skalering",
    "href": "automatisering-avansert.html#skalering",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Hver kilde kan skalere opp med parallele prosesseringsinstanser. Det gj√∏r det kjappere √• prosessere mange filer. I utgangspunktet er dette begrenset til 5 parallele instanser, men det kan √∏kes ved behov."
  },
  {
    "objectID": "automatisering-avansert.html#testing-i-staging-milj√∏",
    "href": "automatisering-avansert.html#testing-i-staging-milj√∏",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "√Ö benytte b√•de prod og staging milj√∏er er en god praksis for √• sikre at nye funksjoner og endringer fungerer som forventet f√∏r de rulles ut i produksjon. Staging-milj√∏et gir mulighet til √• validere endringer i en mer kontrollert og isolert setting f√∏r de blir lansert i produksjonsmilj√∏et med skarpe data.\nAutomatiseringsl√∏sningen st√∏tter oppsett av kilder i b√•de staging og prod milj√∏. For oppsett av kilder i staging-milj√∏et, m√• man legge til kilden i en egen undermappe kalt staging. Dette kan gj√∏res p√• samme m√•te som beskrevet her, bortsett fra at kilden legges i en undermappe som heter staging.\nHer er et eksempel p√• konfigurasjon av b√•de staging og produksjonsmilj√∏:\n...\n‚îú‚îÄ‚îÄ automation\n‚îÇ¬†¬† ‚îî‚îÄ‚îÄ source_data\n‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ prod\n‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ kilde1\n‚îÇ¬†¬†     ‚îÇ¬†¬†  ¬†¬† ‚îú‚îÄ‚îÄ config.yaml\n‚îÇ¬†¬†     ‚îÇ¬†¬†  ¬†¬† ‚îî‚îÄ‚îÄ process_source_data.py\n‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ staging\n‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ kilde1\n‚îÇ¬†¬†             ‚îú‚îÄ‚îÄ config.yaml\n‚îÇ¬†¬†             ‚îî‚îÄ‚îÄ process_source_data.py\n...\n\n\n\n\n\n\nInnholdet i config.yaml for b√•de staging og produksjonsmilj√∏ er som hovedregel identisk, men b√∏ttene som benyttes vil v√¶re forskjellige. Stagingmilj√∏et har en egen kildedatab√∏tte kalt gs://ssb-staging-my-project-data-kilde."
  },
  {
    "objectID": "automatisering-avansert.html#validering",
    "href": "automatisering-avansert.html#validering",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "For √• gi raskt tilbakemelding p√• noen mulige feilsituasjoner, s√• kj√∏res det enkel validering p√• kilde config og process_source_data.py n√•r en PR er opprettet.\nHvis valideringen feiler, s√• m√• feilen rettes f√∏r PRen merges.\nTestene feiler hvis:\n\nKildemappen ikke har et python script kalt process_source_data.py med metodesignaturen, som beskrevet her.\nKildemappen ikke har en yaml fil og en gyldig folder_prefix definert, som i dette eksempelet.\nPython scriptet ikke kan importeres av tjenesten. Tjenesten st√∏tter kun disse tredjeparts pakkene.\nHvis Pyflakes finner feil med kildens Python script.\n\n\n\nHvis validerings testene feiler kan det v√¶re nyttig √• se p√• loggene for √• finne frem til feilen.\n\nFinn frem til testen som feiler, i bildet feiler valideringstestene for kilde1. Trykk s√• p√• lenken ‚ÄúDetails‚Äù som vist i bilde under. \nP√• siden du n√• har kommet til skal det v√¶re en tabell som heter ‚ÄúBuild Information‚Äù, trykk p√• lenken i Build kolonnen. \nDu har n√• kommet frem til loggene, se etter indikasjoner p√• feil. I eksemplet under ser vi at testen test_main_accepts_expected_number_of_args feiler fordi process_source_data.py mangler en main funksjon. \nFiks feilen og push endingen til samme branch, testen vil da starte p√• nytt."
  },
  {
    "objectID": "automatisering-avansert.html#utrulling",
    "href": "automatisering-avansert.html#utrulling",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Endringer til process_source_data.py blir automatisk rullet ut n√•r en PR er merget til main branchen. Utrullingsprosessen tar noe tid, ca. 2-3 minutter fra branchen er merget til tjenesten er oppdatert, for √• bekrefte at tjenesten er rullet ut kan du f√∏lge stegene i neste avsnitt.\n\n\nStegene under viser hvordan man g√•r frem for √• finne resultat av utrullingen av kilden ‚Äúledstill‚Äù for teamet ‚Äúarbmark-skjema‚Äù. Og forutsetter at koden er pushet til main branchen.\n\nNaviger til GitHub.\nI s√∏kefeltet oppe i venstre hj√∏rne skriv arbmark-skjema og klikk ‚ÄúJump to‚Äù arbmark-skjema-iac. Som i bilde under. \nN√•r utrullingen er ferdig vil en av disse ikonene vises, gr√∏nn hake betyr at tjeneste er rullet ut med koden som ligger i main og at nye filer blir behandlet med koden som ligger der. . R√∏dt kryss indikerer at utrullig har feilet.  Se etter symbolene der hvor den r√∏de pilen i bilde under peker. I eksempel er utrulligen vellykket."
  },
  {
    "objectID": "automatisering-avansert.html#monitorering-av-tjenesten",
    "href": "automatisering-avansert.html#monitorering-av-tjenesten",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Man f√•r en oversikt over kildene man har konfigurert prosessering for og statusen p√• dem ved hjelp av konsollet p√• GCP. Der navigerer man til siden for Cloud Run (se Figur¬†2) som er kj√∏remilj√∏et som kildedata prosessering benyttes av. Eksempel URl er: https://console.cloud.google.com/run?project=&lt;teamets-prosjekt-id&gt;\nHer f√•r man en oversikt av ressursbruk og loggene til prosesseringen.\n\n\n\nFigur¬†2: Cloud Run dashboard\n\n\n\n\nEtter du har valgt kilden kan du se logger ved √• velge fanen ‚ÄúLOGS‚Äù. Her ligger alle logger for den spesifikke kilden. For √• f√• bedre oversikt over eventulle feil kan man sette severity til error. Dette vil uten ekstra konfigurasjon gi oversikt over alle uh√•ndterte exceptions. \n\n\n\nHvis en fil blir mottatt av tjenesten, men ikke lar seg behandle blir det skrevet til loggen. Man kan f√• en oversik over hvilke filer som ikke har blitt prosessert ved √• s√∏ke etter: Could not process object."
  },
  {
    "objectID": "automatisering-avansert.html#trigge-tjenesten-manuelt",
    "href": "automatisering-avansert.html#trigge-tjenesten-manuelt",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Noen ganger vil det v√¶re n√∏dvendig √• trigge kj√∏ring av en kilde uten at de tilh√∏rende filene i kildeb√∏tta er oppdatert f.eks. etter en endring i prosseseringsskriptet. For √• gj√∏re dette kan man benytte seg av dapla toolbelt.\nFor √• trigge en ny kj√∏ring m√• man v√¶re data-admin i teamet og ha denne informasjonen tilgjengelig:\n\nproject_id(prosjekt id) for kilden, den finner man ved √• f√∏lge beskrivelsen her.\nfolder_prefix beskriver stien til filene som skal behandles og fungerer p√• samme m√•te som i config.yaml\nsource_name finner man ved √• se p√• navnet til mappen hvor kilden konfigureres, i eksempelet her ser vi at team smaabakst har to kilder boller og rundstykker.\n\n\n\nDette eksemplet viser hvordan man g√•r frem for √• manuelt trigge kilden boller for team smaabakst.\nTeam smaabakst √∏nsker √• re-prosessere alle filer i kilden boller. Ved √• bruke samme folder_prefix som i config.yaml vil alle filer som tilh√∏rer kilden bli prosessert p√• nytt.\nfrom dapla import trigger_source_data_processing\n\nproject_id = \"prod-smaabakst-b69d\"\nsource_name = \"boller\"\nfolder_prefix = \"boller\"\n\ntrigger_source_data_processing(project_id, source_name, folder_prefix)"
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.html",
    "href": "notebooks/spark/sparkr-intro.html",
    "title": "Introduksjon til SparkR",
    "section": "",
    "text": "Akkurat som PySpark s√• gir SparkR oss et grensesnitt mot Apache Spark fra R. I denne notebooken viser vi noen eksempler hvordan du gj√∏re vanlige operasjoner med SparkR."
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.html#oppsett",
    "href": "notebooks/spark/sparkr-intro.html#oppsett",
    "title": "Introduksjon til SparkR",
    "section": "Oppsett",
    "text": "Oppsett\nEksemplene i notebooken bruker SparkR (k8s cluster) p√• https://jupyter.dapla.ssb.no/. Det vil si at den kan distribuere kj√∏ringene p√• flere maskiner i Kubernetes.\n\nspark\n\nJava ref type org.apache.spark.sql.SparkSession id 1"
  },
  {
    "objectID": "notebooks/spark/sparkr-intro.html#lese-inn-fil",
    "href": "notebooks/spark/sparkr-intro.html#lese-inn-fil",
    "title": "Introduksjon til SparkR",
    "section": "Lese inn fil",
    "text": "Lese inn fil\n\nfile = read.parquet(\"gs://ssb-prod-dapla-felles-data-delt/temp/timeseries.parquet\")\n\n\nselectedColumns &lt;- select(file, \"Date\", \"Year\", \"Quarter\", \"Month\", \"serie00\", \"serie01\")\nshowDF(selectedColumns, numRows = 5)\n\n+----------+----+-------+-----+------------------+------------------+\n|      Date|Year|Quarter|Month|           serie00|           serie01|\n+----------+----+-------+-----+------------------+------------------+\n|2000-01-01|2000|      1|   01| 9.495232388801012|   19.016168503192|\n|2000-02-01|2000|      1|   02| 10.70952411634649|21.404467063442723|\n|2000-03-01|2000|      1|   03|11.118293927071951| 21.25035527677261|\n|2000-04-01|2000|      2|   04| 9.346911680164684|19.982136698759238|\n|2000-05-01|2000|      2|   05| 9.663303382177363|19.925236690504494|\n+----------+----+-------+-----+------------------+------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "annet.html",
    "href": "annet.html",
    "title": "Annet",
    "section": "",
    "text": "Annet"
  },
  {
    "objectID": "overf√∏ring-av-data.html",
    "href": "overf√∏ring-av-data.html",
    "title": "Overf√∏ring av data",
    "section": "",
    "text": "For √• overf√∏re data mellom bakke og sky brukes Data Transfer, som er en tjeneste i Google Cloud Console. Denne tjenesten kan brukes til √• flytte data b√•de til og fra Linuxstammen og Dapla, og er tilgjengelig for teamets kildedataansvarlige.\nFor √• f√• tilgang til √• overf√∏re filer m√• man be om dette ved opprettelsen av teamet. Ber man om det skjer f√∏lgende:\n\nEn mappe blir opprettet p√• Linux i prodsonen under /ssb/cloud_sync/\nEt Google Project blir opprettet med navn &lt;team navn&gt;-ts.\n\nDette Google-prosjektet er ikke det samme som der du lagrer annen data. Det har navnet &lt;team navn&gt;-ts, og filstiene p√• bakken og sky vises i Figur¬†1.\n\n\n\nFigur¬†1: Hvordan Transfer Service kan flytte filer mellom bakke og sky.\n\n\nTeamets kildedataansvarlige vil v√¶re spesifisert som en del av √• opprette et Dapla-team.\n\n\nEnten man skal overf√∏re filer opp til sky eller ned til bakken s√• bruker man den samme Data Transfer tjenesten. For √• f√• tilgang til denne m√• man f√∏rst logge seg inn i Google Cloud Console. Sjekk at du er logget inn med din SSB-konto (xxx@ssb.no).\n√òverst p√• siden, til h√∏yre for teksten Google Cloud finnes det en prosjektvelger, og her er det viktig √• velge korrekt Google prosjekt. Hvis du trykker p√• prosjektvelgeren vil det √•pnes opp et nytt vindu. Sjekk at det st√•r SSB.NO √∏verst i dette vinduet. Trykk deretter p√• fanen ALL for √• f√• opp alle tilgjengelige Google-prosjekter under organisasjonen ssb.no (Figur¬†2)\n\n\n\nFigur¬†2: Prosjektvelgeren i Google Cloud Console\n\n\nUnder ssb.no vil det ligge flere mapper. √Öpne mappen som heter production og let frem en undermappe som har navnet p√• ditt Dapla-team. Strukturen skal se slik ut:\n    ssb.no\n    ‚îú‚îÄ‚îÄ production\n        ‚îî‚îÄ‚îÄ &lt;teamnavn&gt;\n            ‚îú‚îÄ‚îÄ prod-&lt;teamnavn&gt;\n            ‚îî‚îÄ‚îÄ &lt;teamnavn&gt;-ts\nDet underste niv√•et (prod-&lt;teamnavn&gt; og &lt;teamnavn&gt;-ts) viser prosjektene, niv√•et i mellom er mapper, og toppniv√•et er organisasjonen (ssb.no). Prosjektet &lt;teamnavn&gt;-ts er et separat prosjekt som bare teamets kildedataansvarlige har tilgang til, og det er her tjenesten Data Transfer skal settes opp.\n\nVelg derfor prosjektet &lt;teamnavn&gt;-ts.\nI s√∏kefeltet til Google Cloud Console, skriv Data transfer og trykk p√• det valget som kommer opp.\nF√∏rste gang man kommer inn p√• siden til Transfer Services vil man bli vist en bl√• knapp med teksten Set Up Connection. N√•r du trykker p√• denne vil det dukke opp et nytt felt hvor du f√•r valget Create Pub-Sub Resources. Dette er noe som bare trengs √• gj√∏re √©n gang. Trykk p√• den bl√• CREATE knappen, og deretter trykk p√• Close lenger nede.\nI navigasjonsmenyen til venstre trykk Transfer jobs, og deretter trykk p√• + Create transfer job √∏verst p√• siden for √• opprette en ny overf√∏ringsjobb.\n\n\n\nF√∏lgende oppskrift tar utgangspunkt i siden Create a transfer job (Figur¬†3):\n\n\n\nFigur¬†3: Opprett overf√∏ringsjobb i Google Cloud Console\n\n\n\nVelg POSIX filesystem under ‚ÄúSource type‚Äù og Google cloud storage under ‚ÄúDestination type‚Äù (eller motsatt hvis overf√∏ringsjobben skal g√• fra Dapla til Linuxstammen). Trykk Next step\nNedtrekkslisten ‚ÄúAgent pool‚Äù skal normalt bare ha ett valg: transfer_service_default. Velg denne.\nI feltet ‚ÄúSource directory path‚Äù skal man kun skrive data/tilsky siden overf√∏ringsagenten kun har tilgang til mapper som ligger relativt plassert under /ssb/cloud_sync/&lt;teamnavn&gt;/. Trykk Next step\nVelg en destinasjon for overf√∏ringsjobben. Trykk p√• Browse og velg b√∏tten med navn som passer til ssb-prod-&lt;teamnavn&gt;-data-synk-opp. Vi anbefaler at du ogs√• oppretter en mappe inne i denne b√∏tten. Det gj√∏res ved √• trykke p√• mappeikonet med et +-tegn foran. Skriv inn et passende mappenavn og trykk Select i bunnen av siden. Trykk deretter Next step\nNeste steg ‚ÄúChoose how and when to run this job‚Äù er opp til brukeren √• bestemme. Hvis man f.eks. velger at Data Transfer skal overf√∏re data en gang i uken, vil den kun starte en overf√∏ring hvis det finnes nye data. Trykk Next step\nBeskriv overf√∏ringsjobben, f.eks: ‚ÄúFlytter data for  til sky.‚Äù. Resten av feltene er opp til brukeren √• bestemme. Standardverdiene er OK.\n\nTrykk til slutt p√• den bl√• Create-knappen. Du vil kunne se kj√∏rende jobber under menyen Transfer jobs.\nFor √• sjekke om data har blitt overf√∏rt, skriv inn cloud storage i s√∏kefeltet √∏verst p√• siden og trykk p√• det f√∏rste valget som kommer opp. Her vil du finne en oversikt over alle teamets b√∏tter, deriblant en med navn ssb-prod-&lt;team-name&gt;-data-synk-opp. N√•r overf√∏ringsjobben er ferdig vil du kunne finne igjen dataene i den mappen som ble definert i stegene overnfor.\n\n\n\nOverf√∏ringsjobben settes opp nesten identisk med Overf√∏ring fra Linuxstammen til Dapla med unntak av f√∏lgende:\n\nSteg 1: Velg Google cloud storage under ‚ÄúSource type‚Äù og POSIX filesystem under ‚ÄúDestination type‚Äù\nSteg 2: Velg b√∏tten ssb-prod-&lt;team-name&gt;-data-synk-ned\nStep 3: Velg transfer_service_default som ‚ÄúAgent pool‚Äù og skriv data/frasky inn i feltet for ‚ÄúDestination directory path‚Äù.\n\nFor √• se om data har blitt overf√∏rt til Linuxstammen m√• du n√• g√• til mappen /ssb/cloud_sync/&lt;team-name&gt;/data/frasky fra FileZilla.\nHusk: Du kan alltids g√• tilbake og se p√• tidligere fullf√∏rte jobber, og starte en overf√∏ringsjobb manuelt fra menyen Transfer jobs.\n\n\n\n\nN√•r du har satt opp en, enten for √• overf√∏re fra sky eller til sky, kan du skrive ut data til mappen eller b√∏tten som du har bedt Transfer Service om √• overf√∏re data fra.\nHvis du skal overf√∏re data fra bakken/prodsonen til sky, s√• m√• teamets kildedataansvarlige skrive ut data til Linux-mappen /ssb/cloud_sync/&lt;team navn&gt;/data/tilsky, og det vil ende opp i Dapla-b√∏tta gs://ssb-prod-&lt;team navn&gt;-data-synk-opp Dette kan du gj√∏re med alle programmeringsverkt√∏y som har en kobling til Linux-stammene der dataene ligger. For eksempel:\n\nSAS EG\nSAS-installasjon p√• Linux\nJupyterlab i prodsonen\nRstudio p√• sl-stata-03\n\nSkal du flytte data fra Dapla til bakken/prodsonen, s√• m√• teamets kildedataansvarlige skrive ut data til gs://ssb-prod-&lt;team navn&gt;-data-synk-opp-b√∏tta p√• Dapla. Det er noe man typisk gj√∏r fra Jupyterlab p√• Dapla."
  },
  {
    "objectID": "overf√∏ring-av-data.html#sette-opp-overf√∏ringsjobber",
    "href": "overf√∏ring-av-data.html#sette-opp-overf√∏ringsjobber",
    "title": "Overf√∏ring av data",
    "section": "",
    "text": "Enten man skal overf√∏re filer opp til sky eller ned til bakken s√• bruker man den samme Data Transfer tjenesten. For √• f√• tilgang til denne m√• man f√∏rst logge seg inn i Google Cloud Console. Sjekk at du er logget inn med din SSB-konto (xxx@ssb.no).\n√òverst p√• siden, til h√∏yre for teksten Google Cloud finnes det en prosjektvelger, og her er det viktig √• velge korrekt Google prosjekt. Hvis du trykker p√• prosjektvelgeren vil det √•pnes opp et nytt vindu. Sjekk at det st√•r SSB.NO √∏verst i dette vinduet. Trykk deretter p√• fanen ALL for √• f√• opp alle tilgjengelige Google-prosjekter under organisasjonen ssb.no (Figur¬†2)\n\n\n\nFigur¬†2: Prosjektvelgeren i Google Cloud Console\n\n\nUnder ssb.no vil det ligge flere mapper. √Öpne mappen som heter production og let frem en undermappe som har navnet p√• ditt Dapla-team. Strukturen skal se slik ut:\n    ssb.no\n    ‚îú‚îÄ‚îÄ production\n        ‚îî‚îÄ‚îÄ &lt;teamnavn&gt;\n            ‚îú‚îÄ‚îÄ prod-&lt;teamnavn&gt;\n            ‚îî‚îÄ‚îÄ &lt;teamnavn&gt;-ts\nDet underste niv√•et (prod-&lt;teamnavn&gt; og &lt;teamnavn&gt;-ts) viser prosjektene, niv√•et i mellom er mapper, og toppniv√•et er organisasjonen (ssb.no). Prosjektet &lt;teamnavn&gt;-ts er et separat prosjekt som bare teamets kildedataansvarlige har tilgang til, og det er her tjenesten Data Transfer skal settes opp.\n\nVelg derfor prosjektet &lt;teamnavn&gt;-ts.\nI s√∏kefeltet til Google Cloud Console, skriv Data transfer og trykk p√• det valget som kommer opp.\nF√∏rste gang man kommer inn p√• siden til Transfer Services vil man bli vist en bl√• knapp med teksten Set Up Connection. N√•r du trykker p√• denne vil det dukke opp et nytt felt hvor du f√•r valget Create Pub-Sub Resources. Dette er noe som bare trengs √• gj√∏re √©n gang. Trykk p√• den bl√• CREATE knappen, og deretter trykk p√• Close lenger nede.\nI navigasjonsmenyen til venstre trykk Transfer jobs, og deretter trykk p√• + Create transfer job √∏verst p√• siden for √• opprette en ny overf√∏ringsjobb.\n\n\n\nF√∏lgende oppskrift tar utgangspunkt i siden Create a transfer job (Figur¬†3):\n\n\n\nFigur¬†3: Opprett overf√∏ringsjobb i Google Cloud Console\n\n\n\nVelg POSIX filesystem under ‚ÄúSource type‚Äù og Google cloud storage under ‚ÄúDestination type‚Äù (eller motsatt hvis overf√∏ringsjobben skal g√• fra Dapla til Linuxstammen). Trykk Next step\nNedtrekkslisten ‚ÄúAgent pool‚Äù skal normalt bare ha ett valg: transfer_service_default. Velg denne.\nI feltet ‚ÄúSource directory path‚Äù skal man kun skrive data/tilsky siden overf√∏ringsagenten kun har tilgang til mapper som ligger relativt plassert under /ssb/cloud_sync/&lt;teamnavn&gt;/. Trykk Next step\nVelg en destinasjon for overf√∏ringsjobben. Trykk p√• Browse og velg b√∏tten med navn som passer til ssb-prod-&lt;teamnavn&gt;-data-synk-opp. Vi anbefaler at du ogs√• oppretter en mappe inne i denne b√∏tten. Det gj√∏res ved √• trykke p√• mappeikonet med et +-tegn foran. Skriv inn et passende mappenavn og trykk Select i bunnen av siden. Trykk deretter Next step\nNeste steg ‚ÄúChoose how and when to run this job‚Äù er opp til brukeren √• bestemme. Hvis man f.eks. velger at Data Transfer skal overf√∏re data en gang i uken, vil den kun starte en overf√∏ring hvis det finnes nye data. Trykk Next step\nBeskriv overf√∏ringsjobben, f.eks: ‚ÄúFlytter data for  til sky.‚Äù. Resten av feltene er opp til brukeren √• bestemme. Standardverdiene er OK.\n\nTrykk til slutt p√• den bl√• Create-knappen. Du vil kunne se kj√∏rende jobber under menyen Transfer jobs.\nFor √• sjekke om data har blitt overf√∏rt, skriv inn cloud storage i s√∏kefeltet √∏verst p√• siden og trykk p√• det f√∏rste valget som kommer opp. Her vil du finne en oversikt over alle teamets b√∏tter, deriblant en med navn ssb-prod-&lt;team-name&gt;-data-synk-opp. N√•r overf√∏ringsjobben er ferdig vil du kunne finne igjen dataene i den mappen som ble definert i stegene overnfor.\n\n\n\nOverf√∏ringsjobben settes opp nesten identisk med Overf√∏ring fra Linuxstammen til Dapla med unntak av f√∏lgende:\n\nSteg 1: Velg Google cloud storage under ‚ÄúSource type‚Äù og POSIX filesystem under ‚ÄúDestination type‚Äù\nSteg 2: Velg b√∏tten ssb-prod-&lt;team-name&gt;-data-synk-ned\nStep 3: Velg transfer_service_default som ‚ÄúAgent pool‚Äù og skriv data/frasky inn i feltet for ‚ÄúDestination directory path‚Äù.\n\nFor √• se om data har blitt overf√∏rt til Linuxstammen m√• du n√• g√• til mappen /ssb/cloud_sync/&lt;team-name&gt;/data/frasky fra FileZilla.\nHusk: Du kan alltids g√• tilbake og se p√• tidligere fullf√∏rte jobber, og starte en overf√∏ringsjobb manuelt fra menyen Transfer jobs."
  },
  {
    "objectID": "overf√∏ring-av-data.html#skrive-ut-data",
    "href": "overf√∏ring-av-data.html#skrive-ut-data",
    "title": "Overf√∏ring av data",
    "section": "",
    "text": "N√•r du har satt opp en, enten for √• overf√∏re fra sky eller til sky, kan du skrive ut data til mappen eller b√∏tten som du har bedt Transfer Service om √• overf√∏re data fra.\nHvis du skal overf√∏re data fra bakken/prodsonen til sky, s√• m√• teamets kildedataansvarlige skrive ut data til Linux-mappen /ssb/cloud_sync/&lt;team navn&gt;/data/tilsky, og det vil ende opp i Dapla-b√∏tta gs://ssb-prod-&lt;team navn&gt;-data-synk-opp Dette kan du gj√∏re med alle programmeringsverkt√∏y som har en kobling til Linux-stammene der dataene ligger. For eksempel:\n\nSAS EG\nSAS-installasjon p√• Linux\nJupyterlab i prodsonen\nRstudio p√• sl-stata-03\n\nSkal du flytte data fra Dapla til bakken/prodsonen, s√• m√• teamets kildedataansvarlige skrive ut data til gs://ssb-prod-&lt;team navn&gt;-data-synk-opp-b√∏tta p√• Dapla. Det er noe man typisk gj√∏r fra Jupyterlab p√• Dapla."
  },
  {
    "objectID": "statistikkbanken.html",
    "href": "statistikkbanken.html",
    "title": "Statistikkbanken",
    "section": "",
    "text": "Pakken ‚Äúdapla-statbank-client‚Äù kan brukes til √• overf√∏re tabeller til Statistikkbanken fra Jupyterlab i prodsonen og p√• Dapla. Den henter ogs√• ‚Äúfilbeskrivelsen‚Äù som beskriver formen dataene skal ha n√•r de sendes inn til Statistikkbanken. Og den kan ogs√• hente publiserte data fra Statistikkbanken. Pakken er en python-pakke som baserer seg p√• at dataene (deltabellene) lastes inn i en eller flere pandas DataFrames f√∏r overf√∏ring. Ved √• hente ned ‚Äúfilbeskrivelsen‚Äù kan man validere dataene sine (dataframene) mot denne lokalt, uten √• sende dataene til Statistikkbanken. Dette kan v√¶re til hjelp under setting av formen p√• dataene. √Ö hente publiserte data fra Statistikkbanken kan gj√∏res gjennom l√∏se funksjoner, eller via ‚Äúklienten‚Äù.\nLenker: - Pakken ligger her p√• Pypi. Og kan installeres via poetry med: poetry add dapla-statbank-client - Kodebasen for pakken ligger her, readme-en gir en teknisk innf√∏ring som du kan f√∏lge og kopiere kode fra, og om du finner noe du vil rapportere om bruken av pakken s√• gj√∏r det gjerne under ‚Äúissues‚Äù p√• github-sidene. - Noe demokode ligger i repoet, og kan v√¶re ett godt utgangspunkt √• kopiere og endre fra.\n\n\nStatistikkbanken opererer med ulike databaser for testing og produksjonssetting. Om du vil at tallene faktisk skal oppdateres p√• nettsidene s√• m√• du sende til Statistikkbankens ‚ÄúPROD‚Äù-database. Om du kun vil teste innsending skal du sende til databasen ‚ÄúTEST‚Äù. Disse databasene har de samme lastebrukerne, men passordene er ulike. Om du vil teste innsending m√• du derfor skaffe deg ‚Äútest-passordet‚Äù til den lastebrukeren som du har tilgjengelig. For √• gj√∏re tester via pakken m√• du v√¶re i staging p√• dapla: https://jupyter.dapla-staging.ssb.no/ eller staging i prodosonen: https://sl-jupyter-t.ssb.no/ Om du faktisk vil sende inn data til publisering i Statistikkbanken m√• du v√¶re i dapla-prod (den vanlige): https://jupyter.dapla.ssb.no/ eller i prodsonen p√•: https://sl-jupyter-p.ssb.no/ For √• teste er det fint √• skaffe seg noe data fra fjor√•rets publisering p√• et produksjonsl√∏p man kjenner fra f√∏r. Evt. kan man hente data fra Statistikkbanken og sende disse tilbake snudd rett, og med riktig antall prikke-kolonner lagt til.\n\n\n\nSe mer detaljer i readme-en p√• prosjektets kodebase.\n\n\nFor √• kunne bruke pakken m√• du importere klienten:\nfrom statbank import StatbankClient\nS√• initialiserer du klienten med de innstillingene som oftest er faste p√• tvers av alle innsendingene fra ett produksjonsl√∏p:\nstatcli = StatbankClient(loaduser=\"LAST360\", date=\"2050-01-01\", overwrite=True, approve=2)\nHer vil du bli bedt om √• skrive inn passordet til lastebrukeren.\n\n\n\nOm du har dataene klare er det bare √• overf√∏re, men du m√• vite navnet p√• deltabell-dat-filene. (Statistikkbanken lagrer disse ned igjen som dat-filer.)\ndata_07495 = {\"kargrs01fylker1.dat\" : df_07495_fylker,\n              \"kargrs01landet1.dat\" : df_07495_landet,}\nstatcli.transfer(data_07495, tableid=\"07495\")\nEtter innsending kommer det en link til Statistikkbankens GUI for √• f√∏lge med p√• om innsendingen gikk bra hos dem. Om det var det du √∏nsket, s√• er du n√• ferdig‚Ä¶ Men det finnes mer funksjonalitet her‚Ä¶\n\n\n\nFor √• hente filbeskrivelsen til en hovedtabell bruker du denne metoden under klienten:\nfilbeskrivelse = statcli.get_description(tableid=\"07495\")\nprint(filbeskrivelse)\nMed filbeskrivelsen kan du lett f√• en mal p√• dictionaryet du m√• plassere dataene i:\nfilbeskrivelse.transferdata_template()\nDu kan ogs√• validere dataene dine mot filbeskrivelsen:\nfilbeskrivelse.validate(data_07495, tableid=\"07495\")\n\n\n\n\nDet tas noe ekstra ansvar i pakken rundt avrunding av desimaltall, da filbeskrivelsen inneholder informasjon om hvor mange desimaler som blir lagret per kolonne, kan vi runde av til rett antall samtidig som vi tar ett bevisst valg rundt ‚Äúhvilken vei vi skal runde av‚Äù. P√• barneskolen l√¶rte vi at ved 2,5 avrundet til 0 desimaler, s√• runder vi opp til 3. Det samme skjer i utgangspunktet i SAS og Excel. Python og R runder derimot ‚Äúmot n√¶rmeste partall‚Äù, s√• fra 2,5 blir det rundet til 2, men fra 1,5 blir det ogs√• rundet til 2. Dette er for √• forhindre bias i en retning. Dvs. om alle tall rundes opp, vil en hel kolonne med tall ‚Äúdras oppover‚Äù, ved √• gj√∏re annenhver opp og ned, vil ikke helheten bli ‚Äúdratt en spesifikk vei‚Äù. Siden ‚Äúround to even‚Äù ikke er det folk er vandte til, gj√∏r vi derfor noe annet i denne pakken, enn det som er vanlig oppf√∏rsel i Python. Vi runder opp. Om du bruker f√∏lgende metoden under filbeskrivelsen p√• dataene, s√• vil denne runde oppover, samtidig som den konverterer til en streng for √• bevare formateringen.\ndata_07495 = filbeskrivelse.round_data(data_07495)  # For √• ta vare p√• endringene, s√• m√• du skrive tilbake over variabelen\n\n\n\n\nEn date-widget for √• visuelt endre til en valid dato.\nLagring av overf√∏ring og filbeskrivelses-objekter til json-filer\nUthenting av logg fra klienten\nHenting av publisert data fra statbanken\nRotering av tabeller fra statbanken"
  },
  {
    "objectID": "statistikkbanken.html#testoverf√∏ring-fra-staging---faktisk-oppdatering-fra-prod",
    "href": "statistikkbanken.html#testoverf√∏ring-fra-staging---faktisk-oppdatering-fra-prod",
    "title": "Statistikkbanken",
    "section": "",
    "text": "Statistikkbanken opererer med ulike databaser for testing og produksjonssetting. Om du vil at tallene faktisk skal oppdateres p√• nettsidene s√• m√• du sende til Statistikkbankens ‚ÄúPROD‚Äù-database. Om du kun vil teste innsending skal du sende til databasen ‚ÄúTEST‚Äù. Disse databasene har de samme lastebrukerne, men passordene er ulike. Om du vil teste innsending m√• du derfor skaffe deg ‚Äútest-passordet‚Äù til den lastebrukeren som du har tilgjengelig. For √• gj√∏re tester via pakken m√• du v√¶re i staging p√• dapla: https://jupyter.dapla-staging.ssb.no/ eller staging i prodosonen: https://sl-jupyter-t.ssb.no/ Om du faktisk vil sende inn data til publisering i Statistikkbanken m√• du v√¶re i dapla-prod (den vanlige): https://jupyter.dapla.ssb.no/ eller i prodsonen p√•: https://sl-jupyter-p.ssb.no/ For √• teste er det fint √• skaffe seg noe data fra fjor√•rets publisering p√• et produksjonsl√∏p man kjenner fra f√∏r. Evt. kan man hente data fra Statistikkbanken og sende disse tilbake snudd rett, og med riktig antall prikke-kolonner lagt til."
  },
  {
    "objectID": "statistikkbanken.html#kode-eksempler",
    "href": "statistikkbanken.html#kode-eksempler",
    "title": "Statistikkbanken",
    "section": "",
    "text": "Se mer detaljer i readme-en p√• prosjektets kodebase.\n\n\nFor √• kunne bruke pakken m√• du importere klienten:\nfrom statbank import StatbankClient\nS√• initialiserer du klienten med de innstillingene som oftest er faste p√• tvers av alle innsendingene fra ett produksjonsl√∏p:\nstatcli = StatbankClient(loaduser=\"LAST360\", date=\"2050-01-01\", overwrite=True, approve=2)\nHer vil du bli bedt om √• skrive inn passordet til lastebrukeren.\n\n\n\nOm du har dataene klare er det bare √• overf√∏re, men du m√• vite navnet p√• deltabell-dat-filene. (Statistikkbanken lagrer disse ned igjen som dat-filer.)\ndata_07495 = {\"kargrs01fylker1.dat\" : df_07495_fylker,\n              \"kargrs01landet1.dat\" : df_07495_landet,}\nstatcli.transfer(data_07495, tableid=\"07495\")\nEtter innsending kommer det en link til Statistikkbankens GUI for √• f√∏lge med p√• om innsendingen gikk bra hos dem. Om det var det du √∏nsket, s√• er du n√• ferdig‚Ä¶ Men det finnes mer funksjonalitet her‚Ä¶\n\n\n\nFor √• hente filbeskrivelsen til en hovedtabell bruker du denne metoden under klienten:\nfilbeskrivelse = statcli.get_description(tableid=\"07495\")\nprint(filbeskrivelse)\nMed filbeskrivelsen kan du lett f√• en mal p√• dictionaryet du m√• plassere dataene i:\nfilbeskrivelse.transferdata_template()\nDu kan ogs√• validere dataene dine mot filbeskrivelsen:\nfilbeskrivelse.validate(data_07495, tableid=\"07495\")"
  },
  {
    "objectID": "statistikkbanken.html#problematikk-knyttet-til-avrunding",
    "href": "statistikkbanken.html#problematikk-knyttet-til-avrunding",
    "title": "Statistikkbanken",
    "section": "",
    "text": "Det tas noe ekstra ansvar i pakken rundt avrunding av desimaltall, da filbeskrivelsen inneholder informasjon om hvor mange desimaler som blir lagret per kolonne, kan vi runde av til rett antall samtidig som vi tar ett bevisst valg rundt ‚Äúhvilken vei vi skal runde av‚Äù. P√• barneskolen l√¶rte vi at ved 2,5 avrundet til 0 desimaler, s√• runder vi opp til 3. Det samme skjer i utgangspunktet i SAS og Excel. Python og R runder derimot ‚Äúmot n√¶rmeste partall‚Äù, s√• fra 2,5 blir det rundet til 2, men fra 1,5 blir det ogs√• rundet til 2. Dette er for √• forhindre bias i en retning. Dvs. om alle tall rundes opp, vil en hel kolonne med tall ‚Äúdras oppover‚Äù, ved √• gj√∏re annenhver opp og ned, vil ikke helheten bli ‚Äúdratt en spesifikk vei‚Äù. Siden ‚Äúround to even‚Äù ikke er det folk er vandte til, gj√∏r vi derfor noe annet i denne pakken, enn det som er vanlig oppf√∏rsel i Python. Vi runder opp. Om du bruker f√∏lgende metoden under filbeskrivelsen p√• dataene, s√• vil denne runde oppover, samtidig som den konverterer til en streng for √• bevare formateringen.\ndata_07495 = filbeskrivelse.round_data(data_07495)  # For √• ta vare p√• endringene, s√• m√• du skrive tilbake over variabelen"
  },
  {
    "objectID": "statistikkbanken.html#annen-funksjonalitet-du-kan-lese-om-i-readme-en-er",
    "href": "statistikkbanken.html#annen-funksjonalitet-du-kan-lese-om-i-readme-en-er",
    "title": "Statistikkbanken",
    "section": "",
    "text": "En date-widget for √• visuelt endre til en valid dato.\nLagring av overf√∏ring og filbeskrivelses-objekter til json-filer\nUthenting av logg fra klienten\nHenting av publisert data fra statbanken\nRotering av tabeller fra statbanken"
  },
  {
    "objectID": "kildedata.html",
    "href": "kildedata.html",
    "title": "Kildedata",
    "section": "",
    "text": "Kildedata er data lagret slik de ble levert til SSB fra dataeier, det vil si p√• dataeiers dataformat og med informasjon om tidspunkt og rekkef√∏lge for avlevering. Kildedata er en del av statistikkenes dokumentasjon, og kan v√¶re en n√∏dvendig kilde for forskning og nye statistikker. Uten kildedataene vil det ikke v√¶re mulig √• etterpr√∏ve SSB sine statistikker. De originale kildedataene vil ofte komprimeres og krypteres etter at relevante deler er transformert til inndata.\n\n(Standardutvalget 2021, 7)\n\nStatistikkloven ¬ß 9 Informasjonssikkerhet stiller krav om at direkte identifiserende opplysninger skal behandles og lagres adskilt fra √∏vrige opplysninger, med mindre det vil v√¶re uforenlig med form√•let med behandlingen eller √•penbart un√∏dvendig. I henhold til policy om Datatilstander er kildedata i utgangspunktet den eneste datatilstanden som kan inneholde denne type data. I √∏vrige tilstander skal direkteidentifiserende opplysninger som hovedregel v√¶re pseudonymisert. Avvik skal dokumenteres og godkjennes av seksjonsleder som er ansvarlig for avviket.\n\n(Direkt√∏rm√∏tet 2022, 2)\nFordi Kildedata kan inneholde PII1 implementerer Dapla f√∏lgende tiltak:\n\nKildedata er lagret adskilt fra andre datatilstander.\nTilgang til dataene begrenses s√• langt som mulig, kun en begrenset gruppe personer2 har tilgang til kildedata.\nProsessering av kildedata utf√∏res automatisk for minske behov for tilgang til dataene."
  },
  {
    "objectID": "kildedata.html#footnotes",
    "href": "kildedata.html#footnotes",
    "title": "Kildedata",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPersonidentifiserende Informasjon‚Ü©Ô∏é\nData admins‚Ü©Ô∏é"
  },
  {
    "objectID": "kartdata.html",
    "href": "kartdata.html",
    "title": "Kartdata",
    "section": "",
    "text": "Det er tilrettelagt kartdata i Dapla til analyse, statistikkproduksjon og visualisering. Det er de samme dataene som er i GEODB i vanlig produksjonssone. Kartdataene er lagret p√• ssb-prod-kart-data-delt. De er lagret som parquetfiler i standardprojeksjonen vi bruker i SSB (UTM sone 33N) hvis ikke annet er angitt. Det er ogs√• SSBs standard-rutenett i ulike st√∏rrelser samt Eurostats rutenett over Norge.\nMan s√∏ker om tilgang til dataene til kundeservice (tilgangsrollen kart-consumers), men bruk gjerne standard LDA-prosedyre som for √∏vrige data.\nI tillegg ligger det noe testdata i fellesb√∏tta her: ssb-prod-dapla-felles-data-delt/GIS/testdata\n\n\n\n\nGeopandas er et Python-bibliotek som integrerer Pandas med GIS-analysering og kartlegging. Geopandas er bygd opp rundt GeoDataFrames, som er en pandas DataFrame med geografisk informasjon. Disse kan lastes inn direkte hvis man har geografiske filer, eller man kan konvertere fra DataFrames med koordinat-kolonner (se eksempel under).\nGeopandas kan blant annet brukes til √• kartlegge dataene, beregne avstander og labe variabler for n√¶rmilj√∏ ved √• koble datasett sammen basert p√• geografisk overlapp.\nFor lesing og skriving av geodata i Dapla, kan man bruke SSB-pakken sgis. Med denne kan man blant annet ogs√• beregne reisetider langs veg og lage publiseringsverdige kart.\nPakkene kan installeres i Dapla s√•nn her:\npoetry add geopandas\npoetry add ssb-sgis\nOg s√• importeres det i Python p√• vanlig vis.\nimport geopandas as gpd\nimport sgis as sg\nEksempel p√• lesing/skriving av kartdata\nMan kan lese/skrive geodata i Dapla med sgis-funksjonene read_geopandas og write_geopandas. St√∏ttede formater er blant annet (geo)parquet, geojson og gml. En rekke kartdata for hele landet til analyse- og visualiseringsform√•l ligger i b√∏tta ‚Äúkart‚Äù. For eksempel kommuneflater til analyse eller statistikk:\nfilsti = \"ssb-prod-kart-data-delt/kartdata_analyse/klargjorte-data/2023/ABAS_kommune_flate_p2023_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\nEksempel p√• lesing av kommuneflater for visualisering. Disse dataene er generalisert slik at de egner seg for √• lage kart, men blir un√∏yaktige til statistikk:\nfilsti = \"ssb-prod-kart-data-delt/kartdata_visualisering/klargjorte-data/2023/parquet/N5000_kommune_flate_p2023_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\nTilsvarende for skriving til parquet eller annet geodataformat:\nsg.write_geopandas(kommuner, filsti_ny) \n\n\n\nI tillegg til kartdataene, ligger det testdata i fellesb√∏tta som man kan leke seg med. Testdataene finnes for Oslo og Kongsvinger, og kan leses s√•nn her:\ntestdatasti = \"ssb-prod-dapla-felles-data-delt/GIS/testdata\"\nveger = sg.read_geopandas(f\"{testdatasti}/veger_oslo.parquet\")\nUnder f√∏lger noen eksempler p√• GIS-prosessering med testdataene.\nEksempel p√• avstandsberegning\nAvstand i luftlinje finner man enkelt med sjoin_nearest. S√•nn her finner man avstand fra boligbygg (1000 tilfeldige) i Oslo til n√¶rmeste butikkbygg.\nboligbygg = sg.read_geopandas(f\"{testdatasti}/noen_boligbygg_oslo.parquet\")\nbutikkbygg = sg.read_geopandas(f\"{testdatasti}/butikkbygg_oslo.parquet\")\n\nboligbygg_med_butikkavstand = boligbygg.sjoin_nearest(butikkbygg, distance_col=\"meter_til_butikk\")\nFor √• finne avstand eller reisetid langs veier, kan man gj√∏re nettverksanalyse med sgis. Man m√• f√∏rst klargj√∏re vegnettet og bestemme regler for beregningen(e):\nveger_klargjort = (\n    sg.get_connected_components(veger)\n    .query(\"connected == 1\")\n    .pipe(sg.make_directed_network_norway)\n)\n\nregler = sg.NetworkAnalysisRules(directed=True, weight=\"minutes\")\n\nnwa = sg.NetworkAnalysis(network=veger_klargjort, rules=regler)\nS√• kan man beregne reisetider fra boligbygg til butikkbygg:\navstander = nwa.od_cost_matrix(boligbygg, butikkbygg)\nKorteste reisetid per bolig kan kobles p√• som kolonne i boligdataene s√•nn her:\nboligbygg[\"minutter_til_butikk\"] = avstander.groupby(\"origin\")[\"minutes\"].min()\nUnders√∏k resultatene i interaktivt kart:\nsg.explore(boligbygg, butikkbygg, \"minutter_til_butikk\")\nSe mer informasjon og eksempler i dokumentasjonen.\nOg flere nettverksanalyse-eksempler her.\nEksempel p√• geografisk kobling\nDatasett kan kobles basert p√• geografisk overlapp med geopandas.sjoin. Denne fungerer akkurat som pandas.merge, bare at det kobles basert p√• geometrien.\nKodesnutten under returnerer √©n kommuneflate for hvert bygg som overlapper, med kolonner fra begge datasettene. Inner-join er default, men med left-join beholder man ogs√• geometriene som ikke overlapper.\nkommuner = sg.read_geopandas(f\"{testdatasti}/enkle_kommuner.parquet\")\nkommuner_med_boliginfo = kommuner.sjoin(boligbygg, how=\"left\")\nMed motsatt rekkef√∏lge, f√•r man ett boligbygg per kommune den overlapper med. Siden byggene er punkter, vil hvert bygg havne i maksimalt √©n kommune.\nboligbygg_med_kommuneinfo = boligbygg.sjoin(kommuner, how=\"left\")\nEksempel p√• √• lage kart\nMan kan lage tematiske kart med sgis-klassen ThematicMap. Her er et enkelt eksempel p√• et kart over arealet i kommuner.\nkommuner[\"Kvadratmeter\"] = kommuner.area\n\nm = sg.ThematicMap(kommuner, column=\"Kvadratmeter\", size=15)\nm.title = \"Areal i kommunene\"\nm.plot()\nSe flere kart-eksempler her.\nKonvertere DataFrame til GeoDataFrame\nEn pandas.DataFrame med koordinat-kolonner, kan konverteres til GeoDataFrame s√•nn her:\nimport pandas as pd\ndf = pd.DataFrame({\"x\": [10.8, 10.7, 10.9], \"y\": [59.9, 60, 59.85]})\n\ngdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y), crs=4326)\nFor √• beregne avtand i meter og kunne koble med annen geodata i Dapla, m√• man ha UTM-koordinater (hvis man ikke hadde det fra f√∏r):\ngdf = gdf.to_crs(25833)\nSe ogs√• geopandas dokumentasjon for mer utfyllende informasjon.\n\n\n\n\nDen viktigste GIS-pakken i R er sf. Pakken er bygd opp rundt sf-objekter, som er data.frames med geografisk informasjon. Man kan gj√∏re standard tidyverse-opersjoner p√• sf-objektene, pluss GIS-operasjoner.\nSindre Mikael Haugen har lagd funksjoner for √• lese og skrive blant annet geodata i Dapla. For √• f√• geodata, setter man parametret ‚Äòsf‚Äô til TRUE:\nlibrary(fellesr)\n\nveger = read_SSB(\"ssb-prod-dapla-felles-data-delt/GIS/testdata/veger_oslo.parquet\", sf = TRUE)\nHaugen har ogs√• lagd en pakke for √• gj√∏re nettverksanalyse, som ogs√• lar deg geokode adresser, alts√• √• finne adressenes koordinater.\nlibrary(GISSB)\nLite eksempel p√• kobling basert geografisk sammenfall (enkle kommuner er egentlig til visualisering). Her f√•r man ett bygg per kommune som overlapper (som maksimalt er √©n kommune siden dette er bygningspunkter):\nlibrary(sf)\n\nboligbygg = read_SSB(\"ssb-prod-dapla-felles-data-delt/GIS/testdata/noen_boligbygg_oslo.parquet\", sf = TRUE)\nenkle_kommuner = read_SSB(\"ssb-prod-dapla-felles-data-delt/GIS/testdata/enkle_kommuner.parquet\", sf = TRUE)\n\nboligbygg_med_kommuneinfo = st_join(boligbygg, enkle_kommuner)\nMed motsatt rekkef√∏lge, f√•r man √©n kommuneflate per bolig som overlapper:\nkommuner_med_boliginfo = st_join(enkle_kommuner, boligbygg)"
  },
  {
    "objectID": "kartdata.html#python",
    "href": "kartdata.html#python",
    "title": "Kartdata",
    "section": "",
    "text": "Geopandas er et Python-bibliotek som integrerer Pandas med GIS-analysering og kartlegging. Geopandas er bygd opp rundt GeoDataFrames, som er en pandas DataFrame med geografisk informasjon. Disse kan lastes inn direkte hvis man har geografiske filer, eller man kan konvertere fra DataFrames med koordinat-kolonner (se eksempel under).\nGeopandas kan blant annet brukes til √• kartlegge dataene, beregne avstander og labe variabler for n√¶rmilj√∏ ved √• koble datasett sammen basert p√• geografisk overlapp.\nFor lesing og skriving av geodata i Dapla, kan man bruke SSB-pakken sgis. Med denne kan man blant annet ogs√• beregne reisetider langs veg og lage publiseringsverdige kart.\nPakkene kan installeres i Dapla s√•nn her:\npoetry add geopandas\npoetry add ssb-sgis\nOg s√• importeres det i Python p√• vanlig vis.\nimport geopandas as gpd\nimport sgis as sg\nEksempel p√• lesing/skriving av kartdata\nMan kan lese/skrive geodata i Dapla med sgis-funksjonene read_geopandas og write_geopandas. St√∏ttede formater er blant annet (geo)parquet, geojson og gml. En rekke kartdata for hele landet til analyse- og visualiseringsform√•l ligger i b√∏tta ‚Äúkart‚Äù. For eksempel kommuneflater til analyse eller statistikk:\nfilsti = \"ssb-prod-kart-data-delt/kartdata_analyse/klargjorte-data/2023/ABAS_kommune_flate_p2023_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\nEksempel p√• lesing av kommuneflater for visualisering. Disse dataene er generalisert slik at de egner seg for √• lage kart, men blir un√∏yaktige til statistikk:\nfilsti = \"ssb-prod-kart-data-delt/kartdata_visualisering/klargjorte-data/2023/parquet/N5000_kommune_flate_p2023_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\nTilsvarende for skriving til parquet eller annet geodataformat:\nsg.write_geopandas(kommuner, filsti_ny) \n\n\n\nI tillegg til kartdataene, ligger det testdata i fellesb√∏tta som man kan leke seg med. Testdataene finnes for Oslo og Kongsvinger, og kan leses s√•nn her:\ntestdatasti = \"ssb-prod-dapla-felles-data-delt/GIS/testdata\"\nveger = sg.read_geopandas(f\"{testdatasti}/veger_oslo.parquet\")\nUnder f√∏lger noen eksempler p√• GIS-prosessering med testdataene.\nEksempel p√• avstandsberegning\nAvstand i luftlinje finner man enkelt med sjoin_nearest. S√•nn her finner man avstand fra boligbygg (1000 tilfeldige) i Oslo til n√¶rmeste butikkbygg.\nboligbygg = sg.read_geopandas(f\"{testdatasti}/noen_boligbygg_oslo.parquet\")\nbutikkbygg = sg.read_geopandas(f\"{testdatasti}/butikkbygg_oslo.parquet\")\n\nboligbygg_med_butikkavstand = boligbygg.sjoin_nearest(butikkbygg, distance_col=\"meter_til_butikk\")\nFor √• finne avstand eller reisetid langs veier, kan man gj√∏re nettverksanalyse med sgis. Man m√• f√∏rst klargj√∏re vegnettet og bestemme regler for beregningen(e):\nveger_klargjort = (\n    sg.get_connected_components(veger)\n    .query(\"connected == 1\")\n    .pipe(sg.make_directed_network_norway)\n)\n\nregler = sg.NetworkAnalysisRules(directed=True, weight=\"minutes\")\n\nnwa = sg.NetworkAnalysis(network=veger_klargjort, rules=regler)\nS√• kan man beregne reisetider fra boligbygg til butikkbygg:\navstander = nwa.od_cost_matrix(boligbygg, butikkbygg)\nKorteste reisetid per bolig kan kobles p√• som kolonne i boligdataene s√•nn her:\nboligbygg[\"minutter_til_butikk\"] = avstander.groupby(\"origin\")[\"minutes\"].min()\nUnders√∏k resultatene i interaktivt kart:\nsg.explore(boligbygg, butikkbygg, \"minutter_til_butikk\")\nSe mer informasjon og eksempler i dokumentasjonen.\nOg flere nettverksanalyse-eksempler her.\nEksempel p√• geografisk kobling\nDatasett kan kobles basert p√• geografisk overlapp med geopandas.sjoin. Denne fungerer akkurat som pandas.merge, bare at det kobles basert p√• geometrien.\nKodesnutten under returnerer √©n kommuneflate for hvert bygg som overlapper, med kolonner fra begge datasettene. Inner-join er default, men med left-join beholder man ogs√• geometriene som ikke overlapper.\nkommuner = sg.read_geopandas(f\"{testdatasti}/enkle_kommuner.parquet\")\nkommuner_med_boliginfo = kommuner.sjoin(boligbygg, how=\"left\")\nMed motsatt rekkef√∏lge, f√•r man ett boligbygg per kommune den overlapper med. Siden byggene er punkter, vil hvert bygg havne i maksimalt √©n kommune.\nboligbygg_med_kommuneinfo = boligbygg.sjoin(kommuner, how=\"left\")\nEksempel p√• √• lage kart\nMan kan lage tematiske kart med sgis-klassen ThematicMap. Her er et enkelt eksempel p√• et kart over arealet i kommuner.\nkommuner[\"Kvadratmeter\"] = kommuner.area\n\nm = sg.ThematicMap(kommuner, column=\"Kvadratmeter\", size=15)\nm.title = \"Areal i kommunene\"\nm.plot()\nSe flere kart-eksempler her.\nKonvertere DataFrame til GeoDataFrame\nEn pandas.DataFrame med koordinat-kolonner, kan konverteres til GeoDataFrame s√•nn her:\nimport pandas as pd\ndf = pd.DataFrame({\"x\": [10.8, 10.7, 10.9], \"y\": [59.9, 60, 59.85]})\n\ngdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y), crs=4326)\nFor √• beregne avtand i meter og kunne koble med annen geodata i Dapla, m√• man ha UTM-koordinater (hvis man ikke hadde det fra f√∏r):\ngdf = gdf.to_crs(25833)\nSe ogs√• geopandas dokumentasjon for mer utfyllende informasjon."
  },
  {
    "objectID": "kartdata.html#r",
    "href": "kartdata.html#r",
    "title": "Kartdata",
    "section": "",
    "text": "Den viktigste GIS-pakken i R er sf. Pakken er bygd opp rundt sf-objekter, som er data.frames med geografisk informasjon. Man kan gj√∏re standard tidyverse-opersjoner p√• sf-objektene, pluss GIS-operasjoner.\nSindre Mikael Haugen har lagd funksjoner for √• lese og skrive blant annet geodata i Dapla. For √• f√• geodata, setter man parametret ‚Äòsf‚Äô til TRUE:\nlibrary(fellesr)\n\nveger = read_SSB(\"ssb-prod-dapla-felles-data-delt/GIS/testdata/veger_oslo.parquet\", sf = TRUE)\nHaugen har ogs√• lagd en pakke for √• gj√∏re nettverksanalyse, som ogs√• lar deg geokode adresser, alts√• √• finne adressenes koordinater.\nlibrary(GISSB)\nLite eksempel p√• kobling basert geografisk sammenfall (enkle kommuner er egentlig til visualisering). Her f√•r man ett bygg per kommune som overlapper (som maksimalt er √©n kommune siden dette er bygningspunkter):\nlibrary(sf)\n\nboligbygg = read_SSB(\"ssb-prod-dapla-felles-data-delt/GIS/testdata/noen_boligbygg_oslo.parquet\", sf = TRUE)\nenkle_kommuner = read_SSB(\"ssb-prod-dapla-felles-data-delt/GIS/testdata/enkle_kommuner.parquet\", sf = TRUE)\n\nboligbygg_med_kommuneinfo = st_join(boligbygg, enkle_kommuner)\nMed motsatt rekkef√∏lge, f√•r man √©n kommuneflate per bolig som overlapper:\nkommuner_med_boliginfo = st_join(enkle_kommuner, boligbygg)"
  },
  {
    "objectID": "hva-er-dapla.html",
    "href": "hva-er-dapla.html",
    "title": "Hva er Dapla?",
    "section": "",
    "text": "Hva er Dapla?\nDapla st√•r for dataplattform, og er en skybasert l√∏sning for statistikkproduksjon og forskning.\n\n\nHvorfor Dapla?\nSom dataplattform skal Dapla stimulerere til √∏kt kvalitet p√• statistikk og forskning, samtidig som den gj√∏r organisasjonen mer tilpasningsdyktig i m√∏te med fremtiden.\n\n\nDen nye skybaserte dataplattformen (Dapla) skal bli viktig for √• effektivisere arbeids-og produksjonsprosesser, den skal sikre effektiv lagring og gjenfinning av data og metadata, og st√∏tte opp under deling av data p√• tvers av statistikkomr√•der.\n\nKilde: Langtidsplan for SSB (2022-2024)\n\n\n\nM√•let med Dapla er √• tilby tjenester og verkt√∏y som lar statistikkprodusenter og forskere produsere resultater p√• en sikker og effektiv m√•te."
  },
  {
    "objectID": "navnestandard-datalagring.html",
    "href": "navnestandard-datalagring.html",
    "title": "Navnestandard datalagring",
    "section": "",
    "text": "Krav til dokumentasjon av datasett\n\n\n\nI SSB er det et krav at datasett i datatilstandene1 (inndata), klargjorte data, statistikk og utdata dokumenteres. Det er spesielt viktig at klargjorte data, som er grunnlag b√•de for statistikk, arkiv og datadeling, dokumenteres godt. Dokumentasjon er n√∏dvendig for at vi skal finne, forst√• og (gjen)bruke SSBs data. De fysiske datasettene og variablene som inng√•r i disse skal dokumenteres med bruk av metadatal√∏sningene DataDoc (datasettdokumentasjon), VarDef 2 (variabeldefinisjoner) og KLASS (kodelister og klassifikasjoner).\nSe ogs√• detaljerte krav til datasett-metadata i dokumentene DataDoc - Krav til dokumentasjon av datasett og for variablene som inng√•r i datasettet DataDoc - Variabelforekomst\nEn viktig del av datadokumentasjonsarbeidet er hvordan datasett versjoneres og organiseres i en standardisert mappestruktur (katalogstruktur). Dette dokumentet beskriver en navnestandard for mappestruktur og datasett (datafilene), i tillegg til regler for versjonering av datasett (datafilene).\n1 Datatilstanden ‚Äúkildedata‚Äù omfattes ikke av denne navnestandarden i og med at kildedata mottas av SSB i mange former/strukturer, og at kildedata i liten grad skal deles internt og eksternt (begrenset tilgang). Det m√• eventuelt vurderes om det skal utarbeides egne retningslinjer for lagring av kildedata. Datatilstanden ‚Äúinndata‚Äù er nevnt i parentes i og med at denne datatilstanden ikke er obligatorisk.\n2 Utvikling av nye VarDef er i skrivende stund ikke p√•begynt. SSBs eksisterende variabeldefinisjonsl√∏sning Vardok skal derfor benyttes fram til ny l√∏sning er klar.\n\n\n\n\n\n\n\n\nKrav til lagringsformater for datasett\n\n\n\nDokumentet ‚ÄúStandardformater for datalagring i SSB‚Äù beskriver krav til filformater og dataformater ved lagring av klargjorte data som skal benyttes til utarbeiding av statistikk, datadeling og arkivering."
  },
  {
    "objectID": "navnestandard-datalagring.html#mapper-kataloger",
    "href": "navnestandard-datalagring.html#mapper-kataloger",
    "title": "Navnestandard datalagring",
    "section": "Mapper (kataloger)",
    "text": "Mapper (kataloger)\n\nStandard lagringsomr√•der (b√∏tter) som opprettes for alle statistikkprodukt-team i SSB\nF√∏lgende lagringsomr√•der (b√∏tter) operettes for alle team:\n\nssb-prod-&lt;teamnavn&gt;-data-kilde : Inneholder ubehandlede r√•data fra datakildene.\nssb-prod-&lt;teamnavn&gt;-data-produkt : Inneholder data knyttet til statistikkproduktet.\nssb-prod-&lt;teamnavn&gt;-data-delt : Inneholder data knyttet til statistikkproduktet som kan deles med andre statistikkteam.\n\nDenne navnestandarden gjelder prim√¶rt for lagringsomr√•dene data-produkt og data-delt, men er ogs√• anbefalt brukt for data-kilde*.\n\n\nDatatilstander, prosesser, permanente data og tempor√¶re data\nI en statistikkproduksjon skal det for hver datatilstand lagres permanente datasett (datafiler). Disse datasettene skal f√∏lge denne navnestandarden inkludert kravet til versjonering og dokumentasjon (metadata). Det er imidlertid viktig √• skille mellom behovet for permanente data og tempor√¶re data. I prosessene som kj√∏res mellom hver datatilstand, f.eks. klargj√∏ringsprosessene mellom inndata og klargjorte data, vil det v√¶re behov for tempor√¶r datalagring. Tempor√¶re data skal aldri deles, og det stilles derfor ingen krav til verken navnestandard, versjonering eller dokumentasjon (metadata) av disse. Det er helt opp til hvert produkt-team hvordan de vil organisere tempor√¶re data. Ved √• skille mellom permanent datalagring og tempor√¶re datalagring oppn√•r vi en optimal l√∏sning b√•de for dataprodusenter (statistikkseksjonene) og data-konsumentene (interne og eksterne brukere). Produsentene f√•r n√∏dvendig fleksibilitet til √• prosessere data i tempor√¶re omr√•der, mens konsumentene f√•r godt dokumenterte og versjonerte data i en standardisert mappe-struktur og tilgjengelig for gjenfinning i en s√∏kbar datakatalog.\n\n\n\nFigur 1: Datatilstander, permanente og tempor√¶re data\n\n\n\n\nStatistikkprodukter og dataprodukter\n\nStatistikkprodukter\nAlle SSBs tidligere og n√•v√¶rende statistikkprodukter inng√•r Statistikkregisteret. F√∏r publisering p√• ssb.no m√• alle statistikkprodukter v√¶re registrert i Statistikkregisteret med informasjon om bl.a. statistikkens navn, emne-omr√•de, eierseksjon og publiseringstidspunkt. I tillegg f√•r statistikkene tildelt et kortnavn. Eksempler p√• statistikk-kortnavn er:\n\n\"kpi\" for konsumprisindeksen\n\"reise\" for reiseunders√∏kelsen\n\"ftot\" for n√¶ringslivstjenester, omsetning etter tjenestetype\n\nKortnavnene er unike og stabilt over tid (uforanderlige). De er derfor valgt som grunnlag for kategorisering/inndeling av datasett i Dapla, dvs. benyttes som grunnlag for navn p√• mappene i lagringsomr√•dene (b√∏ttene).\nStatistikkregisteret har ogs√• et API for √• hente informasjon om alle SSBs statistikker i json- format.\n\n\nDataprodukter\nDet er imidlertid ikke slik at alle data i SSB kan knyttes direkte til en statistikk i Statistikkregisteret. Flere statistikkseksjoner i SSB bearbeider ogs√• data til andre bruksomr√•der og form√•l, eksempelvis klargj√∏ring av data til forskning og utl√•n, bearbeiding av data som skal inng√• som en del av andre statistikker, og data som inng√•r i populasjonsregistre. Denne typen data omtales i dette dokumentet som ‚Äúdataprodukter‚Äù, og i navnestandarden skiller vi mellom ‚Äúdataprodukter‚Äù og ‚Äústatistikkprodukter‚Äù. Det eksisterer ikke et register med ‚Äúkortnavn‚Äù for data-produktene i SSB, men hvert team m√• lage kortnavn ogs√• for dataproduktene. Eksempler p√• dataprodukt-kortnavn er ‚Äúnudb‚Äù (utdanningsdatabasen) og ‚Äúfd_trygd‚Äù (forl√∏psdatabasen for trygdedata).\n\n\n\n\n\n\n‚Äú_data‚Äù - navnekonvensjon for dataprodukter\n\n\n\nFor √• skille mellom dataprodukter og statistikkprodukter skal navnet p√• alle mapper som representerer dataprodukter ha endelsen ‚Äú_data‚Äù, f.eks. ‚Äúnudb_data‚Äù og ‚Äúfd_trygd_data‚Äù.\n\n\n\n\n\nMappestruktur for organisering av datasett i Dapla-teamenes lagringsomr√•der (b√∏tter)\nMed utgangspunkt i standard b√∏tter som opprettes for alle statistikk-team i DAPLA, Statistikkregisteret og datatilstander, er det utarbeidet f√∏lgende regler for mappestrukturen og navngiving av mappene i b√∏ttene (gjelder for data-produkt-b√∏tte og data-delt-b√∏tte, men anbefalt ogs√• for data-kilde- b√∏tte) :\nssb-prod-&lt;team-name&gt;-data-produkt/  \n‚îî‚îÄ &lt;statistikk-kortnavn&gt; | &lt;dataprodukt&gt;_data/  \n   ‚îî‚îÄ‚îÄ &lt;datatilstand&gt;/  \n       ‚îú‚îÄ‚îÄ [datasett-1]  \n       ‚îú‚îÄ‚îÄ [datasett-2]  \n       ‚îî‚îÄ‚îÄ [datasett-NN]\n\n\n\n\n\n\nForklaring av mappe-niv√•er i navenstandarden\n\n\n\n\nF√∏rste niv√• er lagringsomr√•det (b√∏tte)\nAndre niv√• er:\n-&gt; enten kortnavn fra Statistikkregisteret\n-&gt; eller et dataprodukt-navn (‚Äúkortnavn‚Äù)\nTredje niv√• er datatilstand\nFjerde niv√• er datasett (datafiler)\n\n\n\n\nSt√∏tte for ‚Äúegendefinerte under-mapper‚Äù ved behov for organisering av datasett i flere niv√•er\nVed behov er det tillatt √• utvide mappestrukturen med med flere egendefinerte niv√•er (niv√• 4, 5, 6, .., N). Dette kan v√¶re nyttig for team som har veldig mange datasett og har behov for √• gruppere disse i flere undermapper:\nssb-prod-&lt;team-name&gt;-data-produkt/  \n‚îî‚îÄ &lt;statistikk-kortnavn&gt; | &lt;dataprodukt&gt;_data/  \n   ‚îî‚îÄ‚îÄ &lt;datatilstand&gt;/  \n       ‚îî‚îÄ‚îÄ &lt;egen under-mappe&gt;/  \n           ‚îî‚îÄ‚îÄ &lt;egen under-under-mappe&gt;/  \n               ‚îî‚îÄ‚îÄ &lt;.. osv.&gt;/  \n                   ‚îú‚îÄ‚îÄ [datasett-1]  \n                   ‚îú‚îÄ‚îÄ [datasett-2]  \n                   ‚îî‚îÄ‚îÄ [datasett-NN]\n\n\n\n\n\n\nEventuell bruk av egendefinerte niv√•er i mappestrukturen\n\n\n\n\nF√∏rste niv√• er lagringsomr√•det (b√∏tte)\nAndre niv√• er:\n-&gt; enten kortnavn fra Statistikkregisteret\n-&gt; eller et dataprodukt-navn (‚Äúkortnavn‚Äù)\nTredje niv√• er datatilstand\nFjerde niv√• er datasett (datafiler)\nNiv√• 4, 5, osv. er egendefinerte under-mapper\nNederste niv√• er datasett (datafiler)\n\n\n\n\n\nSt√∏tte for tempor√¶re data (temp-mappe) og oppdragsdata (oppdrag-mappe)\nVed behov for lagring av tempor√¶re data (tilsvarende wk-katalogene p√• Linux p√• bakken) er det st√∏tte for √• opprette en temp -mappe. Tempor√¶re data er kun tillatt i data-produkt-b√∏tten, b√∏r fjernes etter en viss tid, og skal ikke deles med andre (kun tilgjengelige innenfor eget team).\nDet er ogs√• anbefalt √• opprette en oppdrag -mappe for team som jobber med oppdragsvirksomhet. Egne regler gjelder for behandling og oppbevaring av oppdragsdata. Det er derfor √∏nskelig at disse organiseres i en egen mappe. Utover dette er det anbefalt √• ha med WebSak-saksnummer til oppdraget enten som en undermappe eller som en del av datasett-navnet.\nssb-prod-&lt;team-name&gt;-data-produkt/  \n‚îî‚îÄ &lt;statistikk-kortnavn&gt; | &lt;dataprodukt&gt;_data/  \n   ‚îî‚îÄ‚îÄ &lt;datatilstand&gt;/  \n   ‚îî‚îÄ‚îÄ &lt;datatilstand&gt;/  \n       ‚îú‚îÄ‚îÄ [datasett-1]  \n       ‚îú‚îÄ‚îÄ [datasett-2]  \n       ‚îî‚îÄ‚îÄ [datasett-NN]\n‚îî‚îÄ temp/  \n   ‚îú‚îÄ‚îÄ [temp-datasett-A]  \n   ‚îî‚îÄ‚îÄ [temp-datasett-X]  \n‚îî‚îÄ oppdrag/  \n   ‚îî‚îÄ‚îÄ &lt;WebSak-saksnummer&gt;/  \n       ‚îú‚îÄ‚îÄ [oppdrag-datasett-Y]  \n       ‚îî‚îÄ‚îÄ [oppdrag-datasett-Z]\n\n\n\n\n\n\nBruk av temp-mappe og oppdrag-mappe\n\n\n\n\nF√∏rste niv√• er lagringsomr√•det (b√∏tte)\nAndre niv√• er:\n-&gt; enten kortnavn fra Statistikkregisteret\n-&gt; eller et dataprodukt-navn (‚Äúkortnavn‚Äù)\nTredje niv√• er datatilstand\nFjerde niv√• er datasett (datafiler)\n\nHer vises ogs√•:\n\ntemp-mappe for tempor√¶re data\noppdrag-mappe for oppdragsdata\n\n\n\n\n\n\nEksempel p√• mappestruktur i data-produkt-b√∏tte\nEksempel ‚ÄúTeam overnaturlig‚Äù\nNedenfor vises et eksempel p√• hvordan et tenkt team ‚Äú Team overnaturlig ‚Äù kan organisere sine tenkte statistikkprodukter ‚Äúufo‚Äù og ‚Äúsuperhelt‚Äù i en mappestruktur:\nssb-prod-team-overnaturlig-data-produkt/  \n‚îî‚îÄ‚îÄ ufo/  \n    ‚îú‚îÄ‚îÄ inndata/  \n    ‚îú‚îÄ‚îÄ klargjorte-data/  \n    ‚îú‚îÄ‚îÄ statistikk/  \n    ‚îî‚îÄ‚îÄ utdata/  \n‚îî‚îÄ‚îÄ superhelt/  \n    ‚îú‚îÄ‚îÄ inndata/  \n    ‚îú‚îÄ‚îÄ klargjorte-data/  \n    ‚îú‚îÄ‚îÄ statistikk/  \n    ‚îî‚îÄ‚îÄ utdata/  \n‚îî‚îÄ‚îÄ temp/  \n‚îî‚îÄ‚îÄ oppdrag/  \nEksempel ‚ÄúTeam reiseliv‚Äù - Seksjon for n√¶ringslivets konjunkturer (S422)\nTeamet har ansvar for 3 statistikk-produkter (kortnavn ‚Äúovernatting‚Äù, ‚Äúreise‚Äù og ‚Äúgrensehandel‚Äù)\n\nEtt ALTINN-skjema og 2 utvalgsinnsamlinger med intervjuer p√• telefon/CATI\nProduksjonsl√∏pet har fokus p√• statistikkprodukter fra kildedata til utdata\n\nssb-prod-reiseliv-data-produkt/  \n‚îî‚îÄ‚îÄ overnatting/  \n     ‚îú‚îÄ‚îÄ inndata/  \n     ‚îú‚îÄ‚îÄ klargjorte-data/  \n     ‚îú‚îÄ‚îÄ statistikk/  \n     ‚îî‚îÄ‚îÄ utdata/  \n‚îî‚îÄ‚îÄ reise/  \n     ‚îú‚îÄ‚îÄ inndata/  \n     ‚îú‚îÄ‚îÄ klargjorte-data/  \n     ‚îú‚îÄ‚îÄ statistikk/  \n     ‚îî‚îÄ‚îÄ utdata/  \n‚îî‚îÄ‚îÄ grensehandel/  \n     ‚îú‚îÄ‚îÄ inndata/  \n     ‚îú‚îÄ‚îÄ klargjorte-data/  \n     ‚îú‚îÄ‚îÄ statistikk/  \n     ‚îî‚îÄ‚îÄ utdata/  \n‚îî‚îÄ‚îÄ temp/\nEksempel ‚ÄúTeam trygd‚Äù - Seksjon for inntekts- og levek√•rsstatistikk (S350)\nTeamet har datainnsamling fra flere av NAV sine register\n\nData klargj√∏res og brukes til flere form√•l, bl.a. utl√•n av data til forskere (FD-Trygd) og levering til microdata.no (S380)\n\n\n\n\n\n\n\nEksempel p√• dataprodukt\n\n\n\nTeam trygd klargj√∏r dataprodukter, ikke statistikkprodukter. Alle dataprodukt-kortnavn har derfor endelsen ‚Äú_data‚Äù i eksempel-mappestrukturen nedenfor.\n\n\nssb-prod-trygd-data-produkt/  \n‚îî‚îÄ‚îÄ barnetrygd_data/  \n    ‚îú‚îÄ‚îÄ inndata/  \n    ‚îú‚îÄ‚îÄ klargjorte-data/  \n    ‚îú‚îÄ‚îÄ statistikk/  \n    ‚îî‚îÄ‚îÄ utdata/  \n‚îî‚îÄ‚îÄ foedsykp_data/  \n    ‚îú‚îÄ‚îÄ inndata/  \n    ‚îú‚îÄ‚îÄ klargjorte-data/  \n    ‚îú‚îÄ‚îÄ statistikk/  \n    ‚îî‚îÄ‚îÄ utdata/  \n‚îî‚îÄ‚îÄ pensj_data/  \n    ‚îú‚îÄ‚îÄ inndata/  \n    ‚îú‚îÄ‚îÄ klargjorte-data/  \n    ‚îú‚îÄ‚îÄ statistikk/  \n    ‚îî‚îÄ‚îÄ utdata/  \n‚îî‚îÄ‚îÄ` ‚Ä¶ osv.  \n‚îî‚îÄ‚îÄ temp/  \n‚îî‚îÄ‚îÄ oppdrag/"
  },
  {
    "objectID": "navnestandard-datalagring.html#filnavn-for-datasett",
    "href": "navnestandard-datalagring.html#filnavn-for-datasett",
    "title": "Navnestandard datalagring",
    "section": "Filnavn for datasett",
    "text": "Filnavn for datasett\nFilnavnet til datasettet skal bygges opp av f√∏lgende elementer:\n\n\n\n\nElement\nForklaring\n\n\n\n\n1\nKort beskrivelse\nKort tekst som forklarer datasettets innhold, f.eks. ‚Äúvarehandel, ‚Äúpersoninntekt‚Äù, ‚Äúgrensehandel_imputert‚Äú eller ‚Äúframskrevne-befolkningsendringer‚Äú\n\n\n2\nPeriode - inneholder data f.o.m. dato\nDatasettet inneholder data fra og med dato/tidspunkt. I filnavnet m√• perioden prefikses med ‚Äú_p‚Äù, eksempel ‚Äú_p2022‚Äù eller ‚Äú_p2022-01-01‚Äù. ‚Äú_p‚Äù er en forkortelse for ‚Äúperiode‚Äù. Se ogs√• gyldige formater for periode (dato/tidspunkt) \n\n\n3\nPeriode - inneholder data t.o.m. dato\nDatasettet inneholder data til og med dato/tidspunkt. Denne brukes ved behov, eksempelvis for datasett som inneholder forl√∏psdata eller datasett med flere perioder/√•rganger.\n\n\n4\nVersjon\nVersjon av datasettet. I filnavnet m√• versjonsnummeret prefikses med ‚Äú_v, eksempel ‚Äúv1‚Äù, ‚Äúv2‚Äù eller ‚Äúv3‚Äù. Se ogs√• eget kapittel om regler for versjonering av datasett.\n\n\n5\nFiltype\nFilendelse som sier noen om filtypen, f.eks. ‚Äú.json‚Äù, ‚Äú.csv‚Äù, ‚Äú.xml‚Äù eller ‚Äú.parquet‚Äù.\n\n\n\n\nFormat for filnavn (datasettnavn)\nFilnavnet skal bygges opp p√• f√∏lgende m√•te:\n&lt;kort-beskrivelse&gt;_p&lt;periode-fra-og-med&gt;_p&lt;perode-til-og- med&gt;_v&lt;versjon&gt;.&lt;filtype&gt;\n\nNoen eksempler p√• gyldige filnavn :\nflygende_objekter_p2019_v1.parquet (inneholder en √•rgang med data)\nufo_observasjoner_p2019_p2020_v1.parquet (inneholder 2 √•rganger med data)\nframskrevne-befolkningsendringer_p2019_p2050_v1.parquet (inneholder data fra 2019 til 2050)\nsykepenger_p2022-01-01_p2022-12-31_v1.parquet (inneholder data fra 01.01.2022 til 31.12.2022)\nutanningsnivaa_p2022-10-01_v1.parquet (inneholder tverrsnittsdata (status) per 01.10.2022)\ngrensehandel_imputert_p2022-10_p2022-12_v1.parquet (inneholder data for okt., nov. og des. 2022)\nomsetning_p2020W15_v1.parquet (inneholder data for uke-nummer 15 (week))\nskipsanloep_p2022B1_v1.parquet (inneholder data for f√∏rste 2 m√•neders-periode i 2022 (bimester))\npensjon_p2018Q1_v1.parquet (inneholder data for f√∏rste kvartal (3-m√•neders-periode) i 2018 (quarter))\nnybilreg_p2022T1_v1.parquet (inneholder data for f√∏rste tertial (4 m√•neders-periode) i 2022)\npersoninntekt_p2022H1_v1.parquet (inneholder data for f√∏rste halv√•r (6-m√•neders-periode) i 2022)\nvarehandel_p2018Q1_p2018Q4_v1.parquet (inneholder data for kvartalene 1, 2,3 og 4 i 2018)\n\n\nEksempel p√• datasett-filer i en mappestruktur\nNedenfor vises et eksempel p√• hvordan ‚Äú Team overnaturlig ‚Äù har organisert sine datasett-filer i en mappe-struktur for sin ‚Äú ufo-statistikk ‚Äù:\nssb-prod-team-overnaturlig-data-produkt/  \n‚îî‚îÄ‚îÄ ufo/  \n    ‚îî‚îÄ‚îÄ inndata/  \n         ‚îú‚îÄ‚îÄ lysfenomen_p2019_v1.parquet  \n         ‚îú‚îÄ‚îÄ lysfenomen_p2020_v1.parquet  \n         ‚îú‚îÄ‚îÄ flygende_objekter_p2019_v1.parquet  \n         ‚îî‚îÄ‚îÄ flygende_objekter_p2020_v1.parquet  \n    ‚îî‚îÄ‚îÄ klargjorte-data/  \n         ‚îú‚îÄ‚îÄ ufo_observasjoner_samlet_p2019_v1.parquet  \n         ‚îî‚îÄ‚îÄ ufo_observasjoner_samlet_p2020_v1.parquet  \n    ‚îî‚îÄ‚îÄ statistikk/  \n         ‚îî‚îÄ‚îÄ ufo_statistikk_p2019_p2020_v1.parquet  \n    ‚îî‚îÄ‚îÄ utdata/  \n         ‚îú‚îÄ‚îÄ ufo_statistikk_fylke_p2019_p2020_v1.csv  \n         ‚îî‚îÄ‚îÄ ufo_statistikk_landet_p2019_p2020_v1.csv  \n‚îî‚îÄ‚îÄ ‚Ä¶ osv.\n\n\n\nTillatte tegn for bruk i filnavn og mappe-navn\nDet er kun tillatt √• bruke alfanumerisk tegn begrenset til:\n\na-z og A-Z\n0-9\n- (bindestrek)\n_ (understrek)\n\n\n\n\n\n\n\nAndre krav til filnavn og mappe-navn\n\n\n\nIkke bruk bokstavene ‚Äú√¶‚Äù, ‚Äú√∏‚Äù og ‚Äú√•‚Äù i filnavn eller i mappe-navn.\nAnbefalingen er at disse erstattes med ‚Äúae‚Äù, ‚Äúoe‚Äù og ‚Äúaa‚Äù.\n\nEksempel: ‚Äúnaering‚Äù, ‚Äúoekonomi‚Äù og ‚Äúlevekaar‚Äù\n\nMellomrom/ordskiller (space) erstattes med bindestrek (‚Äú-‚Äù) eller understrek (‚Äú_‚Äù).\n\nEksempel: ‚Äúskatt_for_personer‚Äù og ‚Äúvann-og-avloep‚Äù\n\nPunktum (‚Äú.‚Äù) er kun tillatt brukt for √• skille filnavnet fra filendelsen (filtypen).\n\nEksempel: ‚Äúpersondata.parquet‚Äù og ‚Äúpersondata.csv‚Äù\n\nIngen andre spesialtegn er tillatt brukt i filnavn eller mappe-navn.\n\n\n\n\n‚ÄúDatapartisjonering‚Äù - alternativ organisering av datasett med flere mapper og filer\nDatatjenester/program-bibliotek som PySpark, PyArrow, Pandas og Dask har funksjonalitet for datapartisjonering. Dette er en teknikk som benyttes for √• splitte opp veldig store datasett til flere sm√• datasett og deretter plassere disse filene i en mappe-struktur. En av fordelene med dette er at konsumenter (brukere) kan jobbe med mindre deler av store datasett. En vanlig praksis er √• dele opp (partisjonere) et stort datasett med mange √•rganger/perioder i flere sm√• √•rgangsfiler. Da vil navnet p√• root-mappen tilsvare navnet p√• filen (hvis vi kun hadde √©n stor fil) , og underkatalogene vil v√¶re periodeinndeling, f.eks. ‚Äú/aargang2019‚Äù og ‚Äú/aargang2020‚Äù. Det er ogs√• mulig √• ‚Äúpartisjonere‚Äù data p√• andre m√•ter for √• st√∏tte parallell-prosessering av store datasett i f.eks. Spark.\n\nEksempel p√• partisjonering av stort datasett\n‚ÄúTeam overnaturlig‚Äù har et stort datasett med mange √•rganger med observasjoner av ‚Äúflygende objekter‚Äù.\nssb-prod-team-overnaturlig-data-produkt/  \n‚îî‚îÄ‚îÄ ufo/  \n    ‚îî‚îÄ‚îÄ inndata/  \n        ‚îî‚îÄ‚îÄ flygende_objekter_p1980_p2020_v1.parquet\nTeamet √∏nsker n√• √• ‚Äúpartisjonere‚Äù denne store datafilen i flere sm√• √•rgangsfiler (med filtype parquet).\nDet finnes flere bibliotek og verkt√∏y som st√∏tter partisjonering av store datafiler, eksempelvis Pandas hvor det er mulig √• partisjonere en dataframe p√• f√∏lgende m√•te n√•r den skrives til en parquet-fil(er):\ndf.to_parquet('./flygende_objekter_p1980_p2020_v1', partition_cols=['aar'])\nSe mer informasjon om datapartisjonering med Pandas: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html?highlight=example\nNedenfor vises et eksempel p√• hvordan en mappestruktur med datasett-filer kan se ut etter utf√∏rt data-partisjonering.\nssb-prod-team-overnaturlig-data-produkt/  \n‚îî‚îÄ‚îÄ ufo/  \n    ‚îî‚îÄ‚îÄ inndata/  \n        ‚îî‚îÄ‚îÄ flygende_objekter_p1980_p2020_v1/  \n            ‚îî‚îÄ‚îÄ aar=1980/  \n                ‚îî‚îÄ‚îÄ data.parquet  \n            ‚îî‚îÄ‚îÄ aar=1981/  \n                ‚îî‚îÄ‚îÄ data.parquet  \n            ‚îî‚îÄ‚îÄ aar=1982/  \n                ‚îî‚îÄ‚îÄ data.parquet  \n            ‚Ä¶ osv.  \n            ‚îî‚îÄ‚îÄ aar=2020/  \n                ‚îî‚îÄ‚îÄ data.parquet"
  },
  {
    "objectID": "virtual-env.html",
    "href": "virtual-env.html",
    "title": "Virtuelle milj√∏er",
    "section": "",
    "text": "Et python virtuelt milj√∏ inneholder en spesifikk versjon av python og et sett med pakker. Pakkene er kun tilgjengelige n√•r det virtuelle milj√∏et er aktivert. Dette gj√∏r at man ung√•r avhengighetskonflikter p√• tvers av prosjekter.\nSe her for mer informasjon om virtuelle milj√∏er.\n\n\nDet er anbefalt √• benytte verkt√∏yet poetry for √• administrere prosjekter og deres virtuelle milj√∏.\nPoetry setter opp virtuelt milj√∏, gj√∏r det enkelt √• oppdatere avhengigheter, sette versjonsbegrensninger og reprodusere prosjektet.\nPoetry gj√∏r dette ved √• lagre avhengigheters eksakte versjon i prosjektets ‚Äúpoetry.lock‚Äù. Og eventuelle begrensninger i ‚Äúpyproject.toml‚Äù. Dette gj√∏r det enkelt for andre √• bygge prosjektet med akkurat de samme pakkene og begrensningene."
  },
  {
    "objectID": "virtual-env.html#python",
    "href": "virtual-env.html#python",
    "title": "Virtuelle milj√∏er",
    "section": "",
    "text": "Et python virtuelt milj√∏ inneholder en spesifikk versjon av python og et sett med pakker. Pakkene er kun tilgjengelige n√•r det virtuelle milj√∏et er aktivert. Dette gj√∏r at man ung√•r avhengighetskonflikter p√• tvers av prosjekter.\nSe her for mer informasjon om virtuelle milj√∏er.\n\n\nDet er anbefalt √• benytte verkt√∏yet poetry for √• administrere prosjekter og deres virtuelle milj√∏.\nPoetry setter opp virtuelt milj√∏, gj√∏r det enkelt √• oppdatere avhengigheter, sette versjonsbegrensninger og reprodusere prosjektet.\nPoetry gj√∏r dette ved √• lagre avhengigheters eksakte versjon i prosjektets ‚Äúpoetry.lock‚Äù. Og eventuelle begrensninger i ‚Äúpyproject.toml‚Äù. Dette gj√∏r det enkelt for andre √• bygge prosjektet med akkurat de samme pakkene og begrensningene."
  },
  {
    "objectID": "introduksjon.html",
    "href": "introduksjon.html",
    "title": "Introduksjon",
    "section": "",
    "text": "Introduksjon\nM√•let med dette kapittelet er √• gi en grunnleggende innf√∏ring i hva som legges i ordet Dapla. I tillegg gis en forklaring p√• hvorfor disse valgene er tatt."
  },
  {
    "objectID": "gcc.html",
    "href": "gcc.html",
    "title": "Google Cloud Console",
    "section": "",
    "text": "Google Cloud er SSBs leverand√∏r av skytjenester som Dapla er bygget p√•.\nGoogle Cloud Console er et web-basert grensesnitt for √• administrere ressurser og tjenester p√• Google Cloud. For √• bruke denne m√• man ha en Google-konto. Alle i SSB har en konto knyttet opp mot Google.\n\n\n\n\n\n\nG√• til Google Cloud Console og logg p√• med din SSB-bruker."
  },
  {
    "objectID": "gcc.html#prosjektvelger",
    "href": "gcc.html#prosjektvelger",
    "title": "Google Cloud Console",
    "section": "Prosjektvelger",
    "text": "Prosjektvelger\n√òverst p√• siden, til h√∏yre for teksten Google Cloud finnes det en prosjektvelger. Her er det viktig √• velge ditt teams Google prosjekt, ettersom teamets ressurser kun er tilgjengelige innenfor prosjektet. Hvis du trykker p√• prosjektvelgeren vil det √•pnes opp et nytt vindu. Sjekk at det st√•r SSB.NO √∏verst i dette vinduet.(Figur¬†1).\n\n\n\nFigur¬†1: Prosjektvelgeren i Google Cloud Console\n\n\n\nVelg prosjekt\nHer vises det hvordan man velger et prosjekt I GCC. Eksempelet benytter Dapla teamet demo stat b og fortsetter fra (Figur¬†1).\n\nSkrive teamnavn i s√∏kefeltet, resultatene burde se ut som i (Figur¬†2).\nTrykk p√• lenken prod-demo-stat-b, som markert med r√∏d pil i (Figur¬†2).\n\n\n\n\n\n\n\nI ID kolonnen ser man prosjektets ID (Figur¬†2).\n\n\n\n\n\n\nFigur¬†2: S√∏k i Prosjektvelgeren\n\n\nHar man gjort alle stegene rett vil det i venstre hj√∏rne se ut som i (Figur¬†3).\n\n\n\nFigur¬†3: Aktivt prosjekt i GCC"
  },
  {
    "objectID": "statistikkproduksjon.html",
    "href": "statistikkproduksjon.html",
    "title": "Statistikkproduksjon",
    "section": "",
    "text": "Statistikkproduksjon\nI forrige del beskrev vi noen grunnleggende ting rundt Dapla. I denne delen tar vi for oss hvordan du kan begynne √• jobbe med skarpe data p√• plattformen.\nKapittelet som beskriver hvordan man logger seg inn p√• Dapla vil fungere uten at du m√• gj√∏re noen forberedelser. Er man koblet p√• SSB sitt nettverk s√• vil alle SSB-ansatte kunne g√• inn p√• plattformen og kode i Python og R. Men du f√•r ikke tilgang til SSBs omr√•de for datalagring p√• plattformen. I praksis vil det si at man kan generere data med kode, men man kan ikke jobbe med skarpe data.\nFor √• f√• muligheten til √• jobbe med skarpe data M√Ö du f√∏rst opprette et dapla-team. Dette er det f√∏rste naturlige steget √• ta n√•r man skal begynne √• jobbe med statistikkproduksjon p√• Dapla. I dette kapittelet vil vi forklare det du trenger √• vite om det √• opprette og jobbe innenfor et team."
  },
  {
    "objectID": "ssbproject.html",
    "href": "ssbproject.html",
    "title": "SSB-project",
    "section": "",
    "text": "SSB-project\nI forrige del forklarte vi hvordan man jobber med skarpe data p√• Dapla. Det neste steget vil ofte v√¶re √• begynne √• utvikle kode i Python og/eller R. Dette inneb√¶rer at man helst skal:\n\nversjonsh√•ndtere med Git\nopprette et GitHub-repo\nopprette et virtuelt milj√∏ som husker hvilke versjoner av pakker og programmeringsspr√•k du brukte\n\nI tillegg m√• alt dette konfigureres for hvordan SSB sine systemer er satt opp. Dette har vist seg √• v√¶re un√∏dvendig krevende for mange. Team Statistikktjenester har derfor utviklet et program som gj√∏r alt dette for deg p√• en enkel m√•te som heter ssb-project.\nVi mener at ssb-project er et naturlig sted √• starte n√•r man skal bygge opp koden i Python eller R. Det gjelder b√•de p√• bakken og p√• sky. I denne delen av boken forklarer vi f√∏rst hvordan du bruker ssb-project i det f√∏rste kapittelet. Siden programmet skjuler mye av kompleksiteten rundt dette, s√• bruker vi de andre kapitlene til √• forklare hvordan man ville satt opp dette uten hjelp av programmet. Dermed vil det v√¶re lett for SSB-ansatte √• skj√∏nne hva som gj√∏res og hvorfor det er n√∏dvendig.\nHer kan du kan lese mer om hvordan et SSB-project kan opprettes."
  },
  {
    "objectID": "pakke-install-bakken.html",
    "href": "pakke-install-bakken.html",
    "title": "Installere pakker",
    "section": "",
    "text": "Installering av pakker i Jupyter milj√∏er p√• bakken (f.eks https://sl-jupyter-p.ssb.no) foreg√•r stort sett helt lik som p√• Dapla. Det er √©n viktig forskjell, og det er at installasjon skjer via en proxy som heter Nexus.\n\n\nPip er ferdig konfigurert for bruk av Nexus og kan kj√∏res som beskrevet for Dapla\n\n\n\nHvis man bruker Poetry for h√•ndtering av pakker i et prosjekt, s√• m√• man kj√∏re f√∏lgende kommando i prosjekt-mappe etter prosjektet er opprettet.\npoetry source add --default nexus `echo $PIP_INDEX_URL`\nDa f√•r man installere pakker som vanlig f.eks\npoetry add matplotlib\n\n\n\n\n\n\nHvis man fors√∏ker √• installere prosjektet i et annet milj√∏ (f.eks Dapla), s√• m√• man fjerner nexus kilden ved √• kj√∏re\npoetry source remove nexus\n\n\n\n\n\n\n\nProsessen med √• installere pakker for R p√• bakken er det samme som p√• Dapla. Noen pakker (for eksempel devtools) kan forel√∏pig ikke installeres p√• bakken p√• egenh√•nd pga 3. parti avhengigheter. Vi jobber med √• finne en l√∏sning til dette.\nFor √• installere arrow, kopier og kj√∏r f√∏lgende kommando i R:\ninstall.packages(\"/ssb/bruker/felles/R_pakker/arrow_11.0.0.2.tar.gz\",\n                repos = NULL,\n                type = \"source\")"
  },
  {
    "objectID": "pakke-install-bakken.html#python",
    "href": "pakke-install-bakken.html#python",
    "title": "Installere pakker",
    "section": "",
    "text": "Installering av pakker i Jupyter milj√∏er p√• bakken (f.eks https://sl-jupyter-p.ssb.no) foreg√•r stort sett helt lik som p√• Dapla. Det er √©n viktig forskjell, og det er at installasjon skjer via en proxy som heter Nexus.\n\n\nPip er ferdig konfigurert for bruk av Nexus og kan kj√∏res som beskrevet for Dapla\n\n\n\nHvis man bruker Poetry for h√•ndtering av pakker i et prosjekt, s√• m√• man kj√∏re f√∏lgende kommando i prosjekt-mappe etter prosjektet er opprettet.\npoetry source add --default nexus `echo $PIP_INDEX_URL`\nDa f√•r man installere pakker som vanlig f.eks\npoetry add matplotlib\n\n\n\n\n\n\nHvis man fors√∏ker √• installere prosjektet i et annet milj√∏ (f.eks Dapla), s√• m√• man fjerner nexus kilden ved √• kj√∏re\npoetry source remove nexus"
  },
  {
    "objectID": "pakke-install-bakken.html#r",
    "href": "pakke-install-bakken.html#r",
    "title": "Installere pakker",
    "section": "",
    "text": "Prosessen med √• installere pakker for R p√• bakken er det samme som p√• Dapla. Noen pakker (for eksempel devtools) kan forel√∏pig ikke installeres p√• bakken p√• egenh√•nd pga 3. parti avhengigheter. Vi jobber med √• finne en l√∏sning til dette.\nFor √• installere arrow, kopier og kj√∏r f√∏lgende kommando i R:\ninstall.packages(\"/ssb/bruker/felles/R_pakker/arrow_11.0.0.2.tar.gz\",\n                repos = NULL,\n                type = \"source\")"
  },
  {
    "objectID": "lokale-utviklingsmilj√∏er.html",
    "href": "lokale-utviklingsmilj√∏er.html",
    "title": "Lokale utviklingsmilj√∏er",
    "section": "",
    "text": "Lokale utviklingsmilj√∏er"
  },
  {
    "objectID": "administrasjon-av-team.html",
    "href": "administrasjon-av-team.html",
    "title": "Administrasjon av team",
    "section": "",
    "text": "I dette kapitlet viser vi hvordan du kan opprette et nytt team eller gj√∏re endringer i et eksisterende team. Typiske endringer er √•:\n\nLegge til eller fjerne medlemmer i et team\nListe ut medlemmer og tilgangsgrupper i et team\n\n\n\nFor √• komme i gang med √• opprette et Dapla-team trengs det en oversikt over teamets medlemmer og hvilke tilgangsgrupper medlemmene skal v√¶re med i. Det trengs ogs√• informasjon om hvilke Dapla-tjenester som er aktuelle for teamet √• ta i bruk. Derfor har det blitt opprettet en egen veileder for dette kalt Dapla Start.\n\n\n\n\n\n\nG√• til Dapla Start for starte bestilling av et nytt Dapla-team.\n\n\n\nN√•r teamet er opprettet f√•r alle medlemmene tilgang til sitt eget prosjekt i Google Cloud Platform (GCP), som er SSBs leverand√∏r av skytjenester. Videre f√•r hvert prosjekt et sett med tjenester og tilganger som knyttes til teamet. Det opprettes ogs√• datalagringsomr√•der (ofte kalt b√∏tter) som bare kan aksesseres av brukere som er med i teamets tilgangsgrupper.\nDapla-teamet vil ogs√• f√• sin egen gruppe i SSBs Active Directory slik at medlemskapet i gruppen kan administreres av Kundeservice.\n\n\n\nFor √• legge til eller fjerne medlemmer i et team m√• du forel√∏pig opprette en Kundeservice-sak. Send en e-post til Kundeservice eller registrer en sak i portalen deres med f√∏lgende innhold:\n\nNavnet p√• Dapla-teamet du √∏nsker tilgang\nHvilken tilgang du s√∏ker om. Typisk er det enten data-admins, developers eller consumers). Les mer om hva de ulike tilgangene inneb√¶rer her.\nBeskrivelse av form√•let med tilgangen.\n\nEndringer i team m√• godkjennes av s√∏ker seksjonsleder f√∏r de blir effektuert.\nFor fjerning av tilganger kan man sende en tilsvarende henvendelse, men man trenger ikke inkludere forklaring av form√•l.\n\n\n\n\n\n\nMidlertidig l√∏sning\n\n\n\nAt endringer i team m√• gj√∏res via Kundeservice er midlertidig. Det jobbes med √• lage et eget verkt√∏y for dette.\n\n\n\n\n\nFor √• se hvem som har de ulike tilgangsrollene i et team, s√• kan man bruke pakken dapla-team-cli fra Jupyter. Pakken er installert for alle og du kan liste ut medlemmer av team ved √• gj√∏re f√∏lgende:\n\nLogg deg inn p√• Dapla.\n√Öpne en ny terminal\nSkriv inn dpteam groups list-members og trykk Enter\nI prompten som dukker opp skriver du inn team-navn og trykker Enter.\n\nHvis du ikke har lagret Personal Access Token i Jupyter s√• blir du spurt om GitHub-bruker og passord etter punkt 4. Da oppgir du du bare din GitHub-bruker og token som er autentisert mot statisticsnorway.\nFor de som ikke har mulighet til √• bruke Jupyter s√• kan man ogs√• sende inn en foresp√∏rsel til Kundeservice om √• f√• en oversikt."
  },
  {
    "objectID": "administrasjon-av-team.html#opprette-dapla-team",
    "href": "administrasjon-av-team.html#opprette-dapla-team",
    "title": "Administrasjon av team",
    "section": "",
    "text": "For √• komme i gang med √• opprette et Dapla-team trengs det en oversikt over teamets medlemmer og hvilke tilgangsgrupper medlemmene skal v√¶re med i. Det trengs ogs√• informasjon om hvilke Dapla-tjenester som er aktuelle for teamet √• ta i bruk. Derfor har det blitt opprettet en egen veileder for dette kalt Dapla Start.\n\n\n\n\n\n\nG√• til Dapla Start for starte bestilling av et nytt Dapla-team.\n\n\n\nN√•r teamet er opprettet f√•r alle medlemmene tilgang til sitt eget prosjekt i Google Cloud Platform (GCP), som er SSBs leverand√∏r av skytjenester. Videre f√•r hvert prosjekt et sett med tjenester og tilganger som knyttes til teamet. Det opprettes ogs√• datalagringsomr√•der (ofte kalt b√∏tter) som bare kan aksesseres av brukere som er med i teamets tilgangsgrupper.\nDapla-teamet vil ogs√• f√• sin egen gruppe i SSBs Active Directory slik at medlemskapet i gruppen kan administreres av Kundeservice."
  },
  {
    "objectID": "administrasjon-av-team.html#legge-til-eller-fjerne-medlemmer-i-et-team",
    "href": "administrasjon-av-team.html#legge-til-eller-fjerne-medlemmer-i-et-team",
    "title": "Administrasjon av team",
    "section": "",
    "text": "For √• legge til eller fjerne medlemmer i et team m√• du forel√∏pig opprette en Kundeservice-sak. Send en e-post til Kundeservice eller registrer en sak i portalen deres med f√∏lgende innhold:\n\nNavnet p√• Dapla-teamet du √∏nsker tilgang\nHvilken tilgang du s√∏ker om. Typisk er det enten data-admins, developers eller consumers). Les mer om hva de ulike tilgangene inneb√¶rer her.\nBeskrivelse av form√•let med tilgangen.\n\nEndringer i team m√• godkjennes av s√∏ker seksjonsleder f√∏r de blir effektuert.\nFor fjerning av tilganger kan man sende en tilsvarende henvendelse, men man trenger ikke inkludere forklaring av form√•l.\n\n\n\n\n\n\nMidlertidig l√∏sning\n\n\n\nAt endringer i team m√• gj√∏res via Kundeservice er midlertidig. Det jobbes med √• lage et eget verkt√∏y for dette."
  },
  {
    "objectID": "administrasjon-av-team.html#se-medlemmer-og-roller-i-et-team",
    "href": "administrasjon-av-team.html#se-medlemmer-og-roller-i-et-team",
    "title": "Administrasjon av team",
    "section": "",
    "text": "For √• se hvem som har de ulike tilgangsrollene i et team, s√• kan man bruke pakken dapla-team-cli fra Jupyter. Pakken er installert for alle og du kan liste ut medlemmer av team ved √• gj√∏re f√∏lgende:\n\nLogg deg inn p√• Dapla.\n√Öpne en ny terminal\nSkriv inn dpteam groups list-members og trykk Enter\nI prompten som dukker opp skriver du inn team-navn og trykker Enter.\n\nHvis du ikke har lagret Personal Access Token i Jupyter s√• blir du spurt om GitHub-bruker og passord etter punkt 4. Da oppgir du du bare din GitHub-bruker og token som er autentisert mot statisticsnorway.\nFor de som ikke har mulighet til √• bruke Jupyter s√• kan man ogs√• sende inn en foresp√∏rsel til Kundeservice om √• f√• en oversikt."
  },
  {
    "objectID": "hva-er-dapla-team.html",
    "href": "hva-er-dapla-team.html",
    "title": "Hva er Dapla-team?",
    "section": "",
    "text": "For √• kunne jobbe med skarpe/ekte data p√• Dapla m√• man opprette et et Dapla-team. Et dapla-team er en gruppe personer som jobber med ett eller flere emneomr√•der p√• SSBs dataplattform/Dapla. N√•r man oppretter et Dapla-team i SSB f√•r man f√∏lgende:\n\nLagringsomr√•de for data i Google Cloud Storage (GCS)1\nTilgangskontroll til lagringsomr√•det\nTjeneste for synkronisering av data mellom bakke og sky (Transfer Service)\n\nEt Dapla-team er ikke bare et sett med Google-tjenester knyttet til en gruppe ansatte. Det er ogs√• knyttet arbeidsprosesser og rutiner til et Dapla-team som er bestemt av SSB selv. I det f√∏lgende forklarer vi begge deler.\n\n\nN√•r man oppretter et Dapla-team s√• f√•r man tilgang til et sett med ferdig konfigurerte ressurser og tjenester i GCP. For √• f√• en god forst√•else for hvordan disse ressursene og tjenestene fungerer sammen med andre Dapla-team, er det viktig √• skj√∏nne hvordan den felles stukturen p√• GCP er bygd opp.\nAnta at noen oppretter et Dapla-team som heter Arbmark skjema. De vil da f√• tildelt et teknisk navn som er arbmark-skjema. Sistnevnte navn vil bli brukt i alle ressursene som blir opprettet for teamet.\nSSB har sin egen organisasjon p√• GCP. Derfor heter toppniv√•et ssb. Under SSB ligger det 3 Folders2: development, production og staging, som vist i Figur¬†1.\nUnder hver Folder ligger det ett eller to Google-prosjekter. Det som brukes til statistikkproduksjon ligger under production, mens det som brukes til utvikling og testing ligger under henholdsvis development og staging. I det f√∏lgende vil vi kun fokusere p√• production, som er det som brukes til statistikkproduksjon.\n\nFoldersProjectsBuckets\n\n\n\n\n\nFigur¬†1: Folders under SSB sin organisasjon p√• GCP\n\n\n\n\n\n\n\nFigur¬†2: Prosjektene som opprettes i et Dapla-team.\n\n\n\n\n\n\n\nFigur¬†3: B√∏ttene som opprettes i et Dapla-team.\n\n\n\n\n\nI Figur¬†2 ser vi hvilke prosjekter som blir opprettet for Dapla-teamet arbmark-skjema. Under production ligger prosjektene arbmark-skjema-ts og prod-arbmark-skjema. Det f√∏rste prosjektet brukes til √• synkronisere data mellom bakke og sky. prod-arbmark-skjema er det som brukes til i √• lagre data i en statistikkproduksjon.\nI Figur¬†3 ser vi hvordan lagringsb√∏ttene plasserer seg under prosjektene. I neste kapittel forklarer vi hva de ulike lagringsb√∏ttene skal brukes til.\n\n\n\n\n\n\nHva er en b√∏tte?\n\n\n\nVi kommer til √• bruke ordet b√∏tte mye i denne delen, og det er derfor ryddig √• forklare hva det er.\nEn b√∏tte er et lagringsomr√•de for data i GCP. En b√∏tte inneholder objekter av data og metadata som kan organiseres p√• en slik m√•te at det likner p√• filer organisert i mapper og undermapper. Objektene i b√∏tter er lagret ‚Äúdistribuert‚Äù, det vil si at de ligger lagret p√• ulike maskiner ute i ‚Äúskyen‚Äù, og kan n√•s via en tjeneste i GCP som heter Cloud Storage (GCS). B√∏tter er noe annet enn mapper, og har derfor f√•tt et eget ord p√• engelsk (buckets).\nHvis vi skulle sammenlignet det med v√•re systemer p√• bakken vil det ligne mye p√• en diskstasjon, for eksempel X- og S-disken.\n\n\n\n\nLagringsomr√•dene for Dapla-team best√•r av Google Cloud Storage (GCS) buckets. Disse b√∏ttene f√∏lger en navnestandard som henger sammen med SSBs datatilstander og tilgangsroller. I det f√∏lgende forklarer vi hvordan de ulike b√∏ttene er tenkt strukturert.\n\n\nUnder prosjektet prod-arbmark-skjema ligger det 3 b√∏tter:\n\nssb-prod-arbmark-skjema-data-delt\nLagring av data som skal deles med andre i SSB. Kan innholde inndata, klargjorte data, statistikkdata og utdata.\nssb-prod-arbmark-skjema-data-kilde\nLangtidslagring av kildedata (se definisjon). Kan kun inneholde kildedata.\nssb-prod-arbmark-skjema-data-produkt\nLagring av data i statistikkproduksjon. Kan innholde inndata, klargjorte data, statistikkdata og utdata.\n\nDisse lagringsomr√•dene er n√¶rt knyttet til de ulike datatilstandene som blir beskrevet senere.\n\n\n\nUnder prosjektet arbmark-skjema-ts ligger det 2 b√∏tter som kun er tilgjengelig for kildedata-ansvarlig (data-admins) i Dapla-teamet. Disse b√∏ttene er:\n\nssb-arbmark-skjema-ts-data-synk-opp\nHer ligger data som er blitt synkronisert opp fra bakke til sky.\nssb-arbmark-skjema-ts-data-synk-ned\nHer ligger data som skal synkroniseres ned fra sky til bakke.\n/ssb/cloud_sync/arbmark-skjema/\nLagringsomr√•det p√• bakken for synkronisering av data mellom bakke og sky.\n\nKun kildedata-ansvarlig har lese- og skrivetilgang til disse b√∏ttene. Det er ogs√• kildedata-ansvarlig som kan sette opp jobber med Transfer Service for √• synkronisere data mellom bakke og sky.\n\n\n\n\nVed opprettelsen av et Dapla-team s√• blir du bedt om √• plassere medlemmene i teamet i en av tre ulike tilgangsroller. Disse er:\n\ndata-admins\nHar lese- og skrivetilgang i alle lagringsomr√•dene i Dapla-teamet. Siden dette er en priveligert rolle med potensiell tilgang til sensitiv data, s√• er det kun noen f√• personer som skal ha denne rollen i et Dapla-team.\ndevelopers\nHar lese- og skrivetilgang i alle lagringsomr√•dene i Dapla-teamet, med unntak av ssb-prod-arbmark-skjema-data-kilde, og b√∏ttene i prosjektet arbmark-skjema-ts. Dvs. at alle som jobber med statistikkproduksjon tilknyttet teamets data, og som ikke er data-admin, skal ha denne rollen.\nconsumers\nMedlemmer fra andre Dapla-team som har behov for tilgang til dette teamets data. De f√•r lesetilgang til ssb-prod-arbmark-skjema-data-delt.\n\ndata-admin har i tillegg til lagringsomr√•dene p√• sky, tilgang til denne mappen p√• bakken: /ssb/cloud_sync/arbmark-skjema. Her kan de legge filer som de √∏nsker √• flytte til skyen.\nTabell¬†1 viser hvilke roller som har tilgang til hvilke b√∏tter/mapper.\n\n\nTabell¬†1: Tilgangsroller og lagringsomr√•der\n\n\n\n\n\n\n\n\n\ndata-admin\ndeveloper\nconsumer\n\n\n\n\narbmark-skjema-kilde\nX\n\n\n\n\narbmark-skjema-produkt\nX\nX\n\n\n\narbmark-skjema-delt\nX\nX\nX\n\n\narbmark-skjema-synk-opp\nX\n\n\n\n\narbmark-skjema-synk-ned\nX\n\n\n\n\n/ssb/cloud_sync/arbmark_skjema/\nX\n\n\n\n\n\n\n\n\n\nTeam Statistikktjenester jobber med en tjeneste for √• automatisere overgangen fra kildedata til inndata. Denne tjenesten vil bli tilgjengelig for alle Dapla-team.\n\n\n\n\nI tillegg til man f√•r tilgang til spesifikke GCP-tjenester ved opprettelse av et Dapla-team, s√• er det ogs√• lagt opp til noen spesifikke arbeidsprosesser rundt b√∏tter og tilgangsroller. I denne delen forklarer vi hvordan dette forholder seg GCP-tjenestene vi beskrev i forrige del.\n\n\nEt viktig konsept p√• Dapla er datatilstander. Disse er definert i definert i v√•rt interne dokument Datatilstander av Standardutvalget (2021). I dokumentet presiseres det at tilstandene kildedata, klargjorte data og statistikkdata er obligatoriske for statistikkprodusenter p√• Dapla.\nI tillegg har Direkt√∏rm√∏tet (2022) konkretisert hvordan klassifisering og tilgangskontroll skal utf√∏res p√• DAPLA. Under beskriver vi hvordan de to dokumentene p√•virker et Dapla-team.\n\n\n\nKildedata er data som er produsert av andre enn SSB. Det kan v√¶re data fra andre statlige etater, eller fra private akt√∏rer. Kildedata er ofte i form av en fil, eller en mappe med filer. Kildedata skal lagres i b√∏tten kilde i GCS.\nKildedata skal lagres i den formen den kom til SSB i kildeb√∏tta. Det vil ofte forekomme at disse dataene er sensitive og at de kan inneholde informasjon som ikke skal brukes videre i statistikkproduksjon. Derfor er det kun data-admin som skal ha tilgang til denne b√∏tten. Og det b√∏r v√¶re s√• f√• som mulig p√• teamet som har rollen data-admin, spesielt hvis det er sensitive data.\ndata-admin har ansvaret for √• s√∏rge for at kildedata behandles p√• en m√•te som gj√∏r at den tilgjengeliggj√∏res for resten av teamet. Typisk vil dette inneb√¶re3:\n\npseudonymisering\ndataminimering\nkvalitetssikring\nkonvertering til et felles format\n\nDet er ikke tenkt at data-admin skal m√•tte kj√∏re dette manuelt, men at det skal v√¶re en del av en automatisk prosess som kj√∏res hver gang en ny fil kommer inn i kildeb√∏tta. Det er kun ved mistanke om feil i datafangsten, som gir tjenestlige behov for data-admins til √• se data i klartekst, at data-admins bruker tilgangen sin til √• se p√• data i kildeb√∏tta.\n\n\n\nN√•r kildedata har blitt transformert og beveget seg over i en av de andre datatilstandene, vil det ligge i produkt-b√∏tta og v√¶re tilgjengelig for alle med developers-tilgangen.\nI produktb√∏tta skal det lagres tre typer data:\n\nInndata\nKlargjorte data\nStatistikkdata\nUtdata\n\nLes mer om det her.\n\n\n\nN√•r andre Dapla-team skal ha tilgang til data fra ditt team, m√• de s√∏ke om √• f√• tilgangsrollen consumer i ditt team. Du m√• dermed tilgjengeliggj√∏re dataene som skal deles i din delt-b√∏tte.\nMer kommer."
  },
  {
    "objectID": "hva-er-dapla-team.html#google-tjenester",
    "href": "hva-er-dapla-team.html#google-tjenester",
    "title": "Hva er Dapla-team?",
    "section": "",
    "text": "N√•r man oppretter et Dapla-team s√• f√•r man tilgang til et sett med ferdig konfigurerte ressurser og tjenester i GCP. For √• f√• en god forst√•else for hvordan disse ressursene og tjenestene fungerer sammen med andre Dapla-team, er det viktig √• skj√∏nne hvordan den felles stukturen p√• GCP er bygd opp.\nAnta at noen oppretter et Dapla-team som heter Arbmark skjema. De vil da f√• tildelt et teknisk navn som er arbmark-skjema. Sistnevnte navn vil bli brukt i alle ressursene som blir opprettet for teamet.\nSSB har sin egen organisasjon p√• GCP. Derfor heter toppniv√•et ssb. Under SSB ligger det 3 Folders2: development, production og staging, som vist i Figur¬†1.\nUnder hver Folder ligger det ett eller to Google-prosjekter. Det som brukes til statistikkproduksjon ligger under production, mens det som brukes til utvikling og testing ligger under henholdsvis development og staging. I det f√∏lgende vil vi kun fokusere p√• production, som er det som brukes til statistikkproduksjon.\n\nFoldersProjectsBuckets\n\n\n\n\n\nFigur¬†1: Folders under SSB sin organisasjon p√• GCP\n\n\n\n\n\n\n\nFigur¬†2: Prosjektene som opprettes i et Dapla-team.\n\n\n\n\n\n\n\nFigur¬†3: B√∏ttene som opprettes i et Dapla-team.\n\n\n\n\n\nI Figur¬†2 ser vi hvilke prosjekter som blir opprettet for Dapla-teamet arbmark-skjema. Under production ligger prosjektene arbmark-skjema-ts og prod-arbmark-skjema. Det f√∏rste prosjektet brukes til √• synkronisere data mellom bakke og sky. prod-arbmark-skjema er det som brukes til i √• lagre data i en statistikkproduksjon.\nI Figur¬†3 ser vi hvordan lagringsb√∏ttene plasserer seg under prosjektene. I neste kapittel forklarer vi hva de ulike lagringsb√∏ttene skal brukes til.\n\n\n\n\n\n\nHva er en b√∏tte?\n\n\n\nVi kommer til √• bruke ordet b√∏tte mye i denne delen, og det er derfor ryddig √• forklare hva det er.\nEn b√∏tte er et lagringsomr√•de for data i GCP. En b√∏tte inneholder objekter av data og metadata som kan organiseres p√• en slik m√•te at det likner p√• filer organisert i mapper og undermapper. Objektene i b√∏tter er lagret ‚Äúdistribuert‚Äù, det vil si at de ligger lagret p√• ulike maskiner ute i ‚Äúskyen‚Äù, og kan n√•s via en tjeneste i GCP som heter Cloud Storage (GCS). B√∏tter er noe annet enn mapper, og har derfor f√•tt et eget ord p√• engelsk (buckets).\nHvis vi skulle sammenlignet det med v√•re systemer p√• bakken vil det ligne mye p√• en diskstasjon, for eksempel X- og S-disken.\n\n\n\n\nLagringsomr√•dene for Dapla-team best√•r av Google Cloud Storage (GCS) buckets. Disse b√∏ttene f√∏lger en navnestandard som henger sammen med SSBs datatilstander og tilgangsroller. I det f√∏lgende forklarer vi hvordan de ulike b√∏ttene er tenkt strukturert.\n\n\nUnder prosjektet prod-arbmark-skjema ligger det 3 b√∏tter:\n\nssb-prod-arbmark-skjema-data-delt\nLagring av data som skal deles med andre i SSB. Kan innholde inndata, klargjorte data, statistikkdata og utdata.\nssb-prod-arbmark-skjema-data-kilde\nLangtidslagring av kildedata (se definisjon). Kan kun inneholde kildedata.\nssb-prod-arbmark-skjema-data-produkt\nLagring av data i statistikkproduksjon. Kan innholde inndata, klargjorte data, statistikkdata og utdata.\n\nDisse lagringsomr√•dene er n√¶rt knyttet til de ulike datatilstandene som blir beskrevet senere.\n\n\n\nUnder prosjektet arbmark-skjema-ts ligger det 2 b√∏tter som kun er tilgjengelig for kildedata-ansvarlig (data-admins) i Dapla-teamet. Disse b√∏ttene er:\n\nssb-arbmark-skjema-ts-data-synk-opp\nHer ligger data som er blitt synkronisert opp fra bakke til sky.\nssb-arbmark-skjema-ts-data-synk-ned\nHer ligger data som skal synkroniseres ned fra sky til bakke.\n/ssb/cloud_sync/arbmark-skjema/\nLagringsomr√•det p√• bakken for synkronisering av data mellom bakke og sky.\n\nKun kildedata-ansvarlig har lese- og skrivetilgang til disse b√∏ttene. Det er ogs√• kildedata-ansvarlig som kan sette opp jobber med Transfer Service for √• synkronisere data mellom bakke og sky.\n\n\n\n\nVed opprettelsen av et Dapla-team s√• blir du bedt om √• plassere medlemmene i teamet i en av tre ulike tilgangsroller. Disse er:\n\ndata-admins\nHar lese- og skrivetilgang i alle lagringsomr√•dene i Dapla-teamet. Siden dette er en priveligert rolle med potensiell tilgang til sensitiv data, s√• er det kun noen f√• personer som skal ha denne rollen i et Dapla-team.\ndevelopers\nHar lese- og skrivetilgang i alle lagringsomr√•dene i Dapla-teamet, med unntak av ssb-prod-arbmark-skjema-data-kilde, og b√∏ttene i prosjektet arbmark-skjema-ts. Dvs. at alle som jobber med statistikkproduksjon tilknyttet teamets data, og som ikke er data-admin, skal ha denne rollen.\nconsumers\nMedlemmer fra andre Dapla-team som har behov for tilgang til dette teamets data. De f√•r lesetilgang til ssb-prod-arbmark-skjema-data-delt.\n\ndata-admin har i tillegg til lagringsomr√•dene p√• sky, tilgang til denne mappen p√• bakken: /ssb/cloud_sync/arbmark-skjema. Her kan de legge filer som de √∏nsker √• flytte til skyen.\nTabell¬†1 viser hvilke roller som har tilgang til hvilke b√∏tter/mapper.\n\n\nTabell¬†1: Tilgangsroller og lagringsomr√•der\n\n\n\n\n\n\n\n\n\ndata-admin\ndeveloper\nconsumer\n\n\n\n\narbmark-skjema-kilde\nX\n\n\n\n\narbmark-skjema-produkt\nX\nX\n\n\n\narbmark-skjema-delt\nX\nX\nX\n\n\narbmark-skjema-synk-opp\nX\n\n\n\n\narbmark-skjema-synk-ned\nX\n\n\n\n\n/ssb/cloud_sync/arbmark_skjema/\nX\n\n\n\n\n\n\n\n\n\nTeam Statistikktjenester jobber med en tjeneste for √• automatisere overgangen fra kildedata til inndata. Denne tjenesten vil bli tilgjengelig for alle Dapla-team."
  },
  {
    "objectID": "hva-er-dapla-team.html#prosesser-og-arbeidsrutiner",
    "href": "hva-er-dapla-team.html#prosesser-og-arbeidsrutiner",
    "title": "Hva er Dapla-team?",
    "section": "",
    "text": "I tillegg til man f√•r tilgang til spesifikke GCP-tjenester ved opprettelse av et Dapla-team, s√• er det ogs√• lagt opp til noen spesifikke arbeidsprosesser rundt b√∏tter og tilgangsroller. I denne delen forklarer vi hvordan dette forholder seg GCP-tjenestene vi beskrev i forrige del.\n\n\nEt viktig konsept p√• Dapla er datatilstander. Disse er definert i definert i v√•rt interne dokument Datatilstander av Standardutvalget (2021). I dokumentet presiseres det at tilstandene kildedata, klargjorte data og statistikkdata er obligatoriske for statistikkprodusenter p√• Dapla.\nI tillegg har Direkt√∏rm√∏tet (2022) konkretisert hvordan klassifisering og tilgangskontroll skal utf√∏res p√• DAPLA. Under beskriver vi hvordan de to dokumentene p√•virker et Dapla-team.\n\n\n\nKildedata er data som er produsert av andre enn SSB. Det kan v√¶re data fra andre statlige etater, eller fra private akt√∏rer. Kildedata er ofte i form av en fil, eller en mappe med filer. Kildedata skal lagres i b√∏tten kilde i GCS.\nKildedata skal lagres i den formen den kom til SSB i kildeb√∏tta. Det vil ofte forekomme at disse dataene er sensitive og at de kan inneholde informasjon som ikke skal brukes videre i statistikkproduksjon. Derfor er det kun data-admin som skal ha tilgang til denne b√∏tten. Og det b√∏r v√¶re s√• f√• som mulig p√• teamet som har rollen data-admin, spesielt hvis det er sensitive data.\ndata-admin har ansvaret for √• s√∏rge for at kildedata behandles p√• en m√•te som gj√∏r at den tilgjengeliggj√∏res for resten av teamet. Typisk vil dette inneb√¶re3:\n\npseudonymisering\ndataminimering\nkvalitetssikring\nkonvertering til et felles format\n\nDet er ikke tenkt at data-admin skal m√•tte kj√∏re dette manuelt, men at det skal v√¶re en del av en automatisk prosess som kj√∏res hver gang en ny fil kommer inn i kildeb√∏tta. Det er kun ved mistanke om feil i datafangsten, som gir tjenestlige behov for data-admins til √• se data i klartekst, at data-admins bruker tilgangen sin til √• se p√• data i kildeb√∏tta.\n\n\n\nN√•r kildedata har blitt transformert og beveget seg over i en av de andre datatilstandene, vil det ligge i produkt-b√∏tta og v√¶re tilgjengelig for alle med developers-tilgangen.\nI produktb√∏tta skal det lagres tre typer data:\n\nInndata\nKlargjorte data\nStatistikkdata\nUtdata\n\nLes mer om det her.\n\n\n\nN√•r andre Dapla-team skal ha tilgang til data fra ditt team, m√• de s√∏ke om √• f√• tilgangsrollen consumer i ditt team. Du m√• dermed tilgjengeliggj√∏re dataene som skal deles i din delt-b√∏tte.\nMer kommer."
  },
  {
    "objectID": "hva-er-dapla-team.html#footnotes",
    "href": "hva-er-dapla-team.html#footnotes",
    "title": "Hva er Dapla-team?",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nLagringsomr√•det tilsvarer stammene p√• bakken.‚Ü©Ô∏é\nFolders er et element som en organisasjon kan bruke for √• organisere underenheter i GCP. I SSB er Folders brukt til √• skille mellom produksjons-, test- og utviklingsmilj√∏er. Statistikkseksjonene trenger ikke √• forholde seg noe s√¶rlig til det tekniske rundt Folders, bortsett fra at milj√∏ene for test og produksjon er satt opp noe annerledes. Hvis du √∏nsker kan du lese mer om Folders kan g√• til Google sine sider.‚Ü©Ô∏é\nHvilken behandling av dataene som kreves vil avhenge av datakildene og kan variere fra statistikk til statistikk.‚Ü©Ô∏é"
  },
  {
    "objectID": "oracle.html",
    "href": "oracle.html",
    "title": "Oracle",
    "section": "",
    "text": "Oracle"
  },
  {
    "objectID": "datatilstander.html",
    "href": "datatilstander.html",
    "title": "Datatilstander",
    "section": "",
    "text": "Datatilstander\n\nDet er naturlig at hovedfokus i SSBs kvalitetsarbeid er rettet mot statistikkene. Samtidig har b√•deforventninger og krav til SSBs evne til √• dele data √∏kt betydelig de senere √•r. Det betyr at i tillegg til √• produsere hovedproduktet ¬´statistikk¬ª, s√• vil mange statistikkteam ha √∏kt fokus p√• √• produseregjenbrukbare datasett med h√∏y kvalitet. En viktig forutsetning for gjenbruk er at de som vil brukedataene, kan vite hvilke endringer dataene har gjennomg√•tt. Det m√• ogs√• v√¶re mulig for andre √• finne og forst√• dataene. Kvalitetssikret bruk av data i SSB og gjenbruk i og utenfor SSB fordrer godemetadata. Definisjoner av datatilstander og andre statistikkbegreper m√• derfor i st√∏rst mulig gradv√¶re avstemt med internasjonale statistiske rammeverk og definisjoner.\nBegrepet ¬´etterpr√∏vbarhet¬ª brukes flere steder i notatet, og det legges til grunn at vi b√∏r ha som et ideal √• produsere statistikk p√• en slik m√•te at ettertiden eller en uavhengig instans med tilgang til dataene og v√•r dokumentasjon vil komme til samme statistiske resultater som oss selv.\nTilstandene som beskrives er kildedata, inndata, klargjorte data, statistikk og utdata. De tre f√∏rste er i hovedsak mikrodata som gir informasjon om enkeltenheter, mens statistikk og utdata i hovedsak er aggregerte data.\n\n(Standardutvalget 2021, 5)\n\n\n\n\n\nReferanser\n\nStandardutvalget. 2021. ‚ÄúDatatilstander i SSB.‚Äù Statistisk sentralbyr√•. https://ssbno.sharepoint.com/sites/Internedokumenter/Delte%20dokumenter/Forms/AllItems.aspx?id=%2Fsites%2FInternedokumenter%2FDelte%20dokumenter%2FInterne%20dokumenter%202021%2F2021%2D17%20Datatilstander%20i%20SSB%20%2Epdf&parent=%2Fsites%2FInternedokumenter%2FDelte%20dokumenter%2FInterne%20dokumenter%202021."
  },
  {
    "objectID": "sas.html",
    "href": "sas.html",
    "title": "SAS",
    "section": "",
    "text": "SAS"
  },
  {
    "objectID": "versjonering-av-datasett.html",
    "href": "versjonering-av-datasett.html",
    "title": "Versjonering av datasett",
    "section": "",
    "text": "Versjonering av data(sett) er viktig for √• dekke kravet om uforanderlighet og etterpr√∏vbarhet. Hovedpoenget med versjonering er at data-konsumenter (menneske eller maskin) skal ha kontroll p√• endringer, dvs. tilgang b√•de til en stabil versjon (uforanderlighet), men ogs√• tilgang til eventuelle nye data- versjoner som oppst√•r. Se mer om dette i SSBs produksjonsarkitektur og prinsippet om ‚Äúuforandelighet av data‚Äù\n\n\n\n\n\n\nNote\n\n\n\nDet finnes i dag ingen internasjonale standarder eller spesifikasjoner for hvordan endringer av data skal versjoneres. Dette p√•pekes ogs√• av w3.org (https://www.w3.org/TR/dwbp/#dataVersioning) :\nDatasets published on the Web may change over time. Some datasets are updated on a scheduled basis, and other datasets are changed as improvements in collecting the data make updates worthwhile. In order to deal with these changes, new versions of a dataset may be created. Unfortunately, there is no consensus about when changes to a dataset should cause it to be considered a different dataset altogether rather than a new version. In the following, we present some scenarios where most publishers would agree that the revision should be considered a new version of the existing dataset.\n\nScenario 1: a new bus stop is created and it should be added to the dataset;\nScenario 2: an existing bus stop is removed and it should be deleted from the dataset;\nScenario 3: an error was identified in one of the existing bus stops stored in the dataset and this error must be corrected.\n\nAustralian National Data Service (ANDS) beskriver ogs√• i sitt dokument https://www.ands.org.au/working-with-data/data-management/data-versioning behovet for versjonering, men ogs√• utfordringene med √• implementere data- versjonering i praksis.\nErfaringer fra versjonering i microdata.no viser at data- versjoneringen i SSB b√∏r baseres p√• prinsippet om ‚Äúbreaking changes‚Äù ( major changes ) fra Semantic Versioning (SemVer), dvs. at alle endringer som gj√∏r at vi ikke kan gjenskape samme resultat vil medf√∏re at det opprettes en ny versjon av datasettet (en ny versjon i tillegg til gammel/forrige versjon av datasettet).\n\n\n\n\nF√∏lgende hendelser skaper en ny versjon av et datasett:\n\nReberegninger av data med nye metoder.\nKorrigeringer av verdier for eksisterende observasjoner/enheter i datasettet.\n\nSelv manuell endring av bare √©n data-verdi (celle) i et stort datasett skaper en ny versjon!\n\nLagt til nye og/eller fjernet observasjoner/enheter i datasettet.\nOmkodinger, dvs. oppdatert/erstattet kodeverk.\nLagt til nye variabler. Dette skaper en ny versjon i og med at dette kan p√•virke prosesser (programkode).\n\nHvis det gj√∏res vesentlige endringer (mange nye variabler) s√• b√∏r det vurderes om dette er et annet (et helt nytt) datasett, ikke en ny versjon av et eksisterende datasett.\n\nFjernet variabler.\n\nVed fjerning av variabler fra et datasett b√∏r det vurderes om dette egentlig er et annet (et helt nytt) datasett, ikke en ny versjon av et eksisterende datasett!\n\nAndre strukturendringer, f.eks. bytte av datatyper eller formater.\n\n\n\n\n\nData som er under utarbeiding skal ikke deles/publiseres, og det er derfor ikke behov for √• versjonere slike data/datasett. Disse m√• betraktes som tempor√¶re og ‚Äúversjonsl√∏se‚Äù. Bearbeiding av data b√∏r derfor foreg√• i egne tempor√¶re dataomr√•der1. Det er f√∏rst p√• det tidspunktet et datasett er ferdig bearbeidet og klart for deling/publisering at det skal versjoneres og dokumenteres.\n1 Med tempor√¶re dataomr√•der menes f.eks. egne mapper med tempor√¶re datafiler i b√∏tter (noe tilsvarende ‚Äúwk-katalogene‚Äù i SSBs eksisterende stammekataloger p√• Linux). For en del statistikkomr√•der vil databearbeidingen ogs√• foreg√• i tempor√¶re datasett i datatjenester som Google BigQuery og CloudSQL.\n\n\n\nVed f√∏rste gangs deling/publisering av et ferdig bearbeidet datasett oppst√•r ‚Äúversjon 1‚Äù. Dette er datasett som m√• bevares uforandret for ettertiden for √• dekke kravet til etterpr√∏vbarhet av SSBs statistikk. I tillegg til selve versjonsnummeret er det viktig √• dokumentere versjonstidspunktet (metadata om tidspunktet versjonen ble frigitt for bruk) samt en beskrivelse av hvorfor det er utarbeidet en ny versjon. Dette er informasjon data-konsumentene trenger for √• kunne reprodusere statistikk med gamle versjoner av data.\nVersjonsnummer skal legges p√• som et eget element i filnavnet. For versjon 1 vil dette v√¶re ‚Äú_v1‚Äù, eksempelvis ‚Äúframskrevne-befolkningsendringer_p2019_p2050_v1.parquet‚Äù\nEksempel p√• versjon 1 av et datasett i en mappe-struktur:\nssb-prod-team-personstatistikk-data-produkt/  \n‚îî‚îÄ‚îÄ befolkningsframskrivinger/  \n    ‚îî‚îÄ‚îÄ klargjorte_data/  \n        ‚îî‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v1.parquet\n\n\n\nFor hver versjon som oppst√•r av datasettet opprettes det en ny fysisk fil hvor versjonsnummeret √∏kes med 1. Alle gamle versjoner av et datasett skal ogs√• eksistere i mappen.\n\n\n\n\n\n\nPermanente data skal ikke endres eller slettes\n\n\n\nTidligere delte/publiserte data skal ikke slettes eller overskrives!\nDet m√• derfor lagres fysiske filer for hver versjon av datasettet. Dette er viktig for at SSB skal oppfylle krav om etterpr√∏vbarhet av statistikkene.\n\n\nEksempel p√• versjon 1, 2 og 3 av et datasett i en mappe-struktur:\nssb-prod-team-personstatistikk-data-produkt/  \n‚îî‚îÄ‚îÄ befolkningsframskrivinger/  \n    ‚îî‚îÄ‚îÄ klargjorte_data/  \n        ‚îú‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        ‚îú‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v2.parquet  \n        ‚îî‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v3.parquet\n\n\n\n\n\n\nEksempel p√• versjonsinformasjon og bearbeidingshistorikk for et datasett:\n\n\n\nVersjonsinformasjon:\nDatasett: framskrevne-befolkningsendringer_p2019_p2050\nVersjon: 1\nVersjonstidspunkt: 2019-01-01T10:00:00\nVersjonsbeskrivelse: F√∏rste deling/publisering\nFilnavn: framskrevne-befolkningsendringer_p2019_p2050_v1.parquet\nVersjon: 2\nVersjonstidspunkt: 2020-02-15T08:00:00\nVersjonsbeskrivelse: Reberegning med nytt datagrunnlag\nFilnavn: framskrevne-befolkningsendringer_p2019_p2050_v2.parquet\nVersjon: 3\nVersjonstidspunkt: 2021-05-31T10:00:00\nVersjonsbeskrivelse: Revisjon og reberegning med nye framskrivingsmetoder\nFilnavn: framskrevne-befolkningsendringer_p2019_p2050_v3.parquet\n\n\n\n\n\nHvis det er behov for √• dele data som er som er i ‚Äúfart‚Äù, dvs. data som fortsatt er under innsamling eller p√•g√•ende klargj√∏ring, gj√∏res dette ved √• bruke versjonsnummer 0 i filnavnet. Versjonsnummer 0 skal kun brukes midlertidig fram til datasettet oppn√•r stabil tilstand (ved stabil tilstand byttes versjonsnummer for datasettet til 1 eller h√∏yere).\nEksempel p√• deling av ‚Äúversjon 0‚Äù av et datasett:\nssb-prod-team-personstatistikk-data-produkt/  \n‚îî‚îÄ‚îÄ befolkningsframskrivinger/  \n    ‚îî‚îÄ‚îÄ klargjorte_data/  \n        ‚îî‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v0.parquet\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nw3.org anbefaler at gjeldende versjon av et datasett alltid skal v√¶re tilgjengelig ogs√• uten versjonsnummer. Dette vil ogs√• v√¶re sv√¶rt nyttig for statistikkproduksjonen i SSB, i og med at det i de aller fleste tilfeller er siste versjon (den mest oppdaterte) av de klargjorte datasettene som skal benyttes. Den samme programkoden (Python/R) kan da kj√∏res ved hver statistikkproduksjon uten at filnavnet m√• endres i programkoden. Det er kun i de f√• tilfellene hvor gamle versjoner skal benyttes at programkoden m√• endres, f.eks. ved ‚Äúreprodusering‚Äù av gamle tall.\nEksempel fra w3.org:\n\n/data_example/transport/dataset/bus/stops is the ‚Äúgeneric URI‚Äù at which the current version of a dataset is always available\n/data_example/transport/dataset/bus/stops_v2 is the versioned URI for the current dataset\n/data_example/transport/dataset/bus/stops_v1 is the versioned URI of the prior version of the dataset\n\n\n\n\n\nEksempel p√• ‚Äúversjon 1‚Äù (hvor versjonen ogs√• er tilgjengelig uten versjonsnummer i datasettnavnet):\nssb-prod-personstatistikk-data-produkt/  \n‚îî‚îÄ‚îÄ befolkningsframskrivinger/  \n    ‚îî‚îÄ‚îÄ klargjorte-data/  \n        ‚îú‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        ‚îî‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050.parquet\n\n\n\n\n\n\nNote\n\n\n\nw3.org anbefaling er at siste versjon ogs√• er tilgjengelig uten versjonselement i navnet. I eksempelet over er derfor filen framskrevne-befolkningsendringer_p2019_p2050.parquet helt identisk med framskrevne-befolkningsendringer_p2019_p2050_v1.parquet (versjon 1).\n\n\nEksempel med versjon 1, 2, 3 og 4 (hvor versjon 4 ogs√• er tilgjengelig uten versjonsnummer i datasettnavnet)\nssb-prod-personstatistikk-data-produkt/  \n‚îî‚îÄ‚îÄ befolkningsframskrivinger/  \n    ‚îî‚îÄ‚îÄ klargjorte-data/  \n        ‚îú‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        ‚îú‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v2.parquet  \n        ‚îú‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v3.parquet  \n        ‚îú‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v4.parquet  \n        ‚îî‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050.parquet\n\n\n\n\n\n\nNote\n\n\n\nw3.org anbefaling er at siste versjon ogs√• er tilgjengelig uten versjonselement i navnet. I eksempelet over er derfor filen framskrevne-befolkningsendringer_p2019_p2050.parquet helt identisk med framskrevne-befolkningsendringer_p2019_p2050_v4.parquet (versjon 4).\n\n\n\n\n\nFor √• unng√• dobbeltlagring av data i form av at siste versjon av et datasett skal lagres som en fysisk datafil b√•de med og uten versjonsnummer (som vist i kapittelet over), anbefales det at det utvikles felles SSB-programbibliotek i Python og R for utlede denne informasjonen. Da vil da kun v√¶re n√∏dvendig √• lagre filer med fullt versjonsnummer, men statistikkprodusentene kan bruke en funksjon for √• finne siste versjon - eksempelvis gi meg siste versjon av datasettet ‚Äúframskrevne-befolkningsendringer_p2019_p2050‚Äú.\nNedenfor vises en enkel Python-funksjon for hvordan dette kan fungere i praksis. Denne funksjonaliteten er imidlertid ikke tilgjengelig i Dapla i skrivende stund, s√• dette er bare et forslag til l√∏sning.\n# Eksempel p√• en felles SSB-funksjon for √• hente riktig fysisk filnavn til siste versjon\n# av et datasettt i en mappe (katalog) i en b√∏tte hvor det eksisterer flere versjoner\n# (flere filversjoner) av datasettet.\n# NB! Dette er kun ment som et eksempel (konsept), og er ikke en produksjonsklar l√∏sning!\n\nimport operator\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\ndef get_current_dataset_version(path:str,\n                                dataset_name_without_version:str,\n                                file_type:str = \"parquet\"\n                               ) -&gt; str:    \n    gcs_dataset_files = fs.glob(path=path + dataset_name_without_version + \"*.\" + file_type)\n\n    file_list = []\n    for file in gcs_dataset_files:\n        file_version = file.split(\"_v\")[-1].split(\".\")[0]\n        if file_version.isnumeric():\n            file_list.append({\"file_path\": str(file), \"file_version\": int(file_version) })\n\n    file_list.sort(key=operator.itemgetter('file_version'))\n    if len(file_list) &gt; 0:\n        return file_list[-1][\"file_path\"]\n    else:\n        return None\n\n\n### Eksempel p√• bruk ###\n# Hent sti og filnavn til siste versjon av datasettet \"framskrevne-befolkningsendringer_p2019_p2050\"\n# i mappen gs://ssb-prod-befolkning-data-produkt/befolkningsendringer/klargjorte-data/\n# som inneholder 4 versjoner (\"framskrevne-befolkningsendringer_p2019_p2050_v1\" \n# til \"framskrevne-befolkningsendringer_p2019_p2050_v4\")\n\nget_current_dataset_version(\n    path=\"gs://ssb-prod-befolkning-data-produkt/befolkningsendringer/klargjorte-data/\",\n    dataset_name_without_version=\"framskrevne-befolkningsendringer_p2019_p2050\",\n    file_type=\"parquet\")\n\n\n### Eksempel p√• resultat (sti og filnavn til versjon 4 av datasettet) ###\ngs://ssb-prod-befolkning-data-produkt/befolkningsendringer/klargjorte-data/framskrevne-befolkningsendringer_p2019_p2050_v4"
  },
  {
    "objectID": "versjonering-av-datasett.html#regler-for-hva-som-skaper-en-ny-versjon-av-et-datasett-breaking-changes",
    "href": "versjonering-av-datasett.html#regler-for-hva-som-skaper-en-ny-versjon-av-et-datasett-breaking-changes",
    "title": "Versjonering av datasett",
    "section": "",
    "text": "F√∏lgende hendelser skaper en ny versjon av et datasett:\n\nReberegninger av data med nye metoder.\nKorrigeringer av verdier for eksisterende observasjoner/enheter i datasettet.\n\nSelv manuell endring av bare √©n data-verdi (celle) i et stort datasett skaper en ny versjon!\n\nLagt til nye og/eller fjernet observasjoner/enheter i datasettet.\nOmkodinger, dvs. oppdatert/erstattet kodeverk.\nLagt til nye variabler. Dette skaper en ny versjon i og med at dette kan p√•virke prosesser (programkode).\n\nHvis det gj√∏res vesentlige endringer (mange nye variabler) s√• b√∏r det vurderes om dette er et annet (et helt nytt) datasett, ikke en ny versjon av et eksisterende datasett.\n\nFjernet variabler.\n\nVed fjerning av variabler fra et datasett b√∏r det vurderes om dette egentlig er et annet (et helt nytt) datasett, ikke en ny versjon av et eksisterende datasett!\n\nAndre strukturendringer, f.eks. bytte av datatyper eller formater.\n\n\n\n\n\nData som er under utarbeiding skal ikke deles/publiseres, og det er derfor ikke behov for √• versjonere slike data/datasett. Disse m√• betraktes som tempor√¶re og ‚Äúversjonsl√∏se‚Äù. Bearbeiding av data b√∏r derfor foreg√• i egne tempor√¶re dataomr√•der1. Det er f√∏rst p√• det tidspunktet et datasett er ferdig bearbeidet og klart for deling/publisering at det skal versjoneres og dokumenteres.\n1 Med tempor√¶re dataomr√•der menes f.eks. egne mapper med tempor√¶re datafiler i b√∏tter (noe tilsvarende ‚Äúwk-katalogene‚Äù i SSBs eksisterende stammekataloger p√• Linux). For en del statistikkomr√•der vil databearbeidingen ogs√• foreg√• i tempor√¶re datasett i datatjenester som Google BigQuery og CloudSQL.\n\n\n\nVed f√∏rste gangs deling/publisering av et ferdig bearbeidet datasett oppst√•r ‚Äúversjon 1‚Äù. Dette er datasett som m√• bevares uforandret for ettertiden for √• dekke kravet til etterpr√∏vbarhet av SSBs statistikk. I tillegg til selve versjonsnummeret er det viktig √• dokumentere versjonstidspunktet (metadata om tidspunktet versjonen ble frigitt for bruk) samt en beskrivelse av hvorfor det er utarbeidet en ny versjon. Dette er informasjon data-konsumentene trenger for √• kunne reprodusere statistikk med gamle versjoner av data.\nVersjonsnummer skal legges p√• som et eget element i filnavnet. For versjon 1 vil dette v√¶re ‚Äú_v1‚Äù, eksempelvis ‚Äúframskrevne-befolkningsendringer_p2019_p2050_v1.parquet‚Äù\nEksempel p√• versjon 1 av et datasett i en mappe-struktur:\nssb-prod-team-personstatistikk-data-produkt/  \n‚îî‚îÄ‚îÄ befolkningsframskrivinger/  \n    ‚îî‚îÄ‚îÄ klargjorte_data/  \n        ‚îî‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v1.parquet\n\n\n\nFor hver versjon som oppst√•r av datasettet opprettes det en ny fysisk fil hvor versjonsnummeret √∏kes med 1. Alle gamle versjoner av et datasett skal ogs√• eksistere i mappen.\n\n\n\n\n\n\nPermanente data skal ikke endres eller slettes\n\n\n\nTidligere delte/publiserte data skal ikke slettes eller overskrives!\nDet m√• derfor lagres fysiske filer for hver versjon av datasettet. Dette er viktig for at SSB skal oppfylle krav om etterpr√∏vbarhet av statistikkene.\n\n\nEksempel p√• versjon 1, 2 og 3 av et datasett i en mappe-struktur:\nssb-prod-team-personstatistikk-data-produkt/  \n‚îî‚îÄ‚îÄ befolkningsframskrivinger/  \n    ‚îî‚îÄ‚îÄ klargjorte_data/  \n        ‚îú‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        ‚îú‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v2.parquet  \n        ‚îî‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v3.parquet\n\n\n\n\n\n\nEksempel p√• versjonsinformasjon og bearbeidingshistorikk for et datasett:\n\n\n\nVersjonsinformasjon:\nDatasett: framskrevne-befolkningsendringer_p2019_p2050\nVersjon: 1\nVersjonstidspunkt: 2019-01-01T10:00:00\nVersjonsbeskrivelse: F√∏rste deling/publisering\nFilnavn: framskrevne-befolkningsendringer_p2019_p2050_v1.parquet\nVersjon: 2\nVersjonstidspunkt: 2020-02-15T08:00:00\nVersjonsbeskrivelse: Reberegning med nytt datagrunnlag\nFilnavn: framskrevne-befolkningsendringer_p2019_p2050_v2.parquet\nVersjon: 3\nVersjonstidspunkt: 2021-05-31T10:00:00\nVersjonsbeskrivelse: Revisjon og reberegning med nye framskrivingsmetoder\nFilnavn: framskrevne-befolkningsendringer_p2019_p2050_v3.parquet\n\n\n\n\n\nHvis det er behov for √• dele data som er som er i ‚Äúfart‚Äù, dvs. data som fortsatt er under innsamling eller p√•g√•ende klargj√∏ring, gj√∏res dette ved √• bruke versjonsnummer 0 i filnavnet. Versjonsnummer 0 skal kun brukes midlertidig fram til datasettet oppn√•r stabil tilstand (ved stabil tilstand byttes versjonsnummer for datasettet til 1 eller h√∏yere).\nEksempel p√• deling av ‚Äúversjon 0‚Äù av et datasett:\nssb-prod-team-personstatistikk-data-produkt/  \n‚îî‚îÄ‚îÄ befolkningsframskrivinger/  \n    ‚îî‚îÄ‚îÄ klargjorte_data/  \n        ‚îî‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v0.parquet"
  },
  {
    "objectID": "versjonering-av-datasett.html#funksjonalitet-for-√•-hente-siste-gjeldende-versjon-av-et-datasett",
    "href": "versjonering-av-datasett.html#funksjonalitet-for-√•-hente-siste-gjeldende-versjon-av-et-datasett",
    "title": "Versjonering av datasett",
    "section": "",
    "text": "Note\n\n\n\nw3.org anbefaler at gjeldende versjon av et datasett alltid skal v√¶re tilgjengelig ogs√• uten versjonsnummer. Dette vil ogs√• v√¶re sv√¶rt nyttig for statistikkproduksjonen i SSB, i og med at det i de aller fleste tilfeller er siste versjon (den mest oppdaterte) av de klargjorte datasettene som skal benyttes. Den samme programkoden (Python/R) kan da kj√∏res ved hver statistikkproduksjon uten at filnavnet m√• endres i programkoden. Det er kun i de f√• tilfellene hvor gamle versjoner skal benyttes at programkoden m√• endres, f.eks. ved ‚Äúreprodusering‚Äù av gamle tall.\nEksempel fra w3.org:\n\n/data_example/transport/dataset/bus/stops is the ‚Äúgeneric URI‚Äù at which the current version of a dataset is always available\n/data_example/transport/dataset/bus/stops_v2 is the versioned URI for the current dataset\n/data_example/transport/dataset/bus/stops_v1 is the versioned URI of the prior version of the dataset\n\n\n\n\n\nEksempel p√• ‚Äúversjon 1‚Äù (hvor versjonen ogs√• er tilgjengelig uten versjonsnummer i datasettnavnet):\nssb-prod-personstatistikk-data-produkt/  \n‚îî‚îÄ‚îÄ befolkningsframskrivinger/  \n    ‚îî‚îÄ‚îÄ klargjorte-data/  \n        ‚îú‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        ‚îî‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050.parquet\n\n\n\n\n\n\nNote\n\n\n\nw3.org anbefaling er at siste versjon ogs√• er tilgjengelig uten versjonselement i navnet. I eksempelet over er derfor filen framskrevne-befolkningsendringer_p2019_p2050.parquet helt identisk med framskrevne-befolkningsendringer_p2019_p2050_v1.parquet (versjon 1).\n\n\nEksempel med versjon 1, 2, 3 og 4 (hvor versjon 4 ogs√• er tilgjengelig uten versjonsnummer i datasettnavnet)\nssb-prod-personstatistikk-data-produkt/  \n‚îî‚îÄ‚îÄ befolkningsframskrivinger/  \n    ‚îî‚îÄ‚îÄ klargjorte-data/  \n        ‚îú‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v1.parquet  \n        ‚îú‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v2.parquet  \n        ‚îú‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v3.parquet  \n        ‚îú‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050_v4.parquet  \n        ‚îî‚îÄ‚îÄ framskrevne-befolkningsendringer_p2019_p2050.parquet\n\n\n\n\n\n\nNote\n\n\n\nw3.org anbefaling er at siste versjon ogs√• er tilgjengelig uten versjonselement i navnet. I eksempelet over er derfor filen framskrevne-befolkningsendringer_p2019_p2050.parquet helt identisk med framskrevne-befolkningsendringer_p2019_p2050_v4.parquet (versjon 4).\n\n\n\n\n\nFor √• unng√• dobbeltlagring av data i form av at siste versjon av et datasett skal lagres som en fysisk datafil b√•de med og uten versjonsnummer (som vist i kapittelet over), anbefales det at det utvikles felles SSB-programbibliotek i Python og R for utlede denne informasjonen. Da vil da kun v√¶re n√∏dvendig √• lagre filer med fullt versjonsnummer, men statistikkprodusentene kan bruke en funksjon for √• finne siste versjon - eksempelvis gi meg siste versjon av datasettet ‚Äúframskrevne-befolkningsendringer_p2019_p2050‚Äú.\nNedenfor vises en enkel Python-funksjon for hvordan dette kan fungere i praksis. Denne funksjonaliteten er imidlertid ikke tilgjengelig i Dapla i skrivende stund, s√• dette er bare et forslag til l√∏sning.\n# Eksempel p√• en felles SSB-funksjon for √• hente riktig fysisk filnavn til siste versjon\n# av et datasettt i en mappe (katalog) i en b√∏tte hvor det eksisterer flere versjoner\n# (flere filversjoner) av datasettet.\n# NB! Dette er kun ment som et eksempel (konsept), og er ikke en produksjonsklar l√∏sning!\n\nimport operator\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\ndef get_current_dataset_version(path:str,\n                                dataset_name_without_version:str,\n                                file_type:str = \"parquet\"\n                               ) -&gt; str:    \n    gcs_dataset_files = fs.glob(path=path + dataset_name_without_version + \"*.\" + file_type)\n\n    file_list = []\n    for file in gcs_dataset_files:\n        file_version = file.split(\"_v\")[-1].split(\".\")[0]\n        if file_version.isnumeric():\n            file_list.append({\"file_path\": str(file), \"file_version\": int(file_version) })\n\n    file_list.sort(key=operator.itemgetter('file_version'))\n    if len(file_list) &gt; 0:\n        return file_list[-1][\"file_path\"]\n    else:\n        return None\n\n\n### Eksempel p√• bruk ###\n# Hent sti og filnavn til siste versjon av datasettet \"framskrevne-befolkningsendringer_p2019_p2050\"\n# i mappen gs://ssb-prod-befolkning-data-produkt/befolkningsendringer/klargjorte-data/\n# som inneholder 4 versjoner (\"framskrevne-befolkningsendringer_p2019_p2050_v1\" \n# til \"framskrevne-befolkningsendringer_p2019_p2050_v4\")\n\nget_current_dataset_version(\n    path=\"gs://ssb-prod-befolkning-data-produkt/befolkningsendringer/klargjorte-data/\",\n    dataset_name_without_version=\"framskrevne-befolkningsendringer_p2019_p2050\",\n    file_type=\"parquet\")\n\n\n### Eksempel p√• resultat (sti og filnavn til versjon 4 av datasettet) ###\ngs://ssb-prod-befolkning-data-produkt/befolkningsendringer/klargjorte-data/framskrevne-befolkningsendringer_p2019_p2050_v4"
  },
  {
    "objectID": "hvorfor-dapla.html",
    "href": "hvorfor-dapla.html",
    "title": "Hvorfor Dapla?",
    "section": "",
    "text": "Hvorfor Dapla?\nSom dataplattform skal Dapla stimulerere til √∏kt kvalitet p√• statistikk og forskning, samtidig som den gj√∏r organisasjonen mer tilpasningsdyktig i m√∏te med fremtiden.\n\n\nDen nye skybaserte dataplattformen (Dapla) skal bli viktig for √• effektivisere arbeids-og produksjonsprosesser, den skal sikre effektiv lagring og gjenfinning av data og metadata, og st√∏tte opp under deling av data p√• tvers av statistikkomr√•der.\n\nKilde: Langtidsplan for SSB (2022-2024)\n\n\n\nM√•let med Dapla er √• tilby tjenester og verkt√∏y som lar statistikkprodusenter og forskere produsere resultater p√• en sikker og effektiv m√•te."
  },
  {
    "objectID": "datadoc.html",
    "href": "datadoc.html",
    "title": "DataDoc",
    "section": "",
    "text": "For √• kunne gjenfinne data i SSB er man helt avhengig av at det finnes et enhetlig system for metadata knyttet til dataene. DataDoc er SSBs system for √• dokumentere datasett p√• den nye dataplattformen Dapla.\nDet er bygget et grensesnitt i Python for √• gj√∏re det enklest mulig √• dokumentere et datasett. Forel√∏pig st√∏tter l√∏sningen f√∏lgende filformater:\n\nparquet\nsas7bdat\n\nUnder finner du beskrivelse av hvordan du kan begynne √• bruke l√∏sningen til √• dokumentere datasett.\n\n\n\n\n\n\nWarning\n\n\n\nVi √∏nsker at du skal teste DataDoc-applikasjonen. Den viktigste funksjonaliteten skal v√¶re tilgjengelig, og det er fullt mulig √• benytte DataDoc i SSBs Jupyter-milj√∏er. Det er imidlertid viktig √• v√¶re klar over at applikasjonen fortsatt er i en utviklings- og testfase (beta-l√∏sning) og kan inneholde feil og mangler.\nHar du sp√∏rsm√•l, eventuelt vil rapporterer om feil og mangler, s√• setter vi pris p√• om du gj√∏r dette i Yammer-gruppa Dapla.\n\n\n\n\nF√∏r du tar i bruk DataDoc-applikasjonen er det viktig √• forst√• hvilken informasjon som skal til for √• dokumentere et datasett. I DataDoc-applikasjonen skal du fylle ut flere felter om b√•de datasettet og variablene som inng√•r i datasettet, eksempelvis\n\nkortnavn\nnavn\ndatatilstand\npopulasjonsbeskrivelse\n++\n\nDet er utarbeidet en detaljert beskrivelse hva hvert felt betyr, og hvordan de skal fylles ut b√•de for datasett og variabler: - DataDoc - hvordan dokumentere et datasett - DataDoc - hvordan dokumentere variablene (variabelforekomstene) som inng√•r i datasettet\nDataDoc skal v√¶re installert i alle Jupyter-milj√∏ene i SSB, s√• du trenger ikke installere pakken selv.\n\n\n\n\n\n\nViktig informasjon\n\n\n\nDataDoc kan forel√∏pig ikke kj√∏res i Jupyter notebook med virtuelle milj√∏er (f.eks. et ssb-project), men m√• startes i den vanlige kernelen i en notebook.\n\n\n\n\n\nLa oss lage et test-datasett slik at vi kan leke oss litt med DataDoc:\nimport pandas as pd\nfrom datadoc import main\n  \n# Create fake data\ndata = {'id': ['9999999999', '8888888888', '7777777777', '6666666666'],\n        'fylke': [\"01\", \"02\", \"03\", \"03\"],\n        'inntekt': [500000, 250000, 400000, 440000],\n        'rente': [3.2, 4.1, 3.3, 3.4]}\n  \n# Creates a Pandas dataframe\ndf = pd.DataFrame(data)\n\n# Write a Parquet-file to current folder\ndf.to_parquet(\"./test.parquet\")\nN√• har vi en fil som heter test.parquet i mappen vi st√•r. Da kan vi √•pne DataDoc-grensesnittet for √• legge inn metadataene:\nmain(\"./test.parquet\")\nFigur¬†1 viser hvordan DataDoc-grensesnittet ser ut.\n\n\n\nFigur¬†1: Gif som viser hvordan DataDoc-grensesnittet ser ut.\n\n\n\n\n\nN√•r du trykker p√• Lagre-knappen i DataDoc s√• skrives alle metadata til en fil i samme mappe (katalog) som datafilen. Dette er en JSON-fil med nesten samme navn som datafilen. Navnekonvensjonen for metadatafilen er\n&lt;navn p√• datafil uten endelse&gt;__DOC.json\nEksempelvis hvis datafilen har navnet skattedata_p2022_v1.parquet, s√• vil DataDoc lagre metadata i filen skattedata_p2022_v1__DOC.json.\nFordelen med √• benytte en JSON-fil til √• lagre metadata er at denne filen kan kopieres og flyttes like enkelt som selve datafilen. JSON-filer er strukturerte tekstfiler som kan leses av b√•de maskiner (Python/R) og av mennesker (√•pnes i en tekst-editor).\nSe et eksempel p√• JSON metadata-fil lagret av DataDoc.\n\n\n\n\n\n\nInformasjon\n\n\n\nI Dapla skal det bygges en felles datakatalog for SSB. Tanken er at alle metadata, eksempelvis datasett-dokumentasjon fra DataDoc (JSON-filene), skal inng√• i SSBs datakatalog. Datakatalogen gj√∏r det mulig √• finne (s√∏ke etter), forst√•r og gjenbruke data b√•de internt og ekstern."
  },
  {
    "objectID": "datadoc.html#hvordan-dokumentere-datasett-og-variabler-med-datadoc",
    "href": "datadoc.html#hvordan-dokumentere-datasett-og-variabler-med-datadoc",
    "title": "DataDoc",
    "section": "",
    "text": "F√∏r du tar i bruk DataDoc-applikasjonen er det viktig √• forst√• hvilken informasjon som skal til for √• dokumentere et datasett. I DataDoc-applikasjonen skal du fylle ut flere felter om b√•de datasettet og variablene som inng√•r i datasettet, eksempelvis\n\nkortnavn\nnavn\ndatatilstand\npopulasjonsbeskrivelse\n++\n\nDet er utarbeidet en detaljert beskrivelse hva hvert felt betyr, og hvordan de skal fylles ut b√•de for datasett og variabler: - DataDoc - hvordan dokumentere et datasett - DataDoc - hvordan dokumentere variablene (variabelforekomstene) som inng√•r i datasettet\nDataDoc skal v√¶re installert i alle Jupyter-milj√∏ene i SSB, s√• du trenger ikke installere pakken selv.\n\n\n\n\n\n\nViktig informasjon\n\n\n\nDataDoc kan forel√∏pig ikke kj√∏res i Jupyter notebook med virtuelle milj√∏er (f.eks. et ssb-project), men m√• startes i den vanlige kernelen i en notebook."
  },
  {
    "objectID": "datadoc.html#pr√∏ve-datadoc",
    "href": "datadoc.html#pr√∏ve-datadoc",
    "title": "DataDoc",
    "section": "",
    "text": "La oss lage et test-datasett slik at vi kan leke oss litt med DataDoc:\nimport pandas as pd\nfrom datadoc import main\n  \n# Create fake data\ndata = {'id': ['9999999999', '8888888888', '7777777777', '6666666666'],\n        'fylke': [\"01\", \"02\", \"03\", \"03\"],\n        'inntekt': [500000, 250000, 400000, 440000],\n        'rente': [3.2, 4.1, 3.3, 3.4]}\n  \n# Creates a Pandas dataframe\ndf = pd.DataFrame(data)\n\n# Write a Parquet-file to current folder\ndf.to_parquet(\"./test.parquet\")\nN√• har vi en fil som heter test.parquet i mappen vi st√•r. Da kan vi √•pne DataDoc-grensesnittet for √• legge inn metadataene:\nmain(\"./test.parquet\")\nFigur¬†1 viser hvordan DataDoc-grensesnittet ser ut.\n\n\n\nFigur¬†1: Gif som viser hvordan DataDoc-grensesnittet ser ut."
  },
  {
    "objectID": "datadoc.html#hvor-lagres-datadoc-dokumentasjonen-metadata",
    "href": "datadoc.html#hvor-lagres-datadoc-dokumentasjonen-metadata",
    "title": "DataDoc",
    "section": "",
    "text": "N√•r du trykker p√• Lagre-knappen i DataDoc s√• skrives alle metadata til en fil i samme mappe (katalog) som datafilen. Dette er en JSON-fil med nesten samme navn som datafilen. Navnekonvensjonen for metadatafilen er\n&lt;navn p√• datafil uten endelse&gt;__DOC.json\nEksempelvis hvis datafilen har navnet skattedata_p2022_v1.parquet, s√• vil DataDoc lagre metadata i filen skattedata_p2022_v1__DOC.json.\nFordelen med √• benytte en JSON-fil til √• lagre metadata er at denne filen kan kopieres og flyttes like enkelt som selve datafilen. JSON-filer er strukturerte tekstfiler som kan leses av b√•de maskiner (Python/R) og av mennesker (√•pnes i en tekst-editor).\nSe et eksempel p√• JSON metadata-fil lagret av DataDoc.\n\n\n\n\n\n\nInformasjon\n\n\n\nI Dapla skal det bygges en felles datakatalog for SSB. Tanken er at alle metadata, eksempelvis datasett-dokumentasjon fra DataDoc (JSON-filene), skal inng√• i SSBs datakatalog. Datakatalogen gj√∏r det mulig √• finne (s√∏ke etter), forst√•r og gjenbruke data b√•de internt og ekstern."
  }
]