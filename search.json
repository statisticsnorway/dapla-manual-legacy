[
  {
    "objectID": "kartdata.html",
    "href": "kartdata.html",
    "title": "Kartdata",
    "section": "",
    "text": "Det er tilrettelagt kartdata i Dapla til analyse, statistikkproduksjon og visualisering. Det er de samme dataene som er i GEODB i vanlig produksjonssone. Kartdataene er lagret på ssb-prod-kart-data-delt. De er lagret som parquetfiler i standardprojeksjonen vi bruker i SSB (UTM sone 33N) hvis ikke annet er angitt. Det er også SSBs standard-rutenett i ulike størrelser samt Eurostats rutenett over Norge.\nMan søker om tilgang til dataene til kundeservice (tilgangsrollen kart-consumers), men bruk gjerne standard LDA-prosedyre som for øvrige data.\nI tillegg ligger det noe testdata i fellesbøtta her: ssb-prod-dapla-felles-data-delt/GIS/testdata\n\n\n\n\nGeopandas er et Python-bibliotek som integrerer Pandas med GIS-analysering og kartlegging. Geopandas er bygd opp rundt GeoDataFrames, som er en pandas DataFrame med geografisk informasjon. Disse kan lastes inn direkte hvis man har geografiske filer, eller man kan konvertere fra DataFrames med koordinat-kolonner (se eksempel under).\nGeopandas kan blant annet brukes til å kartlegge dataene, beregne avstander og labe variabler for nærmiljø ved å koble datasett sammen basert på geografisk overlapp.\nFor lesing og skriving av geodata i Dapla, kan man bruke SSB-pakken sgis. Med denne kan man blant annet også beregne reisetider langs veg og lage publiseringsverdige kart.\nPakkene kan installeres i Dapla sånn her:\npoetry add geopandas\npoetry add ssb-sgis\nOg så importeres det i Python på vanlig vis.\nimport geopandas as gpd\nimport sgis as sg\nEksempel på lesing/skriving av kartdata\nMan kan lese/skrive geodata i Dapla med sgis-funksjonene read_geopandas og write_geopandas. Støttede formater er blant annet (geo)parquet, geojson og gml. En rekke kartdata for hele landet til analyse- og visualiseringsformål ligger i bøtta “kart”. For eksempel kommuneflater til analyse eller statistikk:\nfilsti = \"ssb-prod-kart-data-delt/kartdata_analyse/klargjorte-data/2023/ABAS_kommune_flate_p2023_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\nEksempel på lesing av kommuneflater for visualisering. Disse dataene er generalisert slik at de egner seg for å lage kart, men blir unøyaktige til statistikk:\nfilsti = \"ssb-prod-kart-data-delt/kartdata_visualisering/klargjorte-data/2023/parquet/N5000_kommune_flate_p2023_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\nTilsvarende for skriving til parquet eller annet geodataformat:\nsg.write_geopandas(kommuner, filsti_ny) \n\n\n\nI tillegg til kartdataene, ligger det testdata i fellesbøtta som man kan leke seg med. Testdataene finnes for Oslo og Kongsvinger, og kan leses sånn her:\ntestdatasti = \"ssb-prod-dapla-felles-data-delt/GIS/testdata\"\nveger = sg.read_geopandas(f\"{testdatasti}/veger_oslo.parquet\")\nUnder følger noen eksempler på GIS-prosessering med testdataene.\nEksempel på avstandsberegning\nAvstand i luftlinje finner man enkelt med sjoin_nearest. Sånn her finner man avstand fra boligbygg (1000 tilfeldige) i Oslo til nærmeste butikkbygg.\nboligbygg = sg.read_geopandas(f\"{testdatasti}/noen_boligbygg_oslo.parquet\")\nbutikkbygg = sg.read_geopandas(f\"{testdatasti}/butikkbygg_oslo.parquet\")\n\nboligbygg_med_butikkavstand = boligbygg.sjoin_nearest(butikkbygg, distance_col=\"meter_til_butikk\")\nFor å finne avstand eller reisetid langs veier, kan man gjøre nettverksanalyse med sgis. Man må først klargjøre vegnettet og bestemme regler for beregningen(e):\nveger_klargjort = (\n    sg.get_connected_components(veger)\n    .query(\"connected == 1\")\n    .pipe(sg.make_directed_network_norway)\n)\n\nregler = sg.NetworkAnalysisRules(directed=True, weight=\"minutes\")\n\nnwa = sg.NetworkAnalysis(network=veger_klargjort, rules=regler)\nSå kan man beregne reisetider fra boligbygg til butikkbygg:\navstander = nwa.od_cost_matrix(boligbygg, butikkbygg)\nKorteste reisetid per bolig kan kobles på som kolonne i boligdataene sånn her:\nboligbygg[\"minutter_til_butikk\"] = avstander.groupby(\"origin\")[\"minutes\"].min()\nUndersøk resultatene i interaktivt kart:\nsg.explore(boligbygg, butikkbygg, \"minutter_til_butikk\")\nSe mer informasjon og eksempler i dokumentasjonen.\nOg flere nettverksanalyse-eksempler her.\nEksempel på geografisk kobling\nDatasett kan kobles basert på geografisk overlapp med geopandas.sjoin. Denne fungerer akkurat som pandas.merge, bare at det kobles basert på geometrien.\nKodesnutten under returnerer én kommuneflate for hvert bygg som overlapper, med kolonner fra begge datasettene. Inner-join er default, men med left-join beholder man også geometriene som ikke overlapper.\nkommuner = sg.read_geopandas(f\"{testdatasti}/enkle_kommuner.parquet\")\nkommuner_med_boliginfo = kommuner.sjoin(boligbygg, how=\"left\")\nMed motsatt rekkefølge, får man ett boligbygg per kommune den overlapper med. Siden byggene er punkter, vil hvert bygg havne i maksimalt én kommune.\nboligbygg_med_kommuneinfo = boligbygg.sjoin(kommuner, how=\"left\")\nEksempel på å lage kart\nMan kan lage tematiske kart med sgis-klassen ThematicMap. Her er et enkelt eksempel på et kart over arealet i kommuner.\nkommuner[\"Kvadratmeter\"] = kommuner.area\n\nm = sg.ThematicMap(kommuner, column=\"Kvadratmeter\", size=15)\nm.title = \"Areal i kommunene\"\nm.plot()\nSe flere kart-eksempler her.\nKonvertere DataFrame til GeoDataFrame\nEn pandas.DataFrame med koordinat-kolonner, kan konverteres til GeoDataFrame sånn her:\nimport pandas as pd\ndf = pd.DataFrame({\"x\": [10.8, 10.7, 10.9], \"y\": [59.9, 60, 59.85]})\n\ngdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y), crs=4326)\nFor å beregne avtand i meter og kunne koble med annen geodata i Dapla, må man ha UTM-koordinater (hvis man ikke hadde det fra før):\ngdf = gdf.to_crs(25833)\nSe også geopandas dokumentasjon for mer utfyllende informasjon.\n\n\n\n\nDen viktigste GIS-pakken i R er sf. Pakken er bygd opp rundt sf-objekter, som er data.frames med geografisk informasjon. Man kan gjøre standard tidyverse-opersjoner på sf-objektene, pluss GIS-operasjoner.\nSindre Mikael Haugen har lagd funksjoner for å lese og skrive blant annet geodata i Dapla. For å få geodata, setter man parametret ‘sf’ til TRUE:\nlibrary(fellesr)\n\nveger = read_SSB(\"ssb-prod-dapla-felles-data-delt/GIS/testdata/veger_oslo.parquet\", sf = TRUE)\nHaugen har også lagd en pakke for å gjøre nettverksanalyse, som også lar deg geokode adresser, altså å finne adressenes koordinater.\nlibrary(GISSB)\nLite eksempel på kobling basert geografisk sammenfall (enkle kommuner er egentlig til visualisering). Her får man ett bygg per kommune som overlapper (som maksimalt er én kommune siden dette er bygningspunkter):\nlibrary(sf)\n\nboligbygg = read_SSB(\"ssb-prod-dapla-felles-data-delt/GIS/testdata/noen_boligbygg_oslo.parquet\", sf = TRUE)\nenkle_kommuner = read_SSB(\"ssb-prod-dapla-felles-data-delt/GIS/testdata/enkle_kommuner.parquet\", sf = TRUE)\n\nboligbygg_med_kommuneinfo = st_join(boligbygg, enkle_kommuner)\nMed motsatt rekkefølge, får man én kommuneflate per bolig som overlapper:\nkommuner_med_boliginfo = st_join(enkle_kommuner, boligbygg)"
  },
  {
    "objectID": "kartdata.html#python",
    "href": "kartdata.html#python",
    "title": "Kartdata",
    "section": "",
    "text": "Geopandas er et Python-bibliotek som integrerer Pandas med GIS-analysering og kartlegging. Geopandas er bygd opp rundt GeoDataFrames, som er en pandas DataFrame med geografisk informasjon. Disse kan lastes inn direkte hvis man har geografiske filer, eller man kan konvertere fra DataFrames med koordinat-kolonner (se eksempel under).\nGeopandas kan blant annet brukes til å kartlegge dataene, beregne avstander og labe variabler for nærmiljø ved å koble datasett sammen basert på geografisk overlapp.\nFor lesing og skriving av geodata i Dapla, kan man bruke SSB-pakken sgis. Med denne kan man blant annet også beregne reisetider langs veg og lage publiseringsverdige kart.\nPakkene kan installeres i Dapla sånn her:\npoetry add geopandas\npoetry add ssb-sgis\nOg så importeres det i Python på vanlig vis.\nimport geopandas as gpd\nimport sgis as sg\nEksempel på lesing/skriving av kartdata\nMan kan lese/skrive geodata i Dapla med sgis-funksjonene read_geopandas og write_geopandas. Støttede formater er blant annet (geo)parquet, geojson og gml. En rekke kartdata for hele landet til analyse- og visualiseringsformål ligger i bøtta “kart”. For eksempel kommuneflater til analyse eller statistikk:\nfilsti = \"ssb-prod-kart-data-delt/kartdata_analyse/klargjorte-data/2023/ABAS_kommune_flate_p2023_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\nEksempel på lesing av kommuneflater for visualisering. Disse dataene er generalisert slik at de egner seg for å lage kart, men blir unøyaktige til statistikk:\nfilsti = \"ssb-prod-kart-data-delt/kartdata_visualisering/klargjorte-data/2023/parquet/N5000_kommune_flate_p2023_v1.parquet\"\nkommuner = sg.read_geopandas(filsti)\nTilsvarende for skriving til parquet eller annet geodataformat:\nsg.write_geopandas(kommuner, filsti_ny) \n\n\n\nI tillegg til kartdataene, ligger det testdata i fellesbøtta som man kan leke seg med. Testdataene finnes for Oslo og Kongsvinger, og kan leses sånn her:\ntestdatasti = \"ssb-prod-dapla-felles-data-delt/GIS/testdata\"\nveger = sg.read_geopandas(f\"{testdatasti}/veger_oslo.parquet\")\nUnder følger noen eksempler på GIS-prosessering med testdataene.\nEksempel på avstandsberegning\nAvstand i luftlinje finner man enkelt med sjoin_nearest. Sånn her finner man avstand fra boligbygg (1000 tilfeldige) i Oslo til nærmeste butikkbygg.\nboligbygg = sg.read_geopandas(f\"{testdatasti}/noen_boligbygg_oslo.parquet\")\nbutikkbygg = sg.read_geopandas(f\"{testdatasti}/butikkbygg_oslo.parquet\")\n\nboligbygg_med_butikkavstand = boligbygg.sjoin_nearest(butikkbygg, distance_col=\"meter_til_butikk\")\nFor å finne avstand eller reisetid langs veier, kan man gjøre nettverksanalyse med sgis. Man må først klargjøre vegnettet og bestemme regler for beregningen(e):\nveger_klargjort = (\n    sg.get_connected_components(veger)\n    .query(\"connected == 1\")\n    .pipe(sg.make_directed_network_norway)\n)\n\nregler = sg.NetworkAnalysisRules(directed=True, weight=\"minutes\")\n\nnwa = sg.NetworkAnalysis(network=veger_klargjort, rules=regler)\nSå kan man beregne reisetider fra boligbygg til butikkbygg:\navstander = nwa.od_cost_matrix(boligbygg, butikkbygg)\nKorteste reisetid per bolig kan kobles på som kolonne i boligdataene sånn her:\nboligbygg[\"minutter_til_butikk\"] = avstander.groupby(\"origin\")[\"minutes\"].min()\nUndersøk resultatene i interaktivt kart:\nsg.explore(boligbygg, butikkbygg, \"minutter_til_butikk\")\nSe mer informasjon og eksempler i dokumentasjonen.\nOg flere nettverksanalyse-eksempler her.\nEksempel på geografisk kobling\nDatasett kan kobles basert på geografisk overlapp med geopandas.sjoin. Denne fungerer akkurat som pandas.merge, bare at det kobles basert på geometrien.\nKodesnutten under returnerer én kommuneflate for hvert bygg som overlapper, med kolonner fra begge datasettene. Inner-join er default, men med left-join beholder man også geometriene som ikke overlapper.\nkommuner = sg.read_geopandas(f\"{testdatasti}/enkle_kommuner.parquet\")\nkommuner_med_boliginfo = kommuner.sjoin(boligbygg, how=\"left\")\nMed motsatt rekkefølge, får man ett boligbygg per kommune den overlapper med. Siden byggene er punkter, vil hvert bygg havne i maksimalt én kommune.\nboligbygg_med_kommuneinfo = boligbygg.sjoin(kommuner, how=\"left\")\nEksempel på å lage kart\nMan kan lage tematiske kart med sgis-klassen ThematicMap. Her er et enkelt eksempel på et kart over arealet i kommuner.\nkommuner[\"Kvadratmeter\"] = kommuner.area\n\nm = sg.ThematicMap(kommuner, column=\"Kvadratmeter\", size=15)\nm.title = \"Areal i kommunene\"\nm.plot()\nSe flere kart-eksempler her.\nKonvertere DataFrame til GeoDataFrame\nEn pandas.DataFrame med koordinat-kolonner, kan konverteres til GeoDataFrame sånn her:\nimport pandas as pd\ndf = pd.DataFrame({\"x\": [10.8, 10.7, 10.9], \"y\": [59.9, 60, 59.85]})\n\ngdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.x, df.y), crs=4326)\nFor å beregne avtand i meter og kunne koble med annen geodata i Dapla, må man ha UTM-koordinater (hvis man ikke hadde det fra før):\ngdf = gdf.to_crs(25833)\nSe også geopandas dokumentasjon for mer utfyllende informasjon."
  },
  {
    "objectID": "kartdata.html#r",
    "href": "kartdata.html#r",
    "title": "Kartdata",
    "section": "",
    "text": "Den viktigste GIS-pakken i R er sf. Pakken er bygd opp rundt sf-objekter, som er data.frames med geografisk informasjon. Man kan gjøre standard tidyverse-opersjoner på sf-objektene, pluss GIS-operasjoner.\nSindre Mikael Haugen har lagd funksjoner for å lese og skrive blant annet geodata i Dapla. For å få geodata, setter man parametret ‘sf’ til TRUE:\nlibrary(fellesr)\n\nveger = read_SSB(\"ssb-prod-dapla-felles-data-delt/GIS/testdata/veger_oslo.parquet\", sf = TRUE)\nHaugen har også lagd en pakke for å gjøre nettverksanalyse, som også lar deg geokode adresser, altså å finne adressenes koordinater.\nlibrary(GISSB)\nLite eksempel på kobling basert geografisk sammenfall (enkle kommuner er egentlig til visualisering). Her får man ett bygg per kommune som overlapper (som maksimalt er én kommune siden dette er bygningspunkter):\nlibrary(sf)\n\nboligbygg = read_SSB(\"ssb-prod-dapla-felles-data-delt/GIS/testdata/noen_boligbygg_oslo.parquet\", sf = TRUE)\nenkle_kommuner = read_SSB(\"ssb-prod-dapla-felles-data-delt/GIS/testdata/enkle_kommuner.parquet\", sf = TRUE)\n\nboligbygg_med_kommuneinfo = st_join(boligbygg, enkle_kommuner)\nMed motsatt rekkefølge, får man én kommuneflate per bolig som overlapper:\nkommuner_med_boliginfo = st_join(enkle_kommuner, boligbygg)"
  },
  {
    "objectID": "jupyterlab.html",
    "href": "jupyterlab.html",
    "title": "Jupyterlab",
    "section": "",
    "text": "Mer kommer.\n\n\n\nMå nevne operativsystemet og at noe programvare ligger installert her (git, jwsacruncher, quarto, ++)\n\n\n\nNoe er i base-image, noe bør gjøres i virtuelle miløer. Hvordan liste ut pakker som er pre-installert?\n\n\n\nJupyterlab er en samling extension. Kan bare installeres av admin. Sikkerhet. Hvilke extension har vi tilgjengeliggjort?\n\n\n\nSane defaults for Jupyterlab."
  },
  {
    "objectID": "jupyterlab.html#hva-er-jupyterlab",
    "href": "jupyterlab.html#hva-er-jupyterlab",
    "title": "Jupyterlab",
    "section": "",
    "text": "Mer kommer."
  },
  {
    "objectID": "jupyterlab.html#terminalen",
    "href": "jupyterlab.html#terminalen",
    "title": "Jupyterlab",
    "section": "",
    "text": "Må nevne operativsystemet og at noe programvare ligger installert her (git, jwsacruncher, quarto, ++)"
  },
  {
    "objectID": "jupyterlab.html#pakkeinstallasjoner",
    "href": "jupyterlab.html#pakkeinstallasjoner",
    "title": "Jupyterlab",
    "section": "",
    "text": "Noe er i base-image, noe bør gjøres i virtuelle miløer. Hvordan liste ut pakker som er pre-installert?"
  },
  {
    "objectID": "jupyterlab.html#extensions",
    "href": "jupyterlab.html#extensions",
    "title": "Jupyterlab",
    "section": "",
    "text": "Jupyterlab er en samling extension. Kan bare installeres av admin. Sikkerhet. Hvilke extension har vi tilgjengeliggjort?"
  },
  {
    "objectID": "jupyterlab.html#tips-triks",
    "href": "jupyterlab.html#tips-triks",
    "title": "Jupyterlab",
    "section": "",
    "text": "Sane defaults for Jupyterlab."
  },
  {
    "objectID": "produksjonsløp.html",
    "href": "produksjonsløp.html",
    "title": "Produksjonsløp",
    "section": "",
    "text": "Produksjonsløp\nI forrige del beskrev vi noen grunnleggende ting rundt Dapla. I denne delen tar vi for oss hvordan du kan begynne å jobbe med skarpe data på plattformen.\nKapittelet som beskriver hvordan man logger seg inn på Dapla vil fungere uten at du må gjøre noen forberedelser. Er man koblet på SSB sitt nettverk så vil alle SSB-ansatte kunne gå inn på plattformen og kode i Python og R. Men du får ikke tilgang til SSBs område for datalagring på plattformen. I praksis vil det si at man kan generere data med kode, men man kan ikke jobbe med skarpe data.\nFor å få muligheten til å jobbe med skarpe data MÅ du først opprette et dapla-team. Dette er det første naturlige steget å ta når man skal begynne å jobbe med statistikkproduksjon på Dapla. I dette kapittelet vil vi forklare det du trenger å vite om det å opprette og jobbe innenfor et team."
  },
  {
    "objectID": "lokale-utviklingsmiljøer.html",
    "href": "lokale-utviklingsmiljøer.html",
    "title": "Lokale utviklingsmiljøer",
    "section": "",
    "text": "Lokale utviklingsmiljøer"
  },
  {
    "objectID": "bakkesystemer.html",
    "href": "bakkesystemer.html",
    "title": "Bakkesystemer",
    "section": "",
    "text": "Bakkesystemer"
  },
  {
    "objectID": "administrasjon-av-team.html",
    "href": "administrasjon-av-team.html",
    "title": "Administrasjon av team",
    "section": "",
    "text": "I dette kapitlet viser vi hvordan du kan opprette et nytt team eller gjøre endringer i et eksisterende team. Typiske endringer er å:\n\nLegge til eller fjerne medlemmer i et team\nListe ut medlemmer og tilgangsgrupper i et team\n\n\n\nFor å komme i gang med å opprette et Dapla-team trengs det en oversikt over teamets medlemmer og hvilke tilgangsgrupper medlemmene skal være med i. Det trengs også informasjon om hvilke Dapla-tjenester som er aktuelle for teamet å ta i bruk. Derfor har det blitt opprettet en egen veileder for dette kalt Dapla Start.\n\n\n\n\n\n\nGå til Dapla Start for starte bestilling av et nytt Dapla-team.\n\n\n\nNår teamet er opprettet får alle medlemmene tilgang til sitt eget prosjekt i Google Cloud Platform (GCP), som er SSBs leverandør av skytjenester. Videre får hvert prosjekt et sett med tjenester og tilganger som knyttes til teamet. Det opprettes også datalagringsområder (ofte kalt bøtter) som bare kan aksesseres av brukere som er med i teamets tilgangsgrupper.\nDapla-teamet vil også få sin egen gruppe i SSBs Active Directory slik at medlemskapet i gruppen kan administreres av Kundeservice.\n\n\n\nFor å legge til eller fjerne medlemmer i et team må du foreløpig opprette en Kundeservice-sak. Oppgi navnet på teamet du ønsker å endre på og hvilke medlemmer du ønsker å legge til eller fjerne. Oppgi også hvilken tilgangsgruppe medlemmene skal ha (data-admins, developers eller consumers). Se hvilke bøtter du får tilgang til med de ulike tilgangsrollene.\nhttps://manual.dapla.ssb.no/hva-er-dapla-team.html#tbl-tilgangsroller\nEndringer i team må godkjennes av seksjonsleder før de blir gjort.\n\n\n\n\n\n\nMidlertidig løsning\n\n\n\nAt endringer i team må gjøres via Kundeservice er midlertidig. Det jobbes med å lage et eget verktøy for dette.\n\n\n\n\n\nFor å se hvem som har de ulike tilgangsrollene i et team, så kan man bruke pakken dapla-team-cli fra Jupyter. Pakken er installert for alle og du kan bruke den ved å gjøre følgende:\n\nLogg deg inn på Dapla.\nÅpne en ny terminal\nSkriv inn dpteam groups list-members og trykk Enter\nI prompten som dukker opp skriver du inn team-navn og trykker Enter.\n\nDa får du listet ut alle medlemmer av teamet og hvilke tilgangsroller de har.\nFor de som ikke har mulighet til å bruke Jupyter så kan man også sende inn en forespørsel til Kundeservice om å få en oversikt."
  },
  {
    "objectID": "administrasjon-av-team.html#opprette-dapla-team",
    "href": "administrasjon-av-team.html#opprette-dapla-team",
    "title": "Administrasjon av team",
    "section": "",
    "text": "For å komme i gang med å opprette et Dapla-team trengs det en oversikt over teamets medlemmer og hvilke tilgangsgrupper medlemmene skal være med i. Det trengs også informasjon om hvilke Dapla-tjenester som er aktuelle for teamet å ta i bruk. Derfor har det blitt opprettet en egen veileder for dette kalt Dapla Start.\n\n\n\n\n\n\nGå til Dapla Start for starte bestilling av et nytt Dapla-team.\n\n\n\nNår teamet er opprettet får alle medlemmene tilgang til sitt eget prosjekt i Google Cloud Platform (GCP), som er SSBs leverandør av skytjenester. Videre får hvert prosjekt et sett med tjenester og tilganger som knyttes til teamet. Det opprettes også datalagringsområder (ofte kalt bøtter) som bare kan aksesseres av brukere som er med i teamets tilgangsgrupper.\nDapla-teamet vil også få sin egen gruppe i SSBs Active Directory slik at medlemskapet i gruppen kan administreres av Kundeservice."
  },
  {
    "objectID": "administrasjon-av-team.html#legge-til-eller-fjerne-medlemmer-i-et-team",
    "href": "administrasjon-av-team.html#legge-til-eller-fjerne-medlemmer-i-et-team",
    "title": "Administrasjon av team",
    "section": "",
    "text": "For å legge til eller fjerne medlemmer i et team må du foreløpig opprette en Kundeservice-sak. Oppgi navnet på teamet du ønsker å endre på og hvilke medlemmer du ønsker å legge til eller fjerne. Oppgi også hvilken tilgangsgruppe medlemmene skal ha (data-admins, developers eller consumers). Se hvilke bøtter du får tilgang til med de ulike tilgangsrollene.\nhttps://manual.dapla.ssb.no/hva-er-dapla-team.html#tbl-tilgangsroller\nEndringer i team må godkjennes av seksjonsleder før de blir gjort.\n\n\n\n\n\n\nMidlertidig løsning\n\n\n\nAt endringer i team må gjøres via Kundeservice er midlertidig. Det jobbes med å lage et eget verktøy for dette."
  },
  {
    "objectID": "administrasjon-av-team.html#se-medlemmer-og-roller-i-et-team",
    "href": "administrasjon-av-team.html#se-medlemmer-og-roller-i-et-team",
    "title": "Administrasjon av team",
    "section": "",
    "text": "For å se hvem som har de ulike tilgangsrollene i et team, så kan man bruke pakken dapla-team-cli fra Jupyter. Pakken er installert for alle og du kan bruke den ved å gjøre følgende:\n\nLogg deg inn på Dapla.\nÅpne en ny terminal\nSkriv inn dpteam groups list-members og trykk Enter\nI prompten som dukker opp skriver du inn team-navn og trykker Enter.\n\nDa får du listet ut alle medlemmer av teamet og hvilke tilgangsroller de har.\nFor de som ikke har mulighet til å bruke Jupyter så kan man også sende inn en forespørsel til Kundeservice om å få en oversikt."
  },
  {
    "objectID": "git-og-github.html",
    "href": "git-og-github.html",
    "title": "Git og Github",
    "section": "",
    "text": "I SSB anbefales det man versjonhåndterer koden sin med Git og deler koden via GitHub. For å lære seg å bruke disse verktøyene på en god måte er det derfor viktig å forstå forskjellen mellom Git og Github. Helt overordnet er forskjellen følgende:\n\nGit er programvare som er installert på maskinen du jobber på og som sporer endringer i koden din.\nGitHub er et slags felles mappesystem på internett som lar deg dele og samarbeide med andre om kode.\n\nAv definisjonene over så skjønner vi at det er Git som gir oss all funksjonalitet for å lagre versjoner av koden vår. GitHub er mer som et valg av mappesystem. Men måten kodemiljøene våre er satt opp på Dapla så har vi ingen fellesmappe som alle kan kjøre koden fra. Man utvikler kode i sin egen hjemmemappe, som bare du har tilgang til, og når du skal samarbeide med andre, så må du sende koden til GitHub. De du samarbeider med må deretter hente ned denne koden før de kan kjøre den.\nI dette kapittelet ser vi nærmere på Git og Github og hvordan de er implementert i SSB. Selv om SSB har laget programmet ssb-project for å gjøre det lettere å bl.a. forholde seg til Git og GitHub, så vil vi dette kapittelet forklare nærmere hvordan det funker uten dette hjelpemiddelet. Forhåpentligvis vil det gjøre det lettere å håndtere mer kompliserte situasjoner som oppstår i arbeidshverdagen som statistikker.\n\n\nGit er terminalprogram som installert på maskinen du jobber. Hvis man ikke liker å bruke terminalen finnes det mange pek-og-klikk versjoner av Git, blant annet i Jupyterlab, SAS EG og RStudio. Men typisk vil det en eller annen gang oppstå situasjoner der det ikke finnes løsninger i pek-og-klikk versjonen, og man må ordne opp i terminalen. Av den grunn velger vi her å fokusere på hvordan Git fungerer fra terminalen. Vi vil også fokusere på hvordan Git fungerer fra terminalen i Jupyterlab på Dapla.\n\n\nGit er en programvare for distribuert versjonshåndtering av filer. Det vil si at den tar vare på historien til koden din. At Git er distribuert betyr at alle som jobber med koden har en kopi av koden hos seg selv. Når man ønsker å dele koden med andre, så laster man opp koden til et felles kodelager som GitHub. Typisk vil Git brukes til å passe på historien til rene tekstfiler, f.eks. kode-script, hvor det gir mening å se på forskjeller mellom filen på ulike tidspunkter. Men når det er sagt, så kan Git også brukes til å følge med på endringer i andre filtyper, f.eks. binære filer som bilder, PDF-filer, etc.. Men binære filer er ikke så vanlig inkludere i Git-prosjekter siden forskjellene i historikken til filene er vanskelig å forstå for mennesker.\nMan aktiverer Git på en mappe i filsystemet sitt med kommandoen git init når man står i mappen som skal versjonshånderes. Da vil Git versjonshåndtere alle filer som er i den mappen og i eventuelle undermapper. Når du så gjør endringer på en fil i mappen, så vil Git registrere endringer. Ønsker du at endringen skal bli et punkt i historikken til prosjektet, så må du først legge til filen i Git med kommandoen git add filnavn. Når du har gjort dette, så kan du lagre endringen med kommandoen git commit -m \"Din melding her\". Når du har gjort dette, så vil endringen være lagret i Git. Når du har gjort mange endringer, så kan du sende endringene til GitHub med kommandoen git push. Når du har gjort dette, så vil endringene være synlige for alle som har tilgang til GitHub-prosjektet.\nI SSB anbefaler vi at du starter et nytt Git-prosjekt ved å benytte ssb-project. Da vil du ikke bare aktivere Git, men du kan også få implementert en del andre gode praksiser for å holde koden din ryddig, oversiktlig og sikker.\nMen før vi kan begynne å bruke Git må vi konfigurere vår egen bruker for Git, som er forklart i neste seksjon.\nLes mer om Git på https://git-scm.com/.\n\n\n\n\n\n\n\n\n\nBruk SSB sin konfigurasjon!\n\n\n\nAlle som bruker GitHub i SSB bør bruke SSB sin Git-konfigurasjon. Hvis ikke kan man bl.a. risikere at output fra Notebooks blir pushet til GitHub. Du kan bruke SSBs konfigurasjon ved å kjøre ssb-gitconfig.py i terminalen og svare på spørsmålene som dukker opp.\n\n\nFor å jobbe med Git så må man konfigurere brukeren sin slik at Git vet hvem som gjør endringer i koden. I praksis betyr det at du må ha filen .gitconfig på hjemmeområdet ditt (f.eks. /home/jovyan/.gitconfig på Dapla) med noe grunnleggende informasjon:\n# /home/jovyan/.gitconfig på Dapla\n[user]\n        name = Ola Nordmann\n        email = &lt;ini&gt;@ssb.no\nMed denne konfigurasjonen så kan man bruke Git lokalt. Men skal man også bruke GitHub i SSB, dvs. dele kode med andre, så må man også legge til mer informasjon i .gitconfig. Heldigvis er det skrevet et script som gjør dette for deg. For å få anbefalt konfigurasjon for Git så kan du kjøre følgende kommando i terminalen:\nssb-gitconfig.py\nDette scriptet vil spørre deg om ditt brukernavn i SSB, og så vil det opprette en fil som heter .gitconfig i hjemmeområdet ditt. Denne filen vil inneholde informasjon om brukernavn, e-post og tilgang til GitHub. I tillegg vil den sørge for at ditt ssb-project ikke sender output fra notebooks til GitHub.\n\n\n\nPå Dapla er det Jupyterlab som er utviklingsmiljøet for kode. I Jupyterlab vil man ofte jobbe i notebooks, og det har noen konsekvenser for versjonshåndtering. En notebook er en ipynb-fil som inneholder både tekst og kode. Åpner vi disse filene i Jupyterlab så ser vi det kjente notebook-grensesnittet med kode- og markdown-celler. Men egentlig er det bare et grensesnitt bygd over en json-fil1. Denne json-filen inneholder metadata og masse annet som gjør det vanskelig å se forskjellen på en fil over tid. Dette er derfor noe som å fikses før Git blir et nyttig verktøy for disse filene.\nI SSB sin anbefalte konfigurasjon setter opp nbdime for å få leselige differ mellom notebooks. Har du satt opp Git med ssb-gitconfig.py så vil dette være automatisk satt opp for deg.\nDet finnes også alternativer til å bruke nbdime. På Dapla er Jupytext installert for de som ikke ønsker å versjonshåndtere ipynb-filer, men heller vil bruke rene script-filer som .py- eller .R-filer. Måten Jupytext gjør dette på er konvertere f.eks. en ipynb-fil py-fil for deg, Den kan gjøre det automatisk når du lagrer, eller du kan gjøre det manuelt. Med denne tilnærmingen så kan du be Git ignorere alle ipynb-filer og bare versjonshåndtere de rene script.filene. Oppsett er Jupytext er ikke en del av SSB sin standard-konfigurasjon og er noe du må sett opp selv.\n\n\n\nGit er veldig sterkt verktøy med mange muligheter. Det er ikke tenkt at alt skal adresseres i dette kapitlet. Men det er noen vanlig operasjoner som er så vanlige at alle som jobber med kode i SSB bør kjenne dem.\nVi har tidligere nevnt at kommandoen for å aktivere versjonshåndtering med Git på en mappe, er git init. Dette gjøres også automatisk når man oppretter et nytt ssb-project.\nHva skjer hvis man gjør en endring i en fil i mappa? For det første kan du kjøre kommandoen git status for å se hvilke endringer Git har oppdaget. Men det betyr ikke at endringene er under versjonskontroll enda. For å fortelle Git om at disse endringene skal registreres så må du kjøre git add &lt;filnavn&gt;. Men fortsatt er ikke endringene et punkt i historien til koden din. For å gjøre det må du kjøre git commit -m \"Din melding her\". Ved å gjøre det så har du laget et unikt punkt i historien til koden din, med en egen ID, som du kan gå tilbake til senere hvis du ønsker.\nNår man utvikler kode så gjør man det fra såkalte branches2. Hvis vi tenker oss at din eksisterende kodebase er stammen på et tre (ofte kalt master eller main), så legger Git opp til at man gjør endringer på denne koden via branches eller grener av treet. Med andre ord holder vi stammen/master urørt helt til vi vet at endringen fungerer som den skal. Til slutt merger vi den inn i master. Vi kan opprette en ny branch og gå inn i den ved å skrive git checkout -b &lt;branch navn&gt;. Da står du i en branch og kan bruke kommandoer som git add og git commit som vist tidligere.\nVi kan merge endringene fra vår branch inn i main ved å først gå inn i main, git switch main, og deretter skrive git merge &lt;branch navn&gt;. Men typisk vil ikke dette være fremgangsmåten i SSB. Når man er fornøyd med endringene i en branch, så vil man pushe den opp til GitHub, slik at en kollega kan vurdere om den skal merges inn i main. Dermed gjøres selve mergen i GitHub-grensenittet. Vi skal se nærmere på GitHub i neste kapittel.\n\n\n\n\nGitHub er et nettsted som bl.a. fungerer som vårt felles mappesystem for deling av kode. SSB har sin egen organisasjonskonto med navn statisticsnorway. Men selv om SSB har en organisasjonskonto på GitHub må alle ansatte opprette sin egen brukerprofil, og knytte den mot SSB sin organisasjonskonto. Under forklarer vi hvordan du gjør dette.\n\n\nDette kapittelet er bare relevant hvis man ikke har en GitHub-brukerkonto fra før. For å bruke ssb-project-programmet til å generere et remote repo på GitHub må du ha en konto. Derfor starter vi med å gjøre dette. Det er en engangsjobb og du trenger aldri gjøre det igjen.\n\n\n\n\n\n\nSSB har valgt å ikke sette opp SSB-brukerne til de ansatte som GitHub-brukere. En viktig årsak er at er en GitHub-konto ofte regnes som en del av den ansattes CV. For de som aldri har brukt GitHub før kan det virke fremmed, men det er nok en fordel på sikt når alle blir godt kjent med denne arbeidsformen.\n\n\n\nSlik gjør du det:\n\nGå til https://github.com/\nTrykk Sign up øverst i høyre hjørne\nI dialogboksen som åpnes, se Figur 1, skriver du inn din e-postkonto og lager et passord. Dette trenger ikke være din SSB-bruker og e-post. Vi anbefaler at du bruker en personlig e-postkonto og velger ditt eget passord. Det samme gjelder brukernavn også.\n\n\n\n\nFigur 1: Dialogboks for opprettelse av GitHub-bruker.\n\n\nDu har nå laget en egen GitHub-bruker. I neste steg skal vi knytte denne kontoen til din SSB-bruker.\n\n\n\nHvis du har fullført forrige steg så har du nå en GitHub-konto. Hvis du står på din profil-side så ser den ut som i Figur 2.\n\n\n\nFigur 2: Et eksempel på hjemmeområdet til en GitHub-bruker\n\n\nDet neste vi må gjøre er å aktivere 2-faktor autentisering, siden det er dette som benyttes i SSB. Hvis du står på siden i bildet over, så gjør du følgende for å aktivere 2-faktor autentisering mot GitHub:\n\n\n\nTrykk på den lille pilen øverst til høyre og velg Settings(se Figur 3).\nDeretter velger du Password and authentification i menyen til venstre.\nUnder Two-factor authentication trykker du på Enable.\n\n\n\n\n\n\nFigur 3: Åpne settings for din GitHub-bruker.\n\n\n\n\n\nFigur 4 viser dialogboksen som vises. Velg Enable two-factor authentification.\n\n\n\n\nFigur 4: Dialogboks som åpnes når 2FA skrus på første gang.\n\n\n\nFigur 5 viser dialogboksen som vises for å velge hvordan man skal autentisere seg. Her anbefales det å velge Set up using an app, slik at du kan bruke Microsoft Authenticator-appen på din mobil.\n\n\n\n\nFigur 5: Dialogboks for å velge hvordan man skal autentisere seg med 2FA.\n\n\nFigur 6 viser QR-koden som vises. Denne skal vi bruke i neste steg.\n\n\n\nFigur 6: QR-kode som skannes av Microsoft Authenticator.\n\n\n\n\n\nStrekkoden over skal skannes i din Microsoft Authenticator-app på mobilen, som vist i Figur 7. Åpne appen, trykk på Bekreftede ID-er, og til slutt trykk på Skann QR-kode. Deretter skanner du QR-koden fra punkt 5.\nNår koden er skannet har du fått opp følgende bilde på appens hovedside (se bilde til høyre). Skriv inn den 6-siffer koden på GitHub-siden med QR-koden.\nTil slutt lagrer du Recovery-codes et trygt sted som bare du har tilgang til.\n\n\n\n\n\n\nFigur 7: Mobilappen Microsoft authenticator\n\n\n\n\nNå har vi aktivert 2-faktor autentisering for GitHub og er klare til å knytte vår personlige konto til vår SSB-bruker på SSBs “Github organisation” statisticsnorway.\n\n\n\nI forrige steg aktiverte vi 2-faktor autentisering for GitHub. Det neste vi må gjøre er å koble oss til Single Sign On (SSO) for SSB sin organisasjon på GitHub:\n\nTrykk på lenken https://github.com/orgs/statisticsnorway/sso\nI dialogboksen som dukker opp trykker du på Continue, slik som vist i Figur 8.\n\n\n\n\nFigur 8: Single Sign on (SSO) for SSB sin organisasjon på GitHub\n\n\nNår du har gjennomført dette så har du tilgang til statisticsnorway på GitHub. Går du inn på denne lenken så skal du nå kunne lese både Public, Private og Internal repoer, slik som vist i Figur 9.\n\n\n\nFigur 9: Medlemsvisning for SSB sin GitHub-organisasjon.\n\n\n\n\n\nNår vi skal jobbe med SSB-kode som ligger lagret hos statistcsnorway på GitHub, så må vi autentisere oss. Måten vi gjøre det på er ved å generere et Personal Access Token (ofte forkortet PAT) som vi oppgir når vi vil hente eller oppdatere kode på GitHub. Da sender vi med PAT for å autentisere oss for GitHub.\n\n\nFor å lage en PAT som er godkjent mot statisticsnorway så gjør man følgende:\n\nGå til din profilside på GitHub og åpne Settings slik som ble vist Seksjon 1.2.2.\nVelg Developer Settings i menyen til venstre.\nI menyen til venstre velger du Personal Access Token, og deretter Tokens (classic).\nUnder Note kan du gi PAT’en et navn. Velg et navn som er intuitivt for deg. Hvis du skal bruke PAT til å jobbe mot Dapla, så ville jeg ganske enkelt kalt den dapla. Hvis du skal bruke den mot bakkemiljøet ville jeg kalt den prodsone eller noe annet som gjør det lett for det skjønne innholdet i ettertid.\nUnder Expiration velger du hvor lang tid som skal gå før PAT blir ugyldig. Dette er en avvening mellom sikkerhet og hva som er praktisk. Det anbefales at du velger 365 dager. Når PAT utløper må du gjenta stegene i dette kapittelet.\nUnder Select scopes velger du Repo slik som vist i Figur 10.\n\n\n\n\nFigur 10: Gi token et kort og beskrivende navn\n\n\n\nTrykk på Generate token nederst på siden og du får noe lignende det du ser i Figur 11.\n\n\n\n\nFigur 11: Token som ble generert.\n\n\n\nKopier deretter PAT til en midlertidig fil. Grunnen er at du aldri vil se det igjen her etter at vi har gjennomført neste steg.\nDeretter trykker du på Configure SSO og velger Authorize ved siden statisticsnorway, slik som vist i Figur 12. Svar deretter på spørsmålene som dukker opp.\n\n\n\n\nFigur 12: Autorisering av Token mot SSBs GiHub-organisasjon.\n\n\nVi har nå opprettet en PAT som er godkjent for bruk mot SSB sin kode på GitHub. Det betyr at hvis vi vil jobbe med Git på SSB sine maskiner i sky eller på bakken, så må vi sendte med dette tokenet for å få lov til å jobbe med koden som ligger på statisticsnorway på GitHub.\n\n\n\nDet er ganske upraktisk å måtte sende med tokenet hver gang vi skal jobbe med GitHub. Vi bør derfor lagre det lokalt der vi jobber, slik at Git automatisk finner det. Det finnes mange måter å gjøre dette på og det er ikke bestemt hva som skal være beste-praksis i SSB. Men en måte å gjøre det er via en .netrc-fil. Vi oppretter da en fil som heter .netrc på vårt hjemmeområde, og legger følgende informasjon på en (hvilken som helst) linje i filen:\nmachine github.com login &lt;github-bruker&gt; password &lt;Personal Access Token&gt;\nGitHub-bruker er da din personlige bruker og IKKE brukernavnet ditt i SSB. Personal Access Token er det vi lagde i forrige kapittelet.\nEn veldig enkel måte å lagre dette er som følger. Anta at min personlige GitHub-bruker er SSB-Chad og at min Personal Access Token er blablabla. Da kan jeg gjøre følgende for å lagre det i .netrc:\n\nGå inn i Jupyterlab og åpne en Python-notebook.\nI den første kodecellen skriver du:\n\n!echo \"machine github.com login SSB-Chad password blablabla\" &gt;&gt; ~/.netrc\nAlternativt kan du droppe det utropstegnet og kjøre det direkte i en terminal. Det vil gi samme resultat. Koden over legger til en linje med teksten\nmachine github.com login SSB-Chad password blablabla\ni en .netrc-fil på din hjemmeområdet, uanvhengig av om du har en fra før eller ikke. Hvis du har en fil fra før som allerede har et token fra GitHub, ville jeg nok slettet det før jeg legger en et nytt token.\nHver gang du jobber mot GitHub vil Git sjekke om informasjon om autentisering ligger i denne filen, og bruke den hvis den ligger der.\n\n\n\nI eksempelet over lagde vi en PAT som var gyldig i 90 dager. Dermed vil du ikke kunne jobbe mot GitHub med dette tokenet etter 90 dager. For å oppdatere tokenet gjør du følgende:\n\nLag et nytt PAT ved å repetere Seksjon 1.2.4.1.\nI miljøet der du skal jobbe med Git og GitHub går du inn i din .netrc og bytter ut token med det nye.\n\nOg med det er du klar til å jobbe mot statisticsnorway på GitHub."
  },
  {
    "objectID": "git-og-github.html#git-fa-brands-git-alt",
    "href": "git-og-github.html#git-fa-brands-git-alt",
    "title": "Git og Github",
    "section": "",
    "text": "Git er terminalprogram som installert på maskinen du jobber. Hvis man ikke liker å bruke terminalen finnes det mange pek-og-klikk versjoner av Git, blant annet i Jupyterlab, SAS EG og RStudio. Men typisk vil det en eller annen gang oppstå situasjoner der det ikke finnes løsninger i pek-og-klikk versjonen, og man må ordne opp i terminalen. Av den grunn velger vi her å fokusere på hvordan Git fungerer fra terminalen. Vi vil også fokusere på hvordan Git fungerer fra terminalen i Jupyterlab på Dapla.\n\n\nGit er en programvare for distribuert versjonshåndtering av filer. Det vil si at den tar vare på historien til koden din. At Git er distribuert betyr at alle som jobber med koden har en kopi av koden hos seg selv. Når man ønsker å dele koden med andre, så laster man opp koden til et felles kodelager som GitHub. Typisk vil Git brukes til å passe på historien til rene tekstfiler, f.eks. kode-script, hvor det gir mening å se på forskjeller mellom filen på ulike tidspunkter. Men når det er sagt, så kan Git også brukes til å følge med på endringer i andre filtyper, f.eks. binære filer som bilder, PDF-filer, etc.. Men binære filer er ikke så vanlig inkludere i Git-prosjekter siden forskjellene i historikken til filene er vanskelig å forstå for mennesker.\nMan aktiverer Git på en mappe i filsystemet sitt med kommandoen git init når man står i mappen som skal versjonshånderes. Da vil Git versjonshåndtere alle filer som er i den mappen og i eventuelle undermapper. Når du så gjør endringer på en fil i mappen, så vil Git registrere endringer. Ønsker du at endringen skal bli et punkt i historikken til prosjektet, så må du først legge til filen i Git med kommandoen git add filnavn. Når du har gjort dette, så kan du lagre endringen med kommandoen git commit -m \"Din melding her\". Når du har gjort dette, så vil endringen være lagret i Git. Når du har gjort mange endringer, så kan du sende endringene til GitHub med kommandoen git push. Når du har gjort dette, så vil endringene være synlige for alle som har tilgang til GitHub-prosjektet.\nI SSB anbefaler vi at du starter et nytt Git-prosjekt ved å benytte ssb-project. Da vil du ikke bare aktivere Git, men du kan også få implementert en del andre gode praksiser for å holde koden din ryddig, oversiktlig og sikker.\nMen før vi kan begynne å bruke Git må vi konfigurere vår egen bruker for Git, som er forklart i neste seksjon.\nLes mer om Git på https://git-scm.com/.\n\n\n\n\n\n\n\n\n\nBruk SSB sin konfigurasjon!\n\n\n\nAlle som bruker GitHub i SSB bør bruke SSB sin Git-konfigurasjon. Hvis ikke kan man bl.a. risikere at output fra Notebooks blir pushet til GitHub. Du kan bruke SSBs konfigurasjon ved å kjøre ssb-gitconfig.py i terminalen og svare på spørsmålene som dukker opp.\n\n\nFor å jobbe med Git så må man konfigurere brukeren sin slik at Git vet hvem som gjør endringer i koden. I praksis betyr det at du må ha filen .gitconfig på hjemmeområdet ditt (f.eks. /home/jovyan/.gitconfig på Dapla) med noe grunnleggende informasjon:\n# /home/jovyan/.gitconfig på Dapla\n[user]\n        name = Ola Nordmann\n        email = &lt;ini&gt;@ssb.no\nMed denne konfigurasjonen så kan man bruke Git lokalt. Men skal man også bruke GitHub i SSB, dvs. dele kode med andre, så må man også legge til mer informasjon i .gitconfig. Heldigvis er det skrevet et script som gjør dette for deg. For å få anbefalt konfigurasjon for Git så kan du kjøre følgende kommando i terminalen:\nssb-gitconfig.py\nDette scriptet vil spørre deg om ditt brukernavn i SSB, og så vil det opprette en fil som heter .gitconfig i hjemmeområdet ditt. Denne filen vil inneholde informasjon om brukernavn, e-post og tilgang til GitHub. I tillegg vil den sørge for at ditt ssb-project ikke sender output fra notebooks til GitHub.\n\n\n\nPå Dapla er det Jupyterlab som er utviklingsmiljøet for kode. I Jupyterlab vil man ofte jobbe i notebooks, og det har noen konsekvenser for versjonshåndtering. En notebook er en ipynb-fil som inneholder både tekst og kode. Åpner vi disse filene i Jupyterlab så ser vi det kjente notebook-grensesnittet med kode- og markdown-celler. Men egentlig er det bare et grensesnitt bygd over en json-fil1. Denne json-filen inneholder metadata og masse annet som gjør det vanskelig å se forskjellen på en fil over tid. Dette er derfor noe som å fikses før Git blir et nyttig verktøy for disse filene.\nI SSB sin anbefalte konfigurasjon setter opp nbdime for å få leselige differ mellom notebooks. Har du satt opp Git med ssb-gitconfig.py så vil dette være automatisk satt opp for deg.\nDet finnes også alternativer til å bruke nbdime. På Dapla er Jupytext installert for de som ikke ønsker å versjonshåndtere ipynb-filer, men heller vil bruke rene script-filer som .py- eller .R-filer. Måten Jupytext gjør dette på er konvertere f.eks. en ipynb-fil py-fil for deg, Den kan gjøre det automatisk når du lagrer, eller du kan gjøre det manuelt. Med denne tilnærmingen så kan du be Git ignorere alle ipynb-filer og bare versjonshåndtere de rene script.filene. Oppsett er Jupytext er ikke en del av SSB sin standard-konfigurasjon og er noe du må sett opp selv.\n\n\n\nGit er veldig sterkt verktøy med mange muligheter. Det er ikke tenkt at alt skal adresseres i dette kapitlet. Men det er noen vanlig operasjoner som er så vanlige at alle som jobber med kode i SSB bør kjenne dem.\nVi har tidligere nevnt at kommandoen for å aktivere versjonshåndtering med Git på en mappe, er git init. Dette gjøres også automatisk når man oppretter et nytt ssb-project.\nHva skjer hvis man gjør en endring i en fil i mappa? For det første kan du kjøre kommandoen git status for å se hvilke endringer Git har oppdaget. Men det betyr ikke at endringene er under versjonskontroll enda. For å fortelle Git om at disse endringene skal registreres så må du kjøre git add &lt;filnavn&gt;. Men fortsatt er ikke endringene et punkt i historien til koden din. For å gjøre det må du kjøre git commit -m \"Din melding her\". Ved å gjøre det så har du laget et unikt punkt i historien til koden din, med en egen ID, som du kan gå tilbake til senere hvis du ønsker.\nNår man utvikler kode så gjør man det fra såkalte branches2. Hvis vi tenker oss at din eksisterende kodebase er stammen på et tre (ofte kalt master eller main), så legger Git opp til at man gjør endringer på denne koden via branches eller grener av treet. Med andre ord holder vi stammen/master urørt helt til vi vet at endringen fungerer som den skal. Til slutt merger vi den inn i master. Vi kan opprette en ny branch og gå inn i den ved å skrive git checkout -b &lt;branch navn&gt;. Da står du i en branch og kan bruke kommandoer som git add og git commit som vist tidligere.\nVi kan merge endringene fra vår branch inn i main ved å først gå inn i main, git switch main, og deretter skrive git merge &lt;branch navn&gt;. Men typisk vil ikke dette være fremgangsmåten i SSB. Når man er fornøyd med endringene i en branch, så vil man pushe den opp til GitHub, slik at en kollega kan vurdere om den skal merges inn i main. Dermed gjøres selve mergen i GitHub-grensenittet. Vi skal se nærmere på GitHub i neste kapittel."
  },
  {
    "objectID": "git-og-github.html#github-fa-brands-github",
    "href": "git-og-github.html#github-fa-brands-github",
    "title": "Git og Github",
    "section": "",
    "text": "GitHub er et nettsted som bl.a. fungerer som vårt felles mappesystem for deling av kode. SSB har sin egen organisasjonskonto med navn statisticsnorway. Men selv om SSB har en organisasjonskonto på GitHub må alle ansatte opprette sin egen brukerprofil, og knytte den mot SSB sin organisasjonskonto. Under forklarer vi hvordan du gjør dette.\n\n\nDette kapittelet er bare relevant hvis man ikke har en GitHub-brukerkonto fra før. For å bruke ssb-project-programmet til å generere et remote repo på GitHub må du ha en konto. Derfor starter vi med å gjøre dette. Det er en engangsjobb og du trenger aldri gjøre det igjen.\n\n\n\n\n\n\nSSB har valgt å ikke sette opp SSB-brukerne til de ansatte som GitHub-brukere. En viktig årsak er at er en GitHub-konto ofte regnes som en del av den ansattes CV. For de som aldri har brukt GitHub før kan det virke fremmed, men det er nok en fordel på sikt når alle blir godt kjent med denne arbeidsformen.\n\n\n\nSlik gjør du det:\n\nGå til https://github.com/\nTrykk Sign up øverst i høyre hjørne\nI dialogboksen som åpnes, se Figur 1, skriver du inn din e-postkonto og lager et passord. Dette trenger ikke være din SSB-bruker og e-post. Vi anbefaler at du bruker en personlig e-postkonto og velger ditt eget passord. Det samme gjelder brukernavn også.\n\n\n\n\nFigur 1: Dialogboks for opprettelse av GitHub-bruker.\n\n\nDu har nå laget en egen GitHub-bruker. I neste steg skal vi knytte denne kontoen til din SSB-bruker.\n\n\n\nHvis du har fullført forrige steg så har du nå en GitHub-konto. Hvis du står på din profil-side så ser den ut som i Figur 2.\n\n\n\nFigur 2: Et eksempel på hjemmeområdet til en GitHub-bruker\n\n\nDet neste vi må gjøre er å aktivere 2-faktor autentisering, siden det er dette som benyttes i SSB. Hvis du står på siden i bildet over, så gjør du følgende for å aktivere 2-faktor autentisering mot GitHub:\n\n\n\nTrykk på den lille pilen øverst til høyre og velg Settings(se Figur 3).\nDeretter velger du Password and authentification i menyen til venstre.\nUnder Two-factor authentication trykker du på Enable.\n\n\n\n\n\n\nFigur 3: Åpne settings for din GitHub-bruker.\n\n\n\n\n\nFigur 4 viser dialogboksen som vises. Velg Enable two-factor authentification.\n\n\n\n\nFigur 4: Dialogboks som åpnes når 2FA skrus på første gang.\n\n\n\nFigur 5 viser dialogboksen som vises for å velge hvordan man skal autentisere seg. Her anbefales det å velge Set up using an app, slik at du kan bruke Microsoft Authenticator-appen på din mobil.\n\n\n\n\nFigur 5: Dialogboks for å velge hvordan man skal autentisere seg med 2FA.\n\n\nFigur 6 viser QR-koden som vises. Denne skal vi bruke i neste steg.\n\n\n\nFigur 6: QR-kode som skannes av Microsoft Authenticator.\n\n\n\n\n\nStrekkoden over skal skannes i din Microsoft Authenticator-app på mobilen, som vist i Figur 7. Åpne appen, trykk på Bekreftede ID-er, og til slutt trykk på Skann QR-kode. Deretter skanner du QR-koden fra punkt 5.\nNår koden er skannet har du fått opp følgende bilde på appens hovedside (se bilde til høyre). Skriv inn den 6-siffer koden på GitHub-siden med QR-koden.\nTil slutt lagrer du Recovery-codes et trygt sted som bare du har tilgang til.\n\n\n\n\n\n\nFigur 7: Mobilappen Microsoft authenticator\n\n\n\n\nNå har vi aktivert 2-faktor autentisering for GitHub og er klare til å knytte vår personlige konto til vår SSB-bruker på SSBs “Github organisation” statisticsnorway.\n\n\n\nI forrige steg aktiverte vi 2-faktor autentisering for GitHub. Det neste vi må gjøre er å koble oss til Single Sign On (SSO) for SSB sin organisasjon på GitHub:\n\nTrykk på lenken https://github.com/orgs/statisticsnorway/sso\nI dialogboksen som dukker opp trykker du på Continue, slik som vist i Figur 8.\n\n\n\n\nFigur 8: Single Sign on (SSO) for SSB sin organisasjon på GitHub\n\n\nNår du har gjennomført dette så har du tilgang til statisticsnorway på GitHub. Går du inn på denne lenken så skal du nå kunne lese både Public, Private og Internal repoer, slik som vist i Figur 9.\n\n\n\nFigur 9: Medlemsvisning for SSB sin GitHub-organisasjon.\n\n\n\n\n\nNår vi skal jobbe med SSB-kode som ligger lagret hos statistcsnorway på GitHub, så må vi autentisere oss. Måten vi gjøre det på er ved å generere et Personal Access Token (ofte forkortet PAT) som vi oppgir når vi vil hente eller oppdatere kode på GitHub. Da sender vi med PAT for å autentisere oss for GitHub.\n\n\nFor å lage en PAT som er godkjent mot statisticsnorway så gjør man følgende:\n\nGå til din profilside på GitHub og åpne Settings slik som ble vist Seksjon 1.2.2.\nVelg Developer Settings i menyen til venstre.\nI menyen til venstre velger du Personal Access Token, og deretter Tokens (classic).\nUnder Note kan du gi PAT’en et navn. Velg et navn som er intuitivt for deg. Hvis du skal bruke PAT til å jobbe mot Dapla, så ville jeg ganske enkelt kalt den dapla. Hvis du skal bruke den mot bakkemiljøet ville jeg kalt den prodsone eller noe annet som gjør det lett for det skjønne innholdet i ettertid.\nUnder Expiration velger du hvor lang tid som skal gå før PAT blir ugyldig. Dette er en avvening mellom sikkerhet og hva som er praktisk. Det anbefales at du velger 365 dager. Når PAT utløper må du gjenta stegene i dette kapittelet.\nUnder Select scopes velger du Repo slik som vist i Figur 10.\n\n\n\n\nFigur 10: Gi token et kort og beskrivende navn\n\n\n\nTrykk på Generate token nederst på siden og du får noe lignende det du ser i Figur 11.\n\n\n\n\nFigur 11: Token som ble generert.\n\n\n\nKopier deretter PAT til en midlertidig fil. Grunnen er at du aldri vil se det igjen her etter at vi har gjennomført neste steg.\nDeretter trykker du på Configure SSO og velger Authorize ved siden statisticsnorway, slik som vist i Figur 12. Svar deretter på spørsmålene som dukker opp.\n\n\n\n\nFigur 12: Autorisering av Token mot SSBs GiHub-organisasjon.\n\n\nVi har nå opprettet en PAT som er godkjent for bruk mot SSB sin kode på GitHub. Det betyr at hvis vi vil jobbe med Git på SSB sine maskiner i sky eller på bakken, så må vi sendte med dette tokenet for å få lov til å jobbe med koden som ligger på statisticsnorway på GitHub.\n\n\n\nDet er ganske upraktisk å måtte sende med tokenet hver gang vi skal jobbe med GitHub. Vi bør derfor lagre det lokalt der vi jobber, slik at Git automatisk finner det. Det finnes mange måter å gjøre dette på og det er ikke bestemt hva som skal være beste-praksis i SSB. Men en måte å gjøre det er via en .netrc-fil. Vi oppretter da en fil som heter .netrc på vårt hjemmeområde, og legger følgende informasjon på en (hvilken som helst) linje i filen:\nmachine github.com login &lt;github-bruker&gt; password &lt;Personal Access Token&gt;\nGitHub-bruker er da din personlige bruker og IKKE brukernavnet ditt i SSB. Personal Access Token er det vi lagde i forrige kapittelet.\nEn veldig enkel måte å lagre dette er som følger. Anta at min personlige GitHub-bruker er SSB-Chad og at min Personal Access Token er blablabla. Da kan jeg gjøre følgende for å lagre det i .netrc:\n\nGå inn i Jupyterlab og åpne en Python-notebook.\nI den første kodecellen skriver du:\n\n!echo \"machine github.com login SSB-Chad password blablabla\" &gt;&gt; ~/.netrc\nAlternativt kan du droppe det utropstegnet og kjøre det direkte i en terminal. Det vil gi samme resultat. Koden over legger til en linje med teksten\nmachine github.com login SSB-Chad password blablabla\ni en .netrc-fil på din hjemmeområdet, uanvhengig av om du har en fra før eller ikke. Hvis du har en fil fra før som allerede har et token fra GitHub, ville jeg nok slettet det før jeg legger en et nytt token.\nHver gang du jobber mot GitHub vil Git sjekke om informasjon om autentisering ligger i denne filen, og bruke den hvis den ligger der.\n\n\n\nI eksempelet over lagde vi en PAT som var gyldig i 90 dager. Dermed vil du ikke kunne jobbe mot GitHub med dette tokenet etter 90 dager. For å oppdatere tokenet gjør du følgende:\n\nLag et nytt PAT ved å repetere Seksjon 1.2.4.1.\nI miljøet der du skal jobbe med Git og GitHub går du inn i din .netrc og bytter ut token med det nye.\n\nOg med det er du klar til å jobbe mot statisticsnorway på GitHub."
  },
  {
    "objectID": "git-og-github.html#footnotes",
    "href": "git-og-github.html#footnotes",
    "title": "Git og Github",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPrøv selv å åpne en ipynb som json ved høreklikke på fila i Jupyterlab, velge Open with, og velg json. Da vil du se den underliggende json-filen↩︎\nBranches kan oversettes til grener på norsk. Men i denne boken velger vi å bruke det engelske ordet branches. Grunnen er at det erfaringsmessig er lettere forholde seg til det engelske ordet når man skal søke etter informasjon i annen dokumentasjon↩︎"
  },
  {
    "objectID": "kildedata.html",
    "href": "kildedata.html",
    "title": "Kildedata",
    "section": "",
    "text": "Kildedata er data lagret slik de ble levert til SSB fra dataeier, det vil si på dataeiers dataformat og med informasjon om tidspunkt og rekkefølge for avlevering. Kildedata er en del av statistikkenes dokumentasjon, og kan være en nødvendig kilde for forskning og nye statistikker. Uten kildedataene vil det ikke være mulig å etterprøve SSB sine statistikker. De originale kildedataene vil ofte komprimeres og krypteres etter at relevante deler er transformert til inndata.\n\n(Standardutvalget 2021, 7)\n\nStatistikkloven § 9 Informasjonssikkerhet stiller krav om at direkte identifiserende opplysninger skal behandles og lagres adskilt fra øvrige opplysninger, med mindre det vil være uforenlig med formålet med behandlingen eller åpenbart unødvendig. I henhold til policy om Datatilstander er kildedata i utgangspunktet den eneste datatilstanden som kan inneholde denne type data. I øvrige tilstander skal direkteidentifiserende opplysninger som hovedregel være pseudonymisert. Avvik skal dokumenteres og godkjennes av seksjonsleder som er ansvarlig for avviket.\n\n(Direktørmøtet 2022, 2)\nFordi Kildedata kan inneholde PII1 implementerer Dapla følgende tiltak:\n\nKildedata er lagret adskilt fra andre datatilstander.\nTilgang til dataene begrenses så langt som mulig, kun en begrenset gruppe personer2 har tilgang til kildedata.\nProsessering av kildedata utføres automatisk for minske behov for tilgang til dataene."
  },
  {
    "objectID": "kildedata.html#footnotes",
    "href": "kildedata.html#footnotes",
    "title": "Kildedata",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPersonidentifiserende Informasjon↩︎\nData admins↩︎"
  },
  {
    "objectID": "orkestrering.html",
    "href": "orkestrering.html",
    "title": "Orkestrering",
    "section": "",
    "text": "Orkestrering"
  },
  {
    "objectID": "pakke-install.html",
    "href": "pakke-install.html",
    "title": "Installere pakker",
    "section": "",
    "text": "Installering av pakker er kun er mulig I et virtuelt miljø. Det er anbefalt å benytte poetry til dette. Eksemplene videre tar derfor utgangspunkt i et poetry prosjekt.\nDet er mulig å installere pakker med pip. Pakker kan installeres som normalt, hvis man har satt opp og aktivert et virtuelt miljø.\n\n\nDette eksemplet viser hvordan man setter oppe et enkelt poetry prosjekt kalt test, hvis man ønsker å benytte et annet prosjektnavn må man endre dette i hver av kommandoene.\nSett opp prosjektet:\npoetry new test\nNaviger inn i prosjektmappen:\ncd test\nBruk poetry install for å bygge prosjektet:\npoetry install\nHvis man får en tilbakemelding som denne er prosjektet satt opp korrekt:\nCreating virtualenv test-EojoH6Zm-py3.10 in /home/jovyan/.cache/pypoetry/virtualenvs \nUpdating dependencies \nResolving dependencies... (0.1s) \n\nWriting lock file \n\n\n\nFor å legge til pakker i et prosjekt benyttes kommandoen poetry add.\nSkal man legge til pakken “pendulum” vil det se slik ut:\npoetry add pendulum\nPoetry tilbyr måter å sette versjonsbegrensninger for pakker som legges til i et prosjekt, dette kan man lese mer om her.\n\n\n\nFor å fjerne pakker fra et prosjekt benytter man poetry remove.\nHvis man ønsker å fjerne “pendulum” fra et prosjekt vil kommandoen se slik ut:\npoetry remove pendulum\n\n\n\nFor å oppdatere pakker i et prosjekt benytter man kommandoen poetry update.\nSkal man oppdatere pakken “pendulum” bruker man:\npoetry update pendulum\nSkal man oppdatre alle pakken i et prosjekt benytter man:\npoetry update \n\n\n\nNår man installerer pakker så vil det etter hvert utvikle seg et sett av potensielt kompliserte avhengigheter mellom disse pakkene. Dette skyldes at en pakke kan benytte seg av funksjonalitet i andre pakker, som igjen benytter seg av funksjonalitet i andre pakker, osv.. Hvis noen finner en sikkerhetssårbarhet i en pakke så kan det fikses ved at en ny versjon av den pakken slippes, som igjen kan få konsekvenser for pakker som er avhengig av denne.\nI SSB er det tilrettelagt for at alle som versjonshåndterer koden sin på GitHub kan skanne pakkene sine for sårbarheter og nye versjoner av pakker med Dependabot. Dependabot hjelper oss med å finne og fikse sårbarheter og gamle pakkeversjoner. Dette er spesielt viktig når man installerer sine egne pakker.\nDet er anbefalt at alle som installerer sine egne pakker i SSB skrur på Dependabot i sine GitHub-repoer. Du kan skru på ved å gjøre følgende:\n\nGå inn repoet\nTrykk på Settings for det repoet som vist på Figur 1.\n\n\n\n\nFigur 1: Åpne Settings for et GitHub-repo.\n\n\n\nI menyen til venstre velger du Code security and analysis\nUnder seksjonen Dependabot velger Enable på minst Dependabot alerts og Dependabot security updates, slik som vist i Figur 2.\n\n\n\n\nFigur 2: Skru på Dependabot i GitHub.\n\n\nNår du har gjort dette vil GitHub varsle deg hvis det finnes en kjent sårbarhet i pakkene som benyttes.\n\n\n\nFor å kunne benytte det virtuelle miljøet i en notebook må man sette opp en kernel. Kernel burde gis samme navn som prosjektet.\nFørst legger man til ipykernel:\npoetry add ipykernel\nSå opprettes kernel med:\npoetry run python -m ipykernel install --user --name test\nEtter dette er kernelen test opprettet og kan velges for å benytte miljøet i en notebook.\n\n\n\nFor å fjerne en kernel med navn test bruker man:\njupyter kernelspec remove test\nDu vil bli spurt om å bekrefte, trykk y hvis man ønsker å slette:\nKernel specs to remove:\n  test                    /home/jovyan/.local/share/jupyter/kernels/test\nRemove 1 kernel specs [y/N]: y\nEtter dette er kernelen fjernet.\n\n\n\nHvem som helst kan legge til pakker på PyPi, det betyr at de i verstefall, kan inneholde skadelig kode. Her er en list med viktige tiltak som minimere risikoen:\n\nFør man installerer pakker bør man alltid søke de opp på https://pypi.org. Det er anbefalt å klippe og lime inn pakkenavnet når man skal legge det til i et prosjekt.\nEr det et populært/velkjent prosjekt? Hvor mange stjerner og forks har repoet?\n\n\n\n\n\nInstallering av pakker for R-miljøet i Jupyterlab er foreløpig ikke en del av ssb-project. Men vi kan bruke renv. Mer kommer.\n\n\nFor å installere dine egne R-pakker må du opprette et virtuelt miljø med renv. Gå inn i Jupyterlab og åpne R-notebook. Deretter skriver du inn følgende i kodecelle:\nrenv::init()\nDenne kommandoer aktiverer et virtuelt miljø i mappen du står i. Rent praktisk vil det si at du fikk følgende filer/mapper i mappen din:\nrenv.lock\nEn fil som inneholder versjoner av alle pakker du benytter i koden din.\n.Rprofile En fil som inneholder informasjon om oppsetting av miljø og alternative.\nrenv\nMappe som inneholder alle pakkene du installerer.\nrenv/activate.R En fil som aktivere renv miljø for et prosjekt.\nHvis prosjektet ligger på GitHub, skal filene renv.lock, .Rprofile og renv/activate.R være på GitHub\nNå som vi har et virtuelle miljøet på plass kan vi installere en R-pakke. Du kan gjøre dette fra både terminalen og fra en Notebook. Vi anbefaler på gjøre det fra terminalen fordi du da får tilbakemelding på om installeringen gikk bra heller ikke. For å installere i terminalen gjør du følgende:\n\nÅpne en terminal i Jupyterlab\nStå i mappen der du aktiverte det virtuelle miljøet\nSkriv in R og trykk enter.\n\nDet vi nå har gjort er å åpne R fra terminalen slik at vi kan skrive R-kode direkte i terminalen. Det omtales ofte som en R Console. Nå kan du skrive inn en vanlig kommando for å installere R-pakker:\nrenv::install(\"PxWebApiData\")\nOver installerte vi pakken PxWebApiData fra den R sentral repository CRAN. Dette er en pakke skrevet i SSB for å hente ut data fra vår statistikkbank. Det er også mulig å installere pakker som ligger på SSBs GitHub. Da må vi spesifisere at pakke ligger på ‘statisticsnorway’ område. For eksempel:\nrenv::install(\"statisticsnorway/klassR\")\nPakken klassR er skrevet for å hente ut klassifikasjoner fra SSBs KLASS. Det er en public repository på Github og åpen for alle å laste ned. For pakker som er på et lukket område på ‘statsitcsnorway’ må vi bruke Personal Authentication Token for å installere. Vi kan gjøre dette ved hjelp av funksjonen install_github() i devtools pakken. For eksempel:\nrenv::install(\"devtools\")\nrenv::install(\"getPass\")\ndevtools::install_github(\"statisticsnorway/fellesr\", \n                        auth_token = getPass::getPass())\nLa oss bruke pakken PxWebApiData i koden vår med ved å skrive følgende i kodecelle i Notebooken vår:\nlibrary(PxWebApiData)\nApiData(\"https://data.ssb.no/api/v0/en/table/04861\", \n        Region = c(\"1103\", \"0301\"), ContentsCode = \"Bosatte\", Tid = c(1, 2, -2, -1))\nNår vi nå har brukt PxWebApiData i koden vår så kan vi kjøre en kommando som legger til den pakken i renv.lock. Men før vi kan gjøre det må vi være obs på at renv ikke klarer å gjenkjenne pakker som er i bruk Notebooks (ipynb-filer). Det er veldig upraktisk, men noe vi må forholde oss til når vi jobber med renv i Jupyterlab. En mulig løsning for dette er å bruke Jupytext til å synkronisere en ipynb-fil med en Rmd-fil. renv kjenner igjen både R- og Rmd-filer. For å synkronisere filene gjør du følgende:\n\nTrykk Ctrl+Shift C\nSkriv inn Pair i søkefeltet som dukker opp\nVelg Pair Notebook with R Markdown\n\nHvis du nå endrer en av filene så vil den andre oppdatere seg, og renv vil kunne oppdage om du bruker en pakke i koden din. Men for å trigge renv til å lete etter pakker som er i bruk så må du skrive følgende kode i Notebooken eller R Console:\nrenv::snapshot()\nKikker du nå inne i renv.lock-filen så ser du nå at verjsonen av PxWebApiData er lagt til. I bildet under ser du hvordan et arbeidsmiljø typisk kan se ut når man installerer sine egne pakker.\n\nFor å installere alle pakker som ligger i renv.lock-filen med riktig versjon kan du skriver\nrenv:restore()\nDette er nyttig om det er nye medlemmer i gruppen som skal kjøre en produksjonsløp utviklet av andre.\n\n\n\nIndivide pakker kan fjernes fra library ved remove() funksjonen. For eksempel:\nrenv::remove(\"PxWebApiData\")\nFor å fjerne fra renv.lock-filen også må du ta en snapshot() etterpå.\nrenv::snapshot()\nEn annen nyttig funksjon er renv::clean(). Dette fjerner alle pakker fra library som ikke er i bruk\nrenv::clean()\nIgjen må du ta en snapshot() for at endringer skal lagres på renv.lock-filen\n\n\n\nFor å oppgradere en pakke kan du bruke renv::update(). For eksempel å oppgradere PxWebApiData skriv:\nrenv::update(\"PxWebApiData\")\nFor å installere et spesifikk versjon av en pakke kan du spesifisere dette med installering med @ og versjonsnummer. For eksempel å installere PxWEbApiData versjon 0.4.0:\nrenv::install(\"PxWebApiData@0.4.0\")\nHusk å ta en snapshot() etterpå for å lagre endringer til renv.lock-filen. Det betyr at du og andre kan gjenskape miljø på nytt.\nrenv::snapshot()"
  },
  {
    "objectID": "pakke-install.html#python",
    "href": "pakke-install.html#python",
    "title": "Installere pakker",
    "section": "",
    "text": "Installering av pakker er kun er mulig I et virtuelt miljø. Det er anbefalt å benytte poetry til dette. Eksemplene videre tar derfor utgangspunkt i et poetry prosjekt.\nDet er mulig å installere pakker med pip. Pakker kan installeres som normalt, hvis man har satt opp og aktivert et virtuelt miljø.\n\n\nDette eksemplet viser hvordan man setter oppe et enkelt poetry prosjekt kalt test, hvis man ønsker å benytte et annet prosjektnavn må man endre dette i hver av kommandoene.\nSett opp prosjektet:\npoetry new test\nNaviger inn i prosjektmappen:\ncd test\nBruk poetry install for å bygge prosjektet:\npoetry install\nHvis man får en tilbakemelding som denne er prosjektet satt opp korrekt:\nCreating virtualenv test-EojoH6Zm-py3.10 in /home/jovyan/.cache/pypoetry/virtualenvs \nUpdating dependencies \nResolving dependencies... (0.1s) \n\nWriting lock file \n\n\n\nFor å legge til pakker i et prosjekt benyttes kommandoen poetry add.\nSkal man legge til pakken “pendulum” vil det se slik ut:\npoetry add pendulum\nPoetry tilbyr måter å sette versjonsbegrensninger for pakker som legges til i et prosjekt, dette kan man lese mer om her.\n\n\n\nFor å fjerne pakker fra et prosjekt benytter man poetry remove.\nHvis man ønsker å fjerne “pendulum” fra et prosjekt vil kommandoen se slik ut:\npoetry remove pendulum\n\n\n\nFor å oppdatere pakker i et prosjekt benytter man kommandoen poetry update.\nSkal man oppdatere pakken “pendulum” bruker man:\npoetry update pendulum\nSkal man oppdatre alle pakken i et prosjekt benytter man:\npoetry update \n\n\n\nNår man installerer pakker så vil det etter hvert utvikle seg et sett av potensielt kompliserte avhengigheter mellom disse pakkene. Dette skyldes at en pakke kan benytte seg av funksjonalitet i andre pakker, som igjen benytter seg av funksjonalitet i andre pakker, osv.. Hvis noen finner en sikkerhetssårbarhet i en pakke så kan det fikses ved at en ny versjon av den pakken slippes, som igjen kan få konsekvenser for pakker som er avhengig av denne.\nI SSB er det tilrettelagt for at alle som versjonshåndterer koden sin på GitHub kan skanne pakkene sine for sårbarheter og nye versjoner av pakker med Dependabot. Dependabot hjelper oss med å finne og fikse sårbarheter og gamle pakkeversjoner. Dette er spesielt viktig når man installerer sine egne pakker.\nDet er anbefalt at alle som installerer sine egne pakker i SSB skrur på Dependabot i sine GitHub-repoer. Du kan skru på ved å gjøre følgende:\n\nGå inn repoet\nTrykk på Settings for det repoet som vist på Figur 1.\n\n\n\n\nFigur 1: Åpne Settings for et GitHub-repo.\n\n\n\nI menyen til venstre velger du Code security and analysis\nUnder seksjonen Dependabot velger Enable på minst Dependabot alerts og Dependabot security updates, slik som vist i Figur 2.\n\n\n\n\nFigur 2: Skru på Dependabot i GitHub.\n\n\nNår du har gjort dette vil GitHub varsle deg hvis det finnes en kjent sårbarhet i pakkene som benyttes.\n\n\n\nFor å kunne benytte det virtuelle miljøet i en notebook må man sette opp en kernel. Kernel burde gis samme navn som prosjektet.\nFørst legger man til ipykernel:\npoetry add ipykernel\nSå opprettes kernel med:\npoetry run python -m ipykernel install --user --name test\nEtter dette er kernelen test opprettet og kan velges for å benytte miljøet i en notebook.\n\n\n\nFor å fjerne en kernel med navn test bruker man:\njupyter kernelspec remove test\nDu vil bli spurt om å bekrefte, trykk y hvis man ønsker å slette:\nKernel specs to remove:\n  test                    /home/jovyan/.local/share/jupyter/kernels/test\nRemove 1 kernel specs [y/N]: y\nEtter dette er kernelen fjernet.\n\n\n\nHvem som helst kan legge til pakker på PyPi, det betyr at de i verstefall, kan inneholde skadelig kode. Her er en list med viktige tiltak som minimere risikoen:\n\nFør man installerer pakker bør man alltid søke de opp på https://pypi.org. Det er anbefalt å klippe og lime inn pakkenavnet når man skal legge det til i et prosjekt.\nEr det et populært/velkjent prosjekt? Hvor mange stjerner og forks har repoet?"
  },
  {
    "objectID": "pakke-install.html#r",
    "href": "pakke-install.html#r",
    "title": "Installere pakker",
    "section": "",
    "text": "Installering av pakker for R-miljøet i Jupyterlab er foreløpig ikke en del av ssb-project. Men vi kan bruke renv. Mer kommer.\n\n\nFor å installere dine egne R-pakker må du opprette et virtuelt miljø med renv. Gå inn i Jupyterlab og åpne R-notebook. Deretter skriver du inn følgende i kodecelle:\nrenv::init()\nDenne kommandoer aktiverer et virtuelt miljø i mappen du står i. Rent praktisk vil det si at du fikk følgende filer/mapper i mappen din:\nrenv.lock\nEn fil som inneholder versjoner av alle pakker du benytter i koden din.\n.Rprofile En fil som inneholder informasjon om oppsetting av miljø og alternative.\nrenv\nMappe som inneholder alle pakkene du installerer.\nrenv/activate.R En fil som aktivere renv miljø for et prosjekt.\nHvis prosjektet ligger på GitHub, skal filene renv.lock, .Rprofile og renv/activate.R være på GitHub\nNå som vi har et virtuelle miljøet på plass kan vi installere en R-pakke. Du kan gjøre dette fra både terminalen og fra en Notebook. Vi anbefaler på gjøre det fra terminalen fordi du da får tilbakemelding på om installeringen gikk bra heller ikke. For å installere i terminalen gjør du følgende:\n\nÅpne en terminal i Jupyterlab\nStå i mappen der du aktiverte det virtuelle miljøet\nSkriv in R og trykk enter.\n\nDet vi nå har gjort er å åpne R fra terminalen slik at vi kan skrive R-kode direkte i terminalen. Det omtales ofte som en R Console. Nå kan du skrive inn en vanlig kommando for å installere R-pakker:\nrenv::install(\"PxWebApiData\")\nOver installerte vi pakken PxWebApiData fra den R sentral repository CRAN. Dette er en pakke skrevet i SSB for å hente ut data fra vår statistikkbank. Det er også mulig å installere pakker som ligger på SSBs GitHub. Da må vi spesifisere at pakke ligger på ‘statisticsnorway’ område. For eksempel:\nrenv::install(\"statisticsnorway/klassR\")\nPakken klassR er skrevet for å hente ut klassifikasjoner fra SSBs KLASS. Det er en public repository på Github og åpen for alle å laste ned. For pakker som er på et lukket område på ‘statsitcsnorway’ må vi bruke Personal Authentication Token for å installere. Vi kan gjøre dette ved hjelp av funksjonen install_github() i devtools pakken. For eksempel:\nrenv::install(\"devtools\")\nrenv::install(\"getPass\")\ndevtools::install_github(\"statisticsnorway/fellesr\", \n                        auth_token = getPass::getPass())\nLa oss bruke pakken PxWebApiData i koden vår med ved å skrive følgende i kodecelle i Notebooken vår:\nlibrary(PxWebApiData)\nApiData(\"https://data.ssb.no/api/v0/en/table/04861\", \n        Region = c(\"1103\", \"0301\"), ContentsCode = \"Bosatte\", Tid = c(1, 2, -2, -1))\nNår vi nå har brukt PxWebApiData i koden vår så kan vi kjøre en kommando som legger til den pakken i renv.lock. Men før vi kan gjøre det må vi være obs på at renv ikke klarer å gjenkjenne pakker som er i bruk Notebooks (ipynb-filer). Det er veldig upraktisk, men noe vi må forholde oss til når vi jobber med renv i Jupyterlab. En mulig løsning for dette er å bruke Jupytext til å synkronisere en ipynb-fil med en Rmd-fil. renv kjenner igjen både R- og Rmd-filer. For å synkronisere filene gjør du følgende:\n\nTrykk Ctrl+Shift C\nSkriv inn Pair i søkefeltet som dukker opp\nVelg Pair Notebook with R Markdown\n\nHvis du nå endrer en av filene så vil den andre oppdatere seg, og renv vil kunne oppdage om du bruker en pakke i koden din. Men for å trigge renv til å lete etter pakker som er i bruk så må du skrive følgende kode i Notebooken eller R Console:\nrenv::snapshot()\nKikker du nå inne i renv.lock-filen så ser du nå at verjsonen av PxWebApiData er lagt til. I bildet under ser du hvordan et arbeidsmiljø typisk kan se ut når man installerer sine egne pakker.\n\nFor å installere alle pakker som ligger i renv.lock-filen med riktig versjon kan du skriver\nrenv:restore()\nDette er nyttig om det er nye medlemmer i gruppen som skal kjøre en produksjonsløp utviklet av andre.\n\n\n\nIndivide pakker kan fjernes fra library ved remove() funksjonen. For eksempel:\nrenv::remove(\"PxWebApiData\")\nFor å fjerne fra renv.lock-filen også må du ta en snapshot() etterpå.\nrenv::snapshot()\nEn annen nyttig funksjon er renv::clean(). Dette fjerner alle pakker fra library som ikke er i bruk\nrenv::clean()\nIgjen må du ta en snapshot() for at endringer skal lagres på renv.lock-filen\n\n\n\nFor å oppgradere en pakke kan du bruke renv::update(). For eksempel å oppgradere PxWebApiData skriv:\nrenv::update(\"PxWebApiData\")\nFor å installere et spesifikk versjon av en pakke kan du spesifisere dette med installering med @ og versjonsnummer. For eksempel å installere PxWEbApiData versjon 0.4.0:\nrenv::install(\"PxWebApiData@0.4.0\")\nHusk å ta en snapshot() etterpå for å lagre endringer til renv.lock-filen. Det betyr at du og andre kan gjenskape miljø på nytt.\nrenv::snapshot()"
  },
  {
    "objectID": "hva-er-dapla.html",
    "href": "hva-er-dapla.html",
    "title": "Hva er Dapla?",
    "section": "",
    "text": "Hva er Dapla?\nDapla står for dataplattform, og er en skybasert løsning for statistikkproduksjon og forskning.\n\n\nHvorfor Dapla?\nSom dataplattform skal Dapla stimulerere til økt kvalitet på statistikk og forskning, samtidig som den gjør organisasjonen mer tilpasningsdyktig i møte med fremtiden.\n\n\nDen nye skybaserte dataplattformen (Dapla) skal bli viktig for å effektivisere arbeids-og produksjonsprosesser, den skal sikre effektiv lagring og gjenfinning av data og metadata, og støtte opp under deling av data på tvers av statistikkområder.\n\nKilde: Langtidsplan for SSB (2022-2024)\n\n\n\nMålet med Dapla er å tilby tjenester og verktøy som lar statistikkprodusenter og forskere produsere resultater på en sikker og effektiv måte."
  },
  {
    "objectID": "standarder.html",
    "href": "standarder.html",
    "title": "Standarder",
    "section": "",
    "text": "Standarder"
  },
  {
    "objectID": "virtual-env.html",
    "href": "virtual-env.html",
    "title": "Virtuelle miljøer",
    "section": "",
    "text": "Et python virtuelt miljø inneholder en spesifikk versjon av python og et sett med pakker. Pakkene er kun tilgjengelige når det virtuelle miljøet er aktivert. Dette gjør at man ungår avhengighetskonflikter på tvers av prosjekter.\nSe her for mer informasjon om virtuelle miljøer.\n\n\nDet er anbefalt å benytte verktøyet poetry for å administrere prosjekter og deres virtuelle miljø.\nPoetry setter opp virtuelt miljø, gjør det enkelt å oppdatere avhengigheter, sette versjonsbegrensninger og reprodusere prosjektet.\nPoetry gjør dette ved å lagre avhengigheters eksakte versjon i prosjektets “poetry.lock”. Og eventuelle begrensninger i “pyproject.toml”. Dette gjør det enkelt for andre å bygge prosjektet med akkurat de samme pakkene og begrensningene."
  },
  {
    "objectID": "virtual-env.html#python",
    "href": "virtual-env.html#python",
    "title": "Virtuelle miljøer",
    "section": "",
    "text": "Et python virtuelt miljø inneholder en spesifikk versjon av python og et sett med pakker. Pakkene er kun tilgjengelige når det virtuelle miljøet er aktivert. Dette gjør at man ungår avhengighetskonflikter på tvers av prosjekter.\nSe her for mer informasjon om virtuelle miljøer.\n\n\nDet er anbefalt å benytte verktøyet poetry for å administrere prosjekter og deres virtuelle miljø.\nPoetry setter opp virtuelt miljø, gjør det enkelt å oppdatere avhengigheter, sette versjonsbegrensninger og reprodusere prosjektet.\nPoetry gjør dette ved å lagre avhengigheters eksakte versjon i prosjektets “poetry.lock”. Og eventuelle begrensninger i “pyproject.toml”. Dette gjør det enkelt for andre å bygge prosjektet med akkurat de samme pakkene og begrensningene."
  },
  {
    "objectID": "introduksjon.html",
    "href": "introduksjon.html",
    "title": "Introduksjon",
    "section": "",
    "text": "Introduksjon\nMålet med dette kapittelet er å gi en grunnleggende innføring i hva som legges i ordet Dapla. I tillegg gis en forklaring på hvorfor disse valgene er tatt."
  },
  {
    "objectID": "innlogging.html",
    "href": "innlogging.html",
    "title": "Innlogging",
    "section": "",
    "text": "Innlogging på Dapla er veldig enkelt. Dapla er en nettadresse som alle SSB-ere kan gå inn på hvis de er logget på SSB sitt nettverk. Å være logget på SSB sitt nettverk betyr i denne sammenhengen at man er logget på med VPN, enten man er på kontoret eller på hjemmekontor. For å gjøre det enda enklere har vi laget en fast snarvei til denne nettadressen på vårt intranett/Byrånettet(se Figur 1).\n\n\n\nFigur 1: Snarvei til Dapla fra intranett\n\n\nMen samtidig som det er lett å logge seg på, så er det noen kompliserende ting som fortjener en forklaring. Noe skyldes at vi mangler et klart språk for å definere bakkemiljøet og skymiljøet slik at alle skjønner hva man snakker om. I denne boken definerer bakkemiljøet som stedet der man har drevet med statistikkproduksjon de siste tiårene. Skymiljøet er den nye dataplattformen Dapla på Google Cloud.\nDet som gjør ting litt komplisert er at vi har 2 Jupyter-miljøer på både bakke og sky. Årsaken er at vi har ett test- og ett prod-område for hver, og det blir i alt 4 Jupyter-miljøer. Figur 2 viser dette.\n\n\n\nFigur 2: De 4 Jupyter-miljøene i SSB. Et test-miljø og et prod-miljø på bakke og sky/Dapla\n\n\nHver av disse miljøene har sin egen nettadresse og sitt eget bruksområde.\n\n\nI de fleste tilfeller vil en statistikker eller forsker ønske å logge seg inn i prod-miljøet. Det er her man skal kjøre koden sin i et produksjonsløp som skal publiseres eller utvikles. I noen tilfeller hvor man ber om å få tilgjengliggjort en ny tjeneste så vil denne først rulles ut i testområdet som vi kaller staging-området. Årsaken er at vi ønsker å beskytte prod-miljøet fra software som potensielt ødelegger for eksisterende funksjonalitet. Derfor ruller vi ut nye ting i staging først. Av den grunn vil de fleste oppleve å bli bedt om å logge seg inn der for testing en eller annen gang. Under forklarer vi hvordan man går frem for å logge seg på de to ulike miljøene på Dapla.\n\n\nFor å logge seg inn inn i prod-miljøet på Dapla kan man gjøre følgende:\n\nGå inn på lenken https://jupyter.dapla.ssb.no/ i en Chrome-nettleser eller klikk på lenken på Byrånettet som vist i Figur 1.\nAlle i SSB har en Google Cloud-konto som må brukes når man logger seg på Dapla. Brukernavnet i Google er det samme som din korte epost-adresse (f.eks. cth@ssb.no). Hvis du ikke allerede er logget inn i Google vil du få spørsmål om å velge hvilken Google-konto som skal brukes (Figur 3). Logg inn med din Google-konto (ssb.no) og ditt AD-passord.\n\n\n\n\nFigur 3: Velg en Google-konto\n\n\n\nDeretter blir man spurt om man godtar at ssb.no (altså Dapla) kan bruke din Google Cloud-konto (Figur 4). Trykk Allow.\n\n\n\n\nFigur 4: Tillat at ssb.no får bruke din Google Cloud-konto\n\n\n\nDeretter lander man på en side som lar deg avgjøre hvor mye maskinkraft som skal holdes av til deg (Figur 5). Det øverste alternativet er valgt som standard, og er tilstrekkelig for de fleste.\n\n\n\n\nFigur 5: Velg hvor mye maskinkraft du trenger\n\n\n\nVent til maskinen din starter opp (Figur 6). Oppstartstiden kan variere.\n\n\n\n\nFigur 6: Starter opp Jupyter\n\n\nEtter dette er man logget inn i et Jupyter-miljø som kjører på en minimal Ubuntu-maskin. Hvis man er del av et Dapla-team får man også tilgang til alt teamet har tilgang til.\n\n\n\nInnlogging til staging-miljøet er identisk med innloggingen til prod-miljøet, med ett viktig unntak: nettadressen er nå https://jupyter.dapla-staging.ssb.no/.\nLitt mer om hva som er tilgjenglig her kommer.\n\n\n\n\nJupyter-miljøet på bakken bruker samme base-image1 for å installere Jupyterlab, og er derfor identisk på mange måter. Men innloggingen er ganske forskjellig.\n\n\n\n\n\n\nNote\n\n\n\nFom. 5. desember 2022 har vi byttet ut Jupyter-miljøet på bakken. Beskrivelsene under gjelder derfor det nye miljøet. Fram til 15. januar vil du kunne bruke det gamle miljøet ved å gå inn på lenken https://jupyter-prod.ssb.no/ manuelt i Google Chrome. Etter 15. januar blir det gamle Jupyter-miljøet avviklet.\n\n\n\n\nDu logger deg inn på prod i bakkemiljøet på følgende måte:\n\nLogg deg inn på Citrix-Windows i bakkemiljøet. Det kan gjøres ved å bruke lenken Citrix på Byrånettet, som også vises i Figur 1.\nTrykk på Jupyterlab-ikonet, som vist på Figur 7, og logg deg inn med vanlig brukernavn og passord.\n\n\n\n\nFigur 7: Jupyterlab-ikon på Skrivebordet i Citrix-Windows.\n\n\nNår du trykker på ikonet blir du tatt til nettadressen https://sl-jupyter-p.ssb.no/. Du kunne også åpnet Jupyterlab ved åpne Chrome-nettleseren og skrive inn adressen manuelt.\n\n\n\nInnlogging til staging-miljøet har ingen snarvei på Skrivebordet, og du må gjøre følgende for å åpne miljøet:\n\nÅpne Chrome-nettleseren i Citrix-Windows.\nSkriv inn url-en https://sl-jupyter-t.ssb.no/"
  },
  {
    "objectID": "innlogging.html#dapla",
    "href": "innlogging.html#dapla",
    "title": "Innlogging",
    "section": "",
    "text": "I de fleste tilfeller vil en statistikker eller forsker ønske å logge seg inn i prod-miljøet. Det er her man skal kjøre koden sin i et produksjonsløp som skal publiseres eller utvikles. I noen tilfeller hvor man ber om å få tilgjengliggjort en ny tjeneste så vil denne først rulles ut i testområdet som vi kaller staging-området. Årsaken er at vi ønsker å beskytte prod-miljøet fra software som potensielt ødelegger for eksisterende funksjonalitet. Derfor ruller vi ut nye ting i staging først. Av den grunn vil de fleste oppleve å bli bedt om å logge seg inn der for testing en eller annen gang. Under forklarer vi hvordan man går frem for å logge seg på de to ulike miljøene på Dapla.\n\n\nFor å logge seg inn inn i prod-miljøet på Dapla kan man gjøre følgende:\n\nGå inn på lenken https://jupyter.dapla.ssb.no/ i en Chrome-nettleser eller klikk på lenken på Byrånettet som vist i Figur 1.\nAlle i SSB har en Google Cloud-konto som må brukes når man logger seg på Dapla. Brukernavnet i Google er det samme som din korte epost-adresse (f.eks. cth@ssb.no). Hvis du ikke allerede er logget inn i Google vil du få spørsmål om å velge hvilken Google-konto som skal brukes (Figur 3). Logg inn med din Google-konto (ssb.no) og ditt AD-passord.\n\n\n\n\nFigur 3: Velg en Google-konto\n\n\n\nDeretter blir man spurt om man godtar at ssb.no (altså Dapla) kan bruke din Google Cloud-konto (Figur 4). Trykk Allow.\n\n\n\n\nFigur 4: Tillat at ssb.no får bruke din Google Cloud-konto\n\n\n\nDeretter lander man på en side som lar deg avgjøre hvor mye maskinkraft som skal holdes av til deg (Figur 5). Det øverste alternativet er valgt som standard, og er tilstrekkelig for de fleste.\n\n\n\n\nFigur 5: Velg hvor mye maskinkraft du trenger\n\n\n\nVent til maskinen din starter opp (Figur 6). Oppstartstiden kan variere.\n\n\n\n\nFigur 6: Starter opp Jupyter\n\n\nEtter dette er man logget inn i et Jupyter-miljø som kjører på en minimal Ubuntu-maskin. Hvis man er del av et Dapla-team får man også tilgang til alt teamet har tilgang til.\n\n\n\nInnlogging til staging-miljøet er identisk med innloggingen til prod-miljøet, med ett viktig unntak: nettadressen er nå https://jupyter.dapla-staging.ssb.no/.\nLitt mer om hva som er tilgjenglig her kommer."
  },
  {
    "objectID": "innlogging.html#bakkemiljøet",
    "href": "innlogging.html#bakkemiljøet",
    "title": "Innlogging",
    "section": "",
    "text": "Jupyter-miljøet på bakken bruker samme base-image1 for å installere Jupyterlab, og er derfor identisk på mange måter. Men innloggingen er ganske forskjellig.\n\n\n\n\n\n\nNote\n\n\n\nFom. 5. desember 2022 har vi byttet ut Jupyter-miljøet på bakken. Beskrivelsene under gjelder derfor det nye miljøet. Fram til 15. januar vil du kunne bruke det gamle miljøet ved å gå inn på lenken https://jupyter-prod.ssb.no/ manuelt i Google Chrome. Etter 15. januar blir det gamle Jupyter-miljøet avviklet.\n\n\n\n\nDu logger deg inn på prod i bakkemiljøet på følgende måte:\n\nLogg deg inn på Citrix-Windows i bakkemiljøet. Det kan gjøres ved å bruke lenken Citrix på Byrånettet, som også vises i Figur 1.\nTrykk på Jupyterlab-ikonet, som vist på Figur 7, og logg deg inn med vanlig brukernavn og passord.\n\n\n\n\nFigur 7: Jupyterlab-ikon på Skrivebordet i Citrix-Windows.\n\n\nNår du trykker på ikonet blir du tatt til nettadressen https://sl-jupyter-p.ssb.no/. Du kunne også åpnet Jupyterlab ved åpne Chrome-nettleseren og skrive inn adressen manuelt.\n\n\n\nInnlogging til staging-miljøet har ingen snarvei på Skrivebordet, og du må gjøre følgende for å åpne miljøet:\n\nÅpne Chrome-nettleseren i Citrix-Windows.\nSkriv inn url-en https://sl-jupyter-t.ssb.no/"
  },
  {
    "objectID": "innlogging.html#footnotes",
    "href": "innlogging.html#footnotes",
    "title": "Innlogging",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nHva er base-image?↩︎"
  },
  {
    "objectID": "visualisering.html",
    "href": "visualisering.html",
    "title": "Visualisering",
    "section": "",
    "text": "Visualisering"
  },
  {
    "objectID": "ordforklaringer.html",
    "href": "ordforklaringer.html",
    "title": "Ordforklaringer",
    "section": "",
    "text": "Ordforklaringer\n\nbip\nbip er det tidligere navnet på den underliggende plattformen som SSB bygger i GCP, hovedsakelig ment for utviklere som bygger tjenester på Dapla. Plattformen skulle være selvbetjent for utviklere og basert på DevOps-prinsipper. bip eksisterer fortsatt, men er nå blitt en del av det større begrepet dapla.\n\n\nbucket\nbucket (eller bøtte på norsk) er en lagringsenhet på Dapla. Det ligner litt på en klassisk diskstasjon, for eksempel X-disken eller C-disken på en lokal maskin. I en bøtte kan det ligge undermapper slik som i et klassisk filsystem.\n\n\nconsumer\nconsumer er en AD-gruppe som gir tilgang til et Dapla-team sin delt-bøtte. En SSB-ansatt som skal bruke data fra et Dapla-team må være medlem av consumer-gruppen til det aktuelle Dapla-teamet.\n\n\ndapla\nDapla er et akronym for den nye dataplattformen til SSB, der Da står for Data og pla står for Plattform. Dapla er en plattform for lagring, prosessering og deling av SSB sine data. Den består både av Jupyter-miljøet, som er et verktøy for å utføre beregninger og analysere data, og et eget område for lagre data. I tillegg inkluderer begrepet Dapla også en rekke andre verktøy som er nødvendige for å kunne bruke plattformen.\n\n\ndapla-team\nKommer snart.\n\n\ndapla-toolbelt\nKommer snart.\n\n\ndata-admin\ndata-admin er en AD-gruppe som gir de videste tilgangene i et dapla-team. En SSB-ansatt som har data-admin-rollen i et Dapla-team har tilgang til alle bøtter for det teamet, inkludert kilde-bøtta som kan inneha sensitive data.\nKommer snart.\n\n\ndapla-start\n*dapla-start** er et brukergrensesnitt der SSB-ansatte kan søke om å få opprettet et nytt dapla-team.\n\n\ndelt-bøtte\nKommer snart.\n\n\ndeveloper\nKommer snart.\n\n\nPersonidentifiserende Informasjon (PII)\nPII er variabler som kan identifisere en person i et datasett.\nMer informasjon finnes hos Datatilsynet.\n\n\ngoogle cloud platform (gcp)\nAllmenn skyplattform utviklet og levert av Google. Konkurrent med Amazon Web Services (AWS) og Microsoft Azure. Dapla primært benytter seg av tjenester på GCP.\nVideo som forklarer hva GCP er.\n\n\ngcp\nForkortelse for Google Cloud Platform. Se forklaring under google cloud platform (GCP).\n\n\nInfrastructure as Code (IaC)\nInfrastuktur som kode på norsk. Kode som defineres ressurser, typisk på en allmenn skyplatform som GCP. Eksempler av ressurser er bøtter, databaser, virtuelle maskiner, nettverk og sikkerhetsregler.\n\n\nkilde-bøtte\nKommer snart.\n\n\nprodukt-bøtte\nKommer snart.\n\n\nPull Request (PR)\nEn PR er en Github konsept, som gir et forum for kodegjennomgang, diskusjon og ikke minst dokumentasjon av kodeendringer.\nDette er anbefalt av KVAKK som måten å endre kode på i SSB.\n\n\nssb-project\nKommer snart.\n\n\ntransfer service\nKommer snart.\n\n\nPyflakes\nPyflakes er et enkelt kodeanalyseverktøy som finner feil i Python kode. Les mer om Pyflakes på deres PyPi side"
  },
  {
    "objectID": "koble-data.html",
    "href": "koble-data.html",
    "title": "Koble data",
    "section": "",
    "text": "Koble data"
  },
  {
    "objectID": "datadoc.html",
    "href": "datadoc.html",
    "title": "DataDoc",
    "section": "",
    "text": "For å kunne gjenfinne data i SSB er man helt avhengig av at det finnes et enhetlig system for metadata knyttet til dataene. DataDoc er SSBs system for å dokumentere datasett på den nye dataplattformen Dapla.\nDet er bygget et grensesnitt i Python for å gjøre det enklest mulig å dokumentere et datasett. Foreløpig støtter løsningen følgende filformater:\n\nparquet\nsas7bdat\n\nUnder finner du beskrivelse av hvordan du kan begynne å bruke løsningen til å dokumentere datasett.\n\n\n\n\n\n\nWarning\n\n\n\nVi ønsker at du skal teste DataDoc-applikasjonen. Den viktigste funksjonaliteten skal være tilgjengelig, og det er fullt mulig å benytte DataDoc i SSBs Jupyter-miljøer. Det er imidlertid viktig å være klar over at applikasjonen fortsatt er i en utviklings- og testfase (beta-løsning) og kan inneholde feil og mangler.\nHar du spørsmål, eventuelt vil rapporterer om feil og mangler, så setter vi pris på om du gjør dette i Yammer-gruppa Dapla.\n\n\n\n\nFør du tar i bruk DataDoc-applikasjonen er det viktig å forstå hvilken informasjon som skal til for å dokumentere et datasett. I DataDoc-applikasjonen skal du fylle ut flere felter om både datasettet og variablene som inngår i datasettet, eksempelvis\n\nkortnavn\nnavn\ndatatilstand\npopulasjonsbeskrivelse\n++\n\nDet er utarbeidet en detaljert beskrivelse hva hvert felt betyr, og hvordan de skal fylles ut både for datasett og variabler: - DataDoc - hvordan dokumentere et datasett - DataDoc - hvordan dokumentere variablene (variabelforekomstene) som inngår i datasettet\nDataDoc skal være installert i alle Jupyter-miljøene i SSB, så du trenger ikke installere pakken selv.\n\n\n\n\n\n\nViktig informasjon\n\n\n\nDataDoc kan foreløpig ikke kjøres i Jupyter notebook med virtuelle miljøer (f.eks. et ssb-project), men må startes i den vanlige kernelen i en notebook.\n\n\n\n\n\nLa oss lage et test-datasett slik at vi kan leke oss litt med DataDoc:\nimport pandas as pd\nfrom datadoc import main\n  \n# Create fake data\ndata = {'id': ['9999999999', '8888888888', '7777777777', '6666666666'],\n        'fylke': [\"01\", \"02\", \"03\", \"03\"],\n        'inntekt': [500000, 250000, 400000, 440000],\n        'rente': [3.2, 4.1, 3.3, 3.4]}\n  \n# Creates a Pandas dataframe\ndf = pd.DataFrame(data)\n\n# Write a Parquet-file to current folder\ndf.to_parquet(\"./test.parquet\")\nNå har vi en fil som heter test.parquet i mappen vi står. Da kan vi åpne DataDoc-grensesnittet for å legge inn metadataene:\nmain(\"./test.parquet\")\nFigur 1 viser hvordan DataDoc-grensesnittet ser ut.\n\n\n\nFigur 1: Gif som viser hvordan DataDoc-grensesnittet ser ut.\n\n\n\n\n\nNår du trykker på Lagre-knappen i DataDoc så skrives alle metadata til en fil i samme mappe (katalog) som datafilen. Dette er en JSON-fil med nesten samme navn som datafilen. Navnekonvensjonen for metadatafilen er\n&lt;navn på datafil uten endelse&gt;__DOC.json\nEksempelvis hvis datafilen har navnet skattedata_p2022_v1.parquet, så vil DataDoc lagre metadata i filen skattedata_p2022_v1__DOC.json.\nFordelen med å benytte en JSON-fil til å lagre metadata er at denne filen kan kopieres og flyttes like enkelt som selve datafilen. JSON-filer er strukturerte tekstfiler som kan leses av både maskiner (Python/R) og av mennesker (åpnes i en tekst-editor).\nSe et eksempel på JSON metadata-fil lagret av DataDoc.\n\n\n\n\n\n\nInformasjon\n\n\n\nI Dapla skal det bygges en felles datakatalog for SSB. Tanken er at alle metadata, eksempelvis datasett-dokumentasjon fra DataDoc (JSON-filene), skal inngå i SSBs datakatalog. Datakatalogen gjør det mulig å finne (søke etter), forstår og gjenbruke data både internt og ekstern."
  },
  {
    "objectID": "datadoc.html#hvordan-dokumentere-datasett-og-variabler-med-datadoc",
    "href": "datadoc.html#hvordan-dokumentere-datasett-og-variabler-med-datadoc",
    "title": "DataDoc",
    "section": "",
    "text": "Før du tar i bruk DataDoc-applikasjonen er det viktig å forstå hvilken informasjon som skal til for å dokumentere et datasett. I DataDoc-applikasjonen skal du fylle ut flere felter om både datasettet og variablene som inngår i datasettet, eksempelvis\n\nkortnavn\nnavn\ndatatilstand\npopulasjonsbeskrivelse\n++\n\nDet er utarbeidet en detaljert beskrivelse hva hvert felt betyr, og hvordan de skal fylles ut både for datasett og variabler: - DataDoc - hvordan dokumentere et datasett - DataDoc - hvordan dokumentere variablene (variabelforekomstene) som inngår i datasettet\nDataDoc skal være installert i alle Jupyter-miljøene i SSB, så du trenger ikke installere pakken selv.\n\n\n\n\n\n\nViktig informasjon\n\n\n\nDataDoc kan foreløpig ikke kjøres i Jupyter notebook med virtuelle miljøer (f.eks. et ssb-project), men må startes i den vanlige kernelen i en notebook."
  },
  {
    "objectID": "datadoc.html#prøve-datadoc",
    "href": "datadoc.html#prøve-datadoc",
    "title": "DataDoc",
    "section": "",
    "text": "La oss lage et test-datasett slik at vi kan leke oss litt med DataDoc:\nimport pandas as pd\nfrom datadoc import main\n  \n# Create fake data\ndata = {'id': ['9999999999', '8888888888', '7777777777', '6666666666'],\n        'fylke': [\"01\", \"02\", \"03\", \"03\"],\n        'inntekt': [500000, 250000, 400000, 440000],\n        'rente': [3.2, 4.1, 3.3, 3.4]}\n  \n# Creates a Pandas dataframe\ndf = pd.DataFrame(data)\n\n# Write a Parquet-file to current folder\ndf.to_parquet(\"./test.parquet\")\nNå har vi en fil som heter test.parquet i mappen vi står. Da kan vi åpne DataDoc-grensesnittet for å legge inn metadataene:\nmain(\"./test.parquet\")\nFigur 1 viser hvordan DataDoc-grensesnittet ser ut.\n\n\n\nFigur 1: Gif som viser hvordan DataDoc-grensesnittet ser ut."
  },
  {
    "objectID": "datadoc.html#hvor-lagres-datadoc-dokumentasjonen-metadata",
    "href": "datadoc.html#hvor-lagres-datadoc-dokumentasjonen-metadata",
    "title": "DataDoc",
    "section": "",
    "text": "Når du trykker på Lagre-knappen i DataDoc så skrives alle metadata til en fil i samme mappe (katalog) som datafilen. Dette er en JSON-fil med nesten samme navn som datafilen. Navnekonvensjonen for metadatafilen er\n&lt;navn på datafil uten endelse&gt;__DOC.json\nEksempelvis hvis datafilen har navnet skattedata_p2022_v1.parquet, så vil DataDoc lagre metadata i filen skattedata_p2022_v1__DOC.json.\nFordelen med å benytte en JSON-fil til å lagre metadata er at denne filen kan kopieres og flyttes like enkelt som selve datafilen. JSON-filer er strukturerte tekstfiler som kan leses av både maskiner (Python/R) og av mennesker (åpnes i en tekst-editor).\nSe et eksempel på JSON metadata-fil lagret av DataDoc.\n\n\n\n\n\n\nInformasjon\n\n\n\nI Dapla skal det bygges en felles datakatalog for SSB. Tanken er at alle metadata, eksempelvis datasett-dokumentasjon fra DataDoc (JSON-filene), skal inngå i SSBs datakatalog. Datakatalogen gjør det mulig å finne (søke etter), forstår og gjenbruke data både internt og ekstern."
  },
  {
    "objectID": "annet.html",
    "href": "annet.html",
    "title": "Annet",
    "section": "",
    "text": "Annet"
  },
  {
    "objectID": "automatisering-avansert.html",
    "href": "automatisering-avansert.html",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Figur 1: Infrastruktur\n\n\n\n\n\nSom utgangspunkt, får hver prosesseringsinstans standard ressurstildeling for Cloud Run, dvs. 1 CPU kjerne og 512MB minne. Dette kan være for lite i noen tilfeller, særlig for større kildedatafiler. Hvis det oppleves at mer minne kreves, ta gjerne kontakt med Dapla Kundeservice for å ordne dette.\n\n\n\nEn liste over Python pakker man kan benytte seg av i process_source_data.py finnes her. Ta gjerne kontakt med Dapla Kundeservice hvis man har behov for ytterligere.\n\n\n\nHver kilde kan skalere opp med parallele prosesseringsinstanser. Det gjør det kjappere å prosessere mange filer. I utgangspunktet er dette begrenset til 5 parallele instanser, men det kan økes ved behov.\n\n\n\nÅ benytte både prod og staging miljøer er en god praksis for å sikre at nye funksjoner og endringer fungerer som forventet før de rulles ut i produksjon. Staging-miljøet gir mulighet til å validere endringer i en mer kontrollert og isolert setting før de blir lansert i produksjonsmiljøet med skarpe data.\nAutomatiseringsløsningen støtter oppsett av kilder i både staging og prod miljø. For oppsett av kilder i staging-miljøet, må man legge til kilden i en egen undermappe kalt staging. Dette kan gjøres på samme måte som beskrevet her, bortsett fra at kilden legges i en undermappe som heter staging.\nHer er et eksempel på konfigurasjon av både staging og produksjonsmiljø:\n...\n├── automation\n│   └── source_data\n│       ├── prod\n│       │   └── kilde1\n│       │       ├── config.yaml\n│       │       └── process_source_data.py\n│       └── staging\n│           └── kilde1\n│               ├── config.yaml\n│               └── process_source_data.py\n...\n\n\n\n\n\n\nInnholdet i config.yaml for både staging og produksjonsmiljø er som hovedregel identisk, men bøttene som benyttes vil være forskjellige. Stagingmiljøet har en egen kildedatabøtte kalt gs://ssb-staging-my-project-data-kilde.\n\n\n\n\n\n\nFor å gi raskt tilbakemelding på noen mulige feilsituasjoner, så kjøres det enkel validering på kilde config og process_source_data.py når en PR er opprettet.\nHvis valideringen feiler, så må feilen rettes før PRen merges.\nTestene feiler hvis:\n\nKildemappen ikke har et python script kalt process_source_data.py med metodesignaturen, som beskrevet her.\nKildemappen ikke har en yaml fil og en gyldig folder_prefix definert, som i dette eksempelet.\nPython scriptet ikke kan importeres av tjenesten. Tjenesten støtter kun disse tredjeparts pakkene.\nHvis Pyflakes finner feil med kildens Python script.\n\n\n\nHvis validerings testene feiler kan det være nyttig å se på loggene for å finne frem til feilen.\n\nFinn frem til testen som feiler, i bildet feiler valideringstestene for kilde1. Trykk så på lenken “Details” som vist i bilde under. \nPå siden du nå har kommet til skal det være en tabell som heter “Build Information”, trykk på lenken i Build kolonnen. \nDu har nå kommet frem til loggene, se etter indikasjoner på feil. I eksemplet under ser vi at testen test_main_accepts_expected_number_of_args feiler fordi process_source_data.py mangler en main funksjon. \nFiks feilen og push endingen til samme branch, testen vil da starte på nytt.\n\n\n\n\n\nEndringer til process_source_data.py blir automatisk rullet ut når en PR er merget til main branchen. Utrullingsprosessen tar noe tid, ca. 2-3 minutter fra branchen er merget til tjenesten er oppdatert, for å bekrefte at tjenesten er rullet ut kan du følge stegene i neste avsnitt.\n\n\nStegene under viser hvordan man går frem for å finne resultat av utrullingen av kilden “ledstill” for teamet “arbmark-skjema”. Og forutsetter at koden er pushet til main branchen.\n\nNaviger til GitHub.\nI søkefeltet oppe i venstre hjørne skriv arbmark-skjema og klikk “Jump to” arbmark-skjema-iac. Som i bilde under. \nNår utrullingen er ferdig vil en av disse ikonene vises, grønn hake betyr at tjeneste er rullet ut med koden som ligger i main og at nye filer blir behandlet med koden som ligger der. . Rødt kryss indikerer at utrullig har feilet.  Se etter symbolene der hvor den røde pilen i bilde under peker. I eksempel er utrulligen vellykket. \n\n\n\n\n\nMan får en oversikt over kildene man har konfigurert prosessering for og statusen på dem ved hjelp av konsollet på GCP. Der navigerer man til siden for Cloud Run (se Figur 2) som er kjøremiljøet som kildedata prosessering benyttes av. Eksempel URl er: https://console.cloud.google.com/run?project=&lt;teamets-prosjekt-id&gt;\nHer får man en oversikt av ressursbruk og loggene til prosesseringen.\n\n\n\nFigur 2: Cloud Run dashboard\n\n\n\n\nEtter du har valgt kilden kan du se logger ved å velge fanen “LOGS”. Her ligger alle logger for den spesifikke kilden. For å få bedre oversikt over eventulle feil kan man sette severity til error. Dette vil uten ekstra konfigurasjon gi oversikt over alle uhåndterte exceptions. \n\n\n\nHvis en fil blir mottatt av tjenesten, men ikke lar seg behandle blir det skrevet til loggen. Man kan få en oversik over hvilke filer som ikke har blitt prosessert ved å søke etter: Could not process object. \n\n\n\n\nNoen ganger vil det være nødvendig å trigge kjøring av en kilde uten at de tilhørende filene i kildebøtta er oppdatert f.eks. etter en endring i prosseseringsskriptet. For å gjøre dette kan man benytte seg av dapla toolbelt.\nFor å trigge en ny kjøring må man være data-admin i teamet og ha denne informasjonen tilgjengelig:\n\nproject_id(prosjekt id) for kilden, den finner man ved å følge beskrivelsen her.\nfolder_prefix beskriver stien til filene som skal behandles og fungerer på samme måte som i config.yaml\nsource_name finner man ved å se på navnet til mappen hvor kilden konfigureres, i eksempelet her ser vi at team smaabakst har to kilder boller og rundstykker.\n\n\n\nDette eksemplet viser hvordan man går frem for å manuelt trigge kilden boller for team smaabakst.\nTeam smaabakst ønsker å re-prosessere alle filer i kilden boller. Ved å bruke samme folder_prefix som i config.yaml vil alle filer som tilhører kilden bli prosessert på nytt.\nfrom dapla import trigger_source_data_processing\n\nproject_id = \"prod-smaabakst-b69d\"\nsource_name = \"boller\"\nfolder_prefix = \"boller\"\n\ntrigger_source_data_processing(project_id, source_name, folder_prefix)"
  },
  {
    "objectID": "automatisering-avansert.html#infrastruktur-oversikt",
    "href": "automatisering-avansert.html#infrastruktur-oversikt",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Figur 1: Infrastruktur"
  },
  {
    "objectID": "automatisering-avansert.html#ressurser",
    "href": "automatisering-avansert.html#ressurser",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Som utgangspunkt, får hver prosesseringsinstans standard ressurstildeling for Cloud Run, dvs. 1 CPU kjerne og 512MB minne. Dette kan være for lite i noen tilfeller, særlig for større kildedatafiler. Hvis det oppleves at mer minne kreves, ta gjerne kontakt med Dapla Kundeservice for å ordne dette."
  },
  {
    "objectID": "automatisering-avansert.html#tilgjengelige-pakker",
    "href": "automatisering-avansert.html#tilgjengelige-pakker",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "En liste over Python pakker man kan benytte seg av i process_source_data.py finnes her. Ta gjerne kontakt med Dapla Kundeservice hvis man har behov for ytterligere."
  },
  {
    "objectID": "automatisering-avansert.html#skalering",
    "href": "automatisering-avansert.html#skalering",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Hver kilde kan skalere opp med parallele prosesseringsinstanser. Det gjør det kjappere å prosessere mange filer. I utgangspunktet er dette begrenset til 5 parallele instanser, men det kan økes ved behov."
  },
  {
    "objectID": "automatisering-avansert.html#testing-i-staging-miljø",
    "href": "automatisering-avansert.html#testing-i-staging-miljø",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Å benytte både prod og staging miljøer er en god praksis for å sikre at nye funksjoner og endringer fungerer som forventet før de rulles ut i produksjon. Staging-miljøet gir mulighet til å validere endringer i en mer kontrollert og isolert setting før de blir lansert i produksjonsmiljøet med skarpe data.\nAutomatiseringsløsningen støtter oppsett av kilder i både staging og prod miljø. For oppsett av kilder i staging-miljøet, må man legge til kilden i en egen undermappe kalt staging. Dette kan gjøres på samme måte som beskrevet her, bortsett fra at kilden legges i en undermappe som heter staging.\nHer er et eksempel på konfigurasjon av både staging og produksjonsmiljø:\n...\n├── automation\n│   └── source_data\n│       ├── prod\n│       │   └── kilde1\n│       │       ├── config.yaml\n│       │       └── process_source_data.py\n│       └── staging\n│           └── kilde1\n│               ├── config.yaml\n│               └── process_source_data.py\n...\n\n\n\n\n\n\nInnholdet i config.yaml for både staging og produksjonsmiljø er som hovedregel identisk, men bøttene som benyttes vil være forskjellige. Stagingmiljøet har en egen kildedatabøtte kalt gs://ssb-staging-my-project-data-kilde."
  },
  {
    "objectID": "automatisering-avansert.html#validering",
    "href": "automatisering-avansert.html#validering",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "For å gi raskt tilbakemelding på noen mulige feilsituasjoner, så kjøres det enkel validering på kilde config og process_source_data.py når en PR er opprettet.\nHvis valideringen feiler, så må feilen rettes før PRen merges.\nTestene feiler hvis:\n\nKildemappen ikke har et python script kalt process_source_data.py med metodesignaturen, som beskrevet her.\nKildemappen ikke har en yaml fil og en gyldig folder_prefix definert, som i dette eksempelet.\nPython scriptet ikke kan importeres av tjenesten. Tjenesten støtter kun disse tredjeparts pakkene.\nHvis Pyflakes finner feil med kildens Python script.\n\n\n\nHvis validerings testene feiler kan det være nyttig å se på loggene for å finne frem til feilen.\n\nFinn frem til testen som feiler, i bildet feiler valideringstestene for kilde1. Trykk så på lenken “Details” som vist i bilde under. \nPå siden du nå har kommet til skal det være en tabell som heter “Build Information”, trykk på lenken i Build kolonnen. \nDu har nå kommet frem til loggene, se etter indikasjoner på feil. I eksemplet under ser vi at testen test_main_accepts_expected_number_of_args feiler fordi process_source_data.py mangler en main funksjon. \nFiks feilen og push endingen til samme branch, testen vil da starte på nytt."
  },
  {
    "objectID": "automatisering-avansert.html#utrulling",
    "href": "automatisering-avansert.html#utrulling",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Endringer til process_source_data.py blir automatisk rullet ut når en PR er merget til main branchen. Utrullingsprosessen tar noe tid, ca. 2-3 minutter fra branchen er merget til tjenesten er oppdatert, for å bekrefte at tjenesten er rullet ut kan du følge stegene i neste avsnitt.\n\n\nStegene under viser hvordan man går frem for å finne resultat av utrullingen av kilden “ledstill” for teamet “arbmark-skjema”. Og forutsetter at koden er pushet til main branchen.\n\nNaviger til GitHub.\nI søkefeltet oppe i venstre hjørne skriv arbmark-skjema og klikk “Jump to” arbmark-skjema-iac. Som i bilde under. \nNår utrullingen er ferdig vil en av disse ikonene vises, grønn hake betyr at tjeneste er rullet ut med koden som ligger i main og at nye filer blir behandlet med koden som ligger der. . Rødt kryss indikerer at utrullig har feilet.  Se etter symbolene der hvor den røde pilen i bilde under peker. I eksempel er utrulligen vellykket."
  },
  {
    "objectID": "automatisering-avansert.html#monitorering-av-tjenesten",
    "href": "automatisering-avansert.html#monitorering-av-tjenesten",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Man får en oversikt over kildene man har konfigurert prosessering for og statusen på dem ved hjelp av konsollet på GCP. Der navigerer man til siden for Cloud Run (se Figur 2) som er kjøremiljøet som kildedata prosessering benyttes av. Eksempel URl er: https://console.cloud.google.com/run?project=&lt;teamets-prosjekt-id&gt;\nHer får man en oversikt av ressursbruk og loggene til prosesseringen.\n\n\n\nFigur 2: Cloud Run dashboard\n\n\n\n\nEtter du har valgt kilden kan du se logger ved å velge fanen “LOGS”. Her ligger alle logger for den spesifikke kilden. For å få bedre oversikt over eventulle feil kan man sette severity til error. Dette vil uten ekstra konfigurasjon gi oversikt over alle uhåndterte exceptions. \n\n\n\nHvis en fil blir mottatt av tjenesten, men ikke lar seg behandle blir det skrevet til loggen. Man kan få en oversik over hvilke filer som ikke har blitt prosessert ved å søke etter: Could not process object."
  },
  {
    "objectID": "automatisering-avansert.html#trigge-tjenesten-manuelt",
    "href": "automatisering-avansert.html#trigge-tjenesten-manuelt",
    "title": "Automatisering Avansert",
    "section": "",
    "text": "Noen ganger vil det være nødvendig å trigge kjøring av en kilde uten at de tilhørende filene i kildebøtta er oppdatert f.eks. etter en endring i prosseseringsskriptet. For å gjøre dette kan man benytte seg av dapla toolbelt.\nFor å trigge en ny kjøring må man være data-admin i teamet og ha denne informasjonen tilgjengelig:\n\nproject_id(prosjekt id) for kilden, den finner man ved å følge beskrivelsen her.\nfolder_prefix beskriver stien til filene som skal behandles og fungerer på samme måte som i config.yaml\nsource_name finner man ved å se på navnet til mappen hvor kilden konfigureres, i eksempelet her ser vi at team smaabakst har to kilder boller og rundstykker.\n\n\n\nDette eksemplet viser hvordan man går frem for å manuelt trigge kilden boller for team smaabakst.\nTeam smaabakst ønsker å re-prosessere alle filer i kilden boller. Ved å bruke samme folder_prefix som i config.yaml vil alle filer som tilhører kilden bli prosessert på nytt.\nfrom dapla import trigger_source_data_processing\n\nproject_id = \"prod-smaabakst-b69d\"\nsource_name = \"boller\"\nfolder_prefix = \"boller\"\n\ntrigger_source_data_processing(project_id, source_name, folder_prefix)"
  },
  {
    "objectID": "kildedata-prosessering.html",
    "href": "kildedata-prosessering.html",
    "title": "Kildedata prosessering",
    "section": "",
    "text": "Denne tjenesten er under utvikling og kan ikke anses som klar for produksjon.\n\n\n\nFor å minske aksessering av PII1, oppfordres alle team på Dapla å benytte seg av automatisering av kildedata prosessering. Automatisering av kildedata er en tjeneste som er tilgjengelig for team å ta i bruk 100% selv-betjent. Kildedata (Standardutvalget 2021, 5) prosesseres til inndata gjennom et bestemt utvalg av operasjoner. Kildedata prosesseres som individuelle filer for å holde oppsettet enkelt og målrettet mot de definerte operasjoner. Mer kompleks operasjoner som går på tvers av flere filer burde utføres på inndata eller senere datatilstander.\n\n\n\n\n\n\nDet er kun teamets kildedataansvarlige som skal aksessere kildedata.\n\n\n\n\n\n\n\n\n\nTeamets kildedataansvarlige tar ansvar for å prosessere kildedata til inndata på en forsvarlig måte.\n\n\n\n\n\n\n\n\nFigur 1: Operasjoner som inngår i kildedata prosessering\n\n\n\nInndata er kildedata som er transformert til SSBs standard lagringsformat. Variabelnavn og -innhold er uendret bortsett fra at\n\ndirekte identifiserende variabler (f.eks. fødselsnummer) er pseudonymisert\ntegnsett, datoformat, adresse mm er endret til SSBs standardformat\ndet benyttes standard kodeverk (Klass) der det er mulig (f.eks. kjønn)\ndataene er minimert slik at kun variablene som er nødvendige i den videre produksjonsprosessen, inngår.\n\n\n(Standardutvalget 2021, 8)\nDet er ikke anbefalt å gjennomføre operasjoner som:\n\nGår på tvers av flere filer\nLegge til nye felt\nEndre navn på felt\nAggregerer data\nosv.\n\n\n\n\n\n\nFølg instruksjonene her for å koble prosjektet ditt til Github.\n\n\n\n\n\nKilder konfigureres i et teams Infrastructure as Code (IaC) repo på Github. Det kan finnes basert på følgende formulering: github.com/statisticsnorway/&lt;teamnavn&gt;-iac. Kilder konfigureres under stien automation/source_data på repoet.\n\n\n\nHver kilde konfigureres ved hjelp av to filer:\n\nconfig.yaml som blant annet konfigurer hvilke stier i teamets kildedatabøtte prosesseres.\nprocess_source_data.py som kjøres når en kildedatafil prosesseres. Her må man skrive en python funksjon på en viss format.\n\nDisse filene er lagt til en mappe per kilde, under automation/source_data i IaC repoet. Se eksemplet under for en detaljert forklaring.\n\n\n\n\n\n\nMappenavnet for kilder i IaC repoene er brukt som navn for ressurser. Dette i praksis betyr at det enesete tillatte tegnene i mappenavnet er bokstaver, tall og bindestrek. Det er ikke tillatt med mellomrom eller andre spesialtegn. Mappenavnet kan bestå av opptil 20 tegn.\n\n\n\n\n\n\nDette går ut på om prosesseringsscriptet kan enkelt håndtere variasjonen i filene som samles inn.\nGrunn til å opprette en ny kilde kan være: - Kildedatafilen har en annen format (f.eks xml eller json) - Kildedataen har ulike felter - Kildedataen inneholder PII2 eller ikke\n\n\n\n\n\n\n\n\n\nDisse instruksjoner forutsetter at prosjektet ditt er koblet til Github allerede.\n\n\n\n\nSkrive skriptet process_source_data.py som prosesserer kildedatafilen til inndata. Dette kan testes av kildedataansvarlige manuelt på Jupyter for å verifisere at dataene blir prosessert som ønsket.\nI en branch i teamets IaC repo, legge til config.yaml og process_source_data.py i en mappe under automation/source_data. Se eksemplet under for en detaljert forklaring av formatet.\nLag en PR på grenen og få den godkjent av kildedataansvarlige.\nVent til alle tester er ferdige. Det skal stå “All checks have passed” før man går videre, hvis testene feiler følg stegene her. \nSkrive atlantis apply i en kommentar på PRen for å opprette det nødvendige infrastruktur for å prosessere kilden.\nMerge PRen.\nSjekk resultatet av det automatiske bygget.\nVerifisere at nye filer lagt i kildedatabøtten blir prosessert til inndata som forventet.\n\n\n\n\n\nLa oss si at et team (småbakst) har to datakilder levert av ulik dataeiere på ulik formater. Den ene er om boller og er på csv format og den andre er om rundstykker og er på json format. Kildedataansvarlige i teamet bestemmer seg for at filene i boller/ er like nok å prosesseres som en kilde, og at filene i rundstykker/ kan prosesseres som en annen kilde.\n\n\nssb-prod-smaabakst-data-kilde\n├── boller\n│   ├── hvetebolle\n│   │   ├── 2018-salg.csv\n│   │   ├── 2019-salg.csv\n│   │   ├── ...\n│   ├── kanelbolle\n│   │   ├── 2018-salg.csv\n│   │   ├── 2019-salg.csv\n│   │   ├── ...\n│   └── skolebolle\n│       ├── 2018-salg.csv\n│       ├── 2019-salg.csv\n│       ├── ...\n└── rundstykker\n    ├── haandverker\n    │   ├── apr-2022-resultater.json\n    │   ├── aug-2022-resultater.json\n    │   ├── ...\n    └── havre\n        ├── apr-2022-resultater.json\n        ├── aug-2022-resultater.json\n        ├── ...\n\n\n\nsmaabakst-iac\n└── automation\n    └── source_data\n        ├── boller\n        │   ├── config.yaml\n        │   └── process_source_data.py\n        └── rundstykker\n            ├── config.yaml\n            └── process_source_data.py\n\n\n\nfolder_prefix: boller\nVerdien for folder_prefix tilsvarer en “fil sti” i kildedatabøtte. I dette tilfellet vil nye filer lagt til under boller/ trigge en prosessering.\n\n\n\nfolder_prefix: rundstykker\n\n\n\n\n\nMed prosesseringsscriptet mener vi filen process_source_data.py som ligger i en mappe per kilde under automation/source_data. Dette scriptet vil bli kalt hver gang det blir lagt til en ny fil i kildedatabøtten. Metodesignaturen ser slik ut:\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\nDette betyr at hvis f.eks. 10 filer blir lagt til i kildedatabøtten samtidig så vil det startes opp 10 individuelle Python-prosesser som kaller denne main-metoden med forskjellig file_path. Parameteren file_path vil inneholde hele filstien inkl. filnavn. Så en enkel flytteoperasjon fra kildedatabøtten til inndatebøtten (uten noen form for konvertering) vil kunne uttrykkes slik:\nimport dapla as dp\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n    source_bucket_name = \"ssb-prod-my-project-data-kilde\"\n    destination_bucket_name = \"ssb-prod-my-project-data-produkt\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    df = dp.read_pandas(file_path)\n    dp.write_pandas(df, destination_bucket_path)\nAlternativt…\nfrom dapla import FileClient\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n    source_bucket_name = \"ssb-prod-my-project-data-kilde\"\n    destination_bucket_name = \"ssb-prod-my-project-data-produkt\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    fs = FileClient.get_gcs_file_system()\n    fs.copy(file_path, destination_path)\n\n\nDet anbefales å bruke Pythons logging modul for å logge, og ikke bruke print eller skrive til stdout/stderr. Det er satt opp en standard logger-konfigurasjon som skriver informasjonsmeldinger (log level info) til stdout og feilmeldinger (log level warning eller error) til stderr. Feil som ikke blir håndtert blir automatisk fanget opp og logget av automatiseringsløsningen. Eksemplet nedenfor logger en informasjonsmelding, en advarsel og en feilmelding:\nimport logging\nlogging.info('Til info')\nlogging.warning('Advarsel!')\nlogging.error('En feil oppstod!')"
  },
  {
    "objectID": "kildedata-prosessering.html#operasjoner-som-inngår-i-kildedata-prosessering",
    "href": "kildedata-prosessering.html#operasjoner-som-inngår-i-kildedata-prosessering",
    "title": "Kildedata prosessering",
    "section": "",
    "text": "Figur 1: Operasjoner som inngår i kildedata prosessering\n\n\n\nInndata er kildedata som er transformert til SSBs standard lagringsformat. Variabelnavn og -innhold er uendret bortsett fra at\n\ndirekte identifiserende variabler (f.eks. fødselsnummer) er pseudonymisert\ntegnsett, datoformat, adresse mm er endret til SSBs standardformat\ndet benyttes standard kodeverk (Klass) der det er mulig (f.eks. kjønn)\ndataene er minimert slik at kun variablene som er nødvendige i den videre produksjonsprosessen, inngår.\n\n\n(Standardutvalget 2021, 8)\nDet er ikke anbefalt å gjennomføre operasjoner som:\n\nGår på tvers av flere filer\nLegge til nye felt\nEndre navn på felt\nAggregerer data\nosv."
  },
  {
    "objectID": "kildedata-prosessering.html#ta-tjenesten-i-bruk",
    "href": "kildedata-prosessering.html#ta-tjenesten-i-bruk",
    "title": "Kildedata prosessering",
    "section": "",
    "text": "Følg instruksjonene her for å koble prosjektet ditt til Github.\n\n\n\n\n\nKilder konfigureres i et teams Infrastructure as Code (IaC) repo på Github. Det kan finnes basert på følgende formulering: github.com/statisticsnorway/&lt;teamnavn&gt;-iac. Kilder konfigureres under stien automation/source_data på repoet.\n\n\n\nHver kilde konfigureres ved hjelp av to filer:\n\nconfig.yaml som blant annet konfigurer hvilke stier i teamets kildedatabøtte prosesseres.\nprocess_source_data.py som kjøres når en kildedatafil prosesseres. Her må man skrive en python funksjon på en viss format.\n\nDisse filene er lagt til en mappe per kilde, under automation/source_data i IaC repoet. Se eksemplet under for en detaljert forklaring.\n\n\n\n\n\n\nMappenavnet for kilder i IaC repoene er brukt som navn for ressurser. Dette i praksis betyr at det enesete tillatte tegnene i mappenavnet er bokstaver, tall og bindestrek. Det er ikke tillatt med mellomrom eller andre spesialtegn. Mappenavnet kan bestå av opptil 20 tegn.\n\n\n\n\n\n\nDette går ut på om prosesseringsscriptet kan enkelt håndtere variasjonen i filene som samles inn.\nGrunn til å opprette en ny kilde kan være: - Kildedatafilen har en annen format (f.eks xml eller json) - Kildedataen har ulike felter - Kildedataen inneholder PII2 eller ikke\n\n\n\n\n\n\n\n\n\nDisse instruksjoner forutsetter at prosjektet ditt er koblet til Github allerede.\n\n\n\n\nSkrive skriptet process_source_data.py som prosesserer kildedatafilen til inndata. Dette kan testes av kildedataansvarlige manuelt på Jupyter for å verifisere at dataene blir prosessert som ønsket.\nI en branch i teamets IaC repo, legge til config.yaml og process_source_data.py i en mappe under automation/source_data. Se eksemplet under for en detaljert forklaring av formatet.\nLag en PR på grenen og få den godkjent av kildedataansvarlige.\nVent til alle tester er ferdige. Det skal stå “All checks have passed” før man går videre, hvis testene feiler følg stegene her. \nSkrive atlantis apply i en kommentar på PRen for å opprette det nødvendige infrastruktur for å prosessere kilden.\nMerge PRen.\nSjekk resultatet av det automatiske bygget.\nVerifisere at nye filer lagt i kildedatabøtten blir prosessert til inndata som forventet.\n\n\n\n\n\nLa oss si at et team (småbakst) har to datakilder levert av ulik dataeiere på ulik formater. Den ene er om boller og er på csv format og den andre er om rundstykker og er på json format. Kildedataansvarlige i teamet bestemmer seg for at filene i boller/ er like nok å prosesseres som en kilde, og at filene i rundstykker/ kan prosesseres som en annen kilde.\n\n\nssb-prod-smaabakst-data-kilde\n├── boller\n│   ├── hvetebolle\n│   │   ├── 2018-salg.csv\n│   │   ├── 2019-salg.csv\n│   │   ├── ...\n│   ├── kanelbolle\n│   │   ├── 2018-salg.csv\n│   │   ├── 2019-salg.csv\n│   │   ├── ...\n│   └── skolebolle\n│       ├── 2018-salg.csv\n│       ├── 2019-salg.csv\n│       ├── ...\n└── rundstykker\n    ├── haandverker\n    │   ├── apr-2022-resultater.json\n    │   ├── aug-2022-resultater.json\n    │   ├── ...\n    └── havre\n        ├── apr-2022-resultater.json\n        ├── aug-2022-resultater.json\n        ├── ...\n\n\n\nsmaabakst-iac\n└── automation\n    └── source_data\n        ├── boller\n        │   ├── config.yaml\n        │   └── process_source_data.py\n        └── rundstykker\n            ├── config.yaml\n            └── process_source_data.py\n\n\n\nfolder_prefix: boller\nVerdien for folder_prefix tilsvarer en “fil sti” i kildedatabøtte. I dette tilfellet vil nye filer lagt til under boller/ trigge en prosessering.\n\n\n\nfolder_prefix: rundstykker"
  },
  {
    "objectID": "kildedata-prosessering.html#skrive-prosesseringsscriptet",
    "href": "kildedata-prosessering.html#skrive-prosesseringsscriptet",
    "title": "Kildedata prosessering",
    "section": "",
    "text": "Med prosesseringsscriptet mener vi filen process_source_data.py som ligger i en mappe per kilde under automation/source_data. Dette scriptet vil bli kalt hver gang det blir lagt til en ny fil i kildedatabøtten. Metodesignaturen ser slik ut:\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\nDette betyr at hvis f.eks. 10 filer blir lagt til i kildedatabøtten samtidig så vil det startes opp 10 individuelle Python-prosesser som kaller denne main-metoden med forskjellig file_path. Parameteren file_path vil inneholde hele filstien inkl. filnavn. Så en enkel flytteoperasjon fra kildedatabøtten til inndatebøtten (uten noen form for konvertering) vil kunne uttrykkes slik:\nimport dapla as dp\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n    source_bucket_name = \"ssb-prod-my-project-data-kilde\"\n    destination_bucket_name = \"ssb-prod-my-project-data-produkt\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    df = dp.read_pandas(file_path)\n    dp.write_pandas(df, destination_bucket_path)\nAlternativt…\nfrom dapla import FileClient\n\n def main(file_path):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         file_path: full file path of the source data file.\n     \"\"\"\n    source_bucket_name = \"ssb-prod-my-project-data-kilde\"\n    destination_bucket_name = \"ssb-prod-my-project-data-produkt\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    fs = FileClient.get_gcs_file_system()\n    fs.copy(file_path, destination_path)\n\n\nDet anbefales å bruke Pythons logging modul for å logge, og ikke bruke print eller skrive til stdout/stderr. Det er satt opp en standard logger-konfigurasjon som skriver informasjonsmeldinger (log level info) til stdout og feilmeldinger (log level warning eller error) til stderr. Feil som ikke blir håndtert blir automatisk fanget opp og logget av automatiseringsløsningen. Eksemplet nedenfor logger en informasjonsmelding, en advarsel og en feilmelding:\nimport logging\nlogging.info('Til info')\nlogging.warning('Advarsel!')\nlogging.error('En feil oppstod!')"
  },
  {
    "objectID": "kildedata-prosessering.html#footnotes",
    "href": "kildedata-prosessering.html#footnotes",
    "title": "Kildedata prosessering",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPersonidentifiserende Informasjon↩︎\nPersonidentifiserende Informasjon↩︎"
  },
  {
    "objectID": "statistikkproduksjon.html",
    "href": "statistikkproduksjon.html",
    "title": "Statistikkproduksjon",
    "section": "",
    "text": "Statistikkproduksjon\nI forrige del beskrev vi noen grunnleggende ting rundt Dapla. I denne delen tar vi for oss hvordan du kan begynne å jobbe med skarpe data på plattformen.\nKapittelet som beskriver hvordan man logger seg inn på Dapla vil fungere uten at du må gjøre noen forberedelser. Er man koblet på SSB sitt nettverk så vil alle SSB-ansatte kunne gå inn på plattformen og kode i Python og R. Men du får ikke tilgang til SSBs område for datalagring på plattformen. I praksis vil det si at man kan generere data med kode, men man kan ikke jobbe med skarpe data.\nFor å få muligheten til å jobbe med skarpe data MÅ du først opprette et dapla-team. Dette er det første naturlige steget å ta når man skal begynne å jobbe med statistikkproduksjon på Dapla. I dette kapittelet vil vi forklare det du trenger å vite om det å opprette og jobbe innenfor et team."
  },
  {
    "objectID": "pakke-install-bakken.html",
    "href": "pakke-install-bakken.html",
    "title": "Installere pakker",
    "section": "",
    "text": "Installering av pakker i Jupyter miljøer på bakken (f.eks https://sl-jupyter-p.ssb.no) foregår stort sett helt lik som på Dapla. Det er én viktig forskjell, og det er at installasjon skjer via en proxy som heter Nexus.\n\n\nPip er ferdig konfigurert for bruk av Nexus og kan kjøres som beskrevet for Dapla\n\n\n\nHvis man bruker Poetry for håndtering av pakker i et prosjekt, så må man kjøre følgende kommando i prosjekt-mappe etter prosjektet er opprettet.\npoetry source add --default nexus `echo $PIP_INDEX_URL`\nDa får man installere pakker som vanlig f.eks\npoetry add matplotlib\n\n\n\n\n\n\nHvis man forsøker å installere prosjektet i et annet miljø (f.eks Dapla), så må man fjerner nexus kilden ved å kjøre\npoetry source remove nexus\n\n\n\n\n\n\n\nProsessen med å installere pakker for R på bakken er det samme som på Dapla. Noen pakker (for eksempel arrow og devtools) kan foreløpig ikke installeres på bakken på egenhånd pga 3. parti avhengigheter. Vi jobber med å finne en løsning til dette."
  },
  {
    "objectID": "pakke-install-bakken.html#python",
    "href": "pakke-install-bakken.html#python",
    "title": "Installere pakker",
    "section": "",
    "text": "Installering av pakker i Jupyter miljøer på bakken (f.eks https://sl-jupyter-p.ssb.no) foregår stort sett helt lik som på Dapla. Det er én viktig forskjell, og det er at installasjon skjer via en proxy som heter Nexus.\n\n\nPip er ferdig konfigurert for bruk av Nexus og kan kjøres som beskrevet for Dapla\n\n\n\nHvis man bruker Poetry for håndtering av pakker i et prosjekt, så må man kjøre følgende kommando i prosjekt-mappe etter prosjektet er opprettet.\npoetry source add --default nexus `echo $PIP_INDEX_URL`\nDa får man installere pakker som vanlig f.eks\npoetry add matplotlib\n\n\n\n\n\n\nHvis man forsøker å installere prosjektet i et annet miljø (f.eks Dapla), så må man fjerner nexus kilden ved å kjøre\npoetry source remove nexus"
  },
  {
    "objectID": "pakke-install-bakken.html#r",
    "href": "pakke-install-bakken.html#r",
    "title": "Installere pakker",
    "section": "",
    "text": "Prosessen med å installere pakker for R på bakken er det samme som på Dapla. Noen pakker (for eksempel arrow og devtools) kan foreløpig ikke installeres på bakken på egenhånd pga 3. parti avhengigheter. Vi jobber med å finne en løsning til dette."
  },
  {
    "objectID": "spark.html",
    "href": "spark.html",
    "title": "Spark",
    "section": "",
    "text": "Kommer snart.\n\n\nFor å kunne benytte det virtuelle miljøet i en notebook må man sette opp en kernel. Dette er beskrevet her. Denne kernelen er imidlertid ikke satt opp til å bruke pyspark som standard, og for å få til det må man gjøre noen manuelle steg.\n\n\nDenne kommandoen legger til pakken pyspark i prosjektet (med samme versjon som på jupyterlab).\npoetry add pyspark==$(pip show pyspark | grep Version | egrep -o \"([0-9]{1,}\\.)+[0-9]{1,}\")\n\n\n\nPyspark kan kjøres enten på lokal maskin eller på flere maskiner samtidig i en såkalt klynge (cluster). Sistnevnte kan være mer effektivt å bruke når man har større mengder data, men det krever også mer konfigurasjon.\n\n\nOppsettet for Pyspark på lokal maskin er det enkleste å sette opp siden Pyspark vil ha direkte tilgang til det lokale filsystemet. Man kan bruke miljøvariabelen PYSPARK_PYTHON til å peke på det virtuelle miljøet, og dermed vil Pyspark også ha tilgang til alle pakkene som er installert der. I en notebook vil dette kunne settes opp slik:\nimport os\nimport subprocess\n\n# Finner filstien til det virtuelle miljøet\npython_path = subprocess.run(['poetry', 'run', 'which', 'python'],\n                             capture_output=True, text=True).stdout.rstrip('\\n')\n\nos.environ[\"PYSPARK_PYTHON\"] = python_path\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = os.environ[\"PYSPARK_LOCAL_SUBMIT_ARGS\"]\nTil slutt må man kjøre et script for å initialisere pyspark for lokal maskin:\n%run /usr/local/share/jupyter/kernels/pyspark_local/init.py\nDette scriptet vil sette et spark objekt som brukes for å kalle API’et til pyspark.\n\n\n\nHvis man vil kjøre Pyspark i et cluster (dvs. på flere maskiner) så vil databehandlingen foregå på andre maskiner som ikke har tilgang til det lokale filsystemet. Man må dermed lage en “pakke” av det virtuelle miljøet på lokal maskin og tilgjengeliggjøre dette for alle maskinene i clusteret. For å lage en slik “pakke” kan man bruke et bibliotek som heter venv-pack. Dette kan kjøres fra et terminalvindu slik:\nvenv-pack -p .venv -o pyspark_venv.tar.gz\nMerk at kommandoen over må kjøres fra rot-mappen i prosjektet ditt. Her er pyspark_venv.tar.gz et tilfeldig valgt filnavn, men dette filnavnet skal brukes videre i notebooken.\nimport os\nimport subprocess\n\n# Miljøvariabel som peker på en utpakket versjon av det virtuelle miljøet\nos.environ[\"PYSPARK_PYTHON\"] = \"./environment/bin/python\"\n\n# Legg til et flagg, --archives, som peker på \"pakken\" med det virtuelle miljøet\nconf = os.environ[\"PYSPARK_K8S_SUBMIT_ARGS\"].split(' ')\nlast_index = conf.index('pyspark-shell')\nconf[last_index:last_index] = ['--archives', 'pyspark_venv.tar.gz#environment']\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = ' '.join(conf)\nTil slutt må man kjøre et script for å initialisere pyspark cluster:\n%run /usr/local/share/jupyter/kernels/pyspark_k8s/init.py\nDette scriptet vil sette et spark objekt som brukes for å kalle API’et til pyspark."
  },
  {
    "objectID": "spark.html#bruk-av-pyspark-i-virtuelle-miljøer",
    "href": "spark.html#bruk-av-pyspark-i-virtuelle-miljøer",
    "title": "Spark",
    "section": "",
    "text": "For å kunne benytte det virtuelle miljøet i en notebook må man sette opp en kernel. Dette er beskrevet her. Denne kernelen er imidlertid ikke satt opp til å bruke pyspark som standard, og for å få til det må man gjøre noen manuelle steg.\n\n\nDenne kommandoen legger til pakken pyspark i prosjektet (med samme versjon som på jupyterlab).\npoetry add pyspark==$(pip show pyspark | grep Version | egrep -o \"([0-9]{1,}\\.)+[0-9]{1,}\")\n\n\n\nPyspark kan kjøres enten på lokal maskin eller på flere maskiner samtidig i en såkalt klynge (cluster). Sistnevnte kan være mer effektivt å bruke når man har større mengder data, men det krever også mer konfigurasjon.\n\n\nOppsettet for Pyspark på lokal maskin er det enkleste å sette opp siden Pyspark vil ha direkte tilgang til det lokale filsystemet. Man kan bruke miljøvariabelen PYSPARK_PYTHON til å peke på det virtuelle miljøet, og dermed vil Pyspark også ha tilgang til alle pakkene som er installert der. I en notebook vil dette kunne settes opp slik:\nimport os\nimport subprocess\n\n# Finner filstien til det virtuelle miljøet\npython_path = subprocess.run(['poetry', 'run', 'which', 'python'],\n                             capture_output=True, text=True).stdout.rstrip('\\n')\n\nos.environ[\"PYSPARK_PYTHON\"] = python_path\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = os.environ[\"PYSPARK_LOCAL_SUBMIT_ARGS\"]\nTil slutt må man kjøre et script for å initialisere pyspark for lokal maskin:\n%run /usr/local/share/jupyter/kernels/pyspark_local/init.py\nDette scriptet vil sette et spark objekt som brukes for å kalle API’et til pyspark.\n\n\n\nHvis man vil kjøre Pyspark i et cluster (dvs. på flere maskiner) så vil databehandlingen foregå på andre maskiner som ikke har tilgang til det lokale filsystemet. Man må dermed lage en “pakke” av det virtuelle miljøet på lokal maskin og tilgjengeliggjøre dette for alle maskinene i clusteret. For å lage en slik “pakke” kan man bruke et bibliotek som heter venv-pack. Dette kan kjøres fra et terminalvindu slik:\nvenv-pack -p .venv -o pyspark_venv.tar.gz\nMerk at kommandoen over må kjøres fra rot-mappen i prosjektet ditt. Her er pyspark_venv.tar.gz et tilfeldig valgt filnavn, men dette filnavnet skal brukes videre i notebooken.\nimport os\nimport subprocess\n\n# Miljøvariabel som peker på en utpakket versjon av det virtuelle miljøet\nos.environ[\"PYSPARK_PYTHON\"] = \"./environment/bin/python\"\n\n# Legg til et flagg, --archives, som peker på \"pakken\" med det virtuelle miljøet\nconf = os.environ[\"PYSPARK_K8S_SUBMIT_ARGS\"].split(' ')\nlast_index = conf.index('pyspark-shell')\nconf[last_index:last_index] = ['--archives', 'pyspark_venv.tar.gz#environment']\nos.environ[\"PYSPARK_SUBMIT_ARGS\"] = ' '.join(conf)\nTil slutt må man kjøre et script for å initialisere pyspark cluster:\n%run /usr/local/share/jupyter/kernels/pyspark_k8s/init.py\nDette scriptet vil sette et spark objekt som brukes for å kalle API’et til pyspark."
  },
  {
    "objectID": "ssbproject.html",
    "href": "ssbproject.html",
    "title": "SSB-project",
    "section": "",
    "text": "SSB-project\nI forrige del forklarte vi hvordan man jobber med skarpe data på Dapla. Det neste steget vil ofte være å begynne å utvikle kode i Python og/eller R. Dette innebærer at man helst skal:\n\nversjonshåndtere med Git\nopprette et GitHub-repo\nopprette et virtuelt miljø som husker hvilke versjoner av pakker og programmeringsspråk du brukte\n\nI tillegg må alt dette konfigureres for hvordan SSB sine systemer er satt opp. Dette har vist seg å være unødvendig krevende for mange. Team Statistikktjenester har derfor utviklet et program som gjør alt dette for deg på en enkel måte som heter ssb-project.\nVi mener at ssb-project er et naturlig sted å starte når man skal bygge opp koden i Python eller R. Det gjelder både på bakken og på sky. I denne delen av boken forklarer vi først hvordan du bruker ssb-project i det første kapittelet. Siden programmet skjuler mye av kompleksiteten rundt dette, så bruker vi de andre kapitlene til å forklare hvordan man ville satt opp dette uten hjelp av programmet. Dermed vil det være lett for SSB-ansatte å skjønne hva som gjøres og hvorfor det er nødvendig."
  },
  {
    "objectID": "oracle.html",
    "href": "oracle.html",
    "title": "Oracle",
    "section": "",
    "text": "Oracle"
  },
  {
    "objectID": "hvorfor-dapla.html",
    "href": "hvorfor-dapla.html",
    "title": "Hvorfor Dapla?",
    "section": "",
    "text": "Hvorfor Dapla?\nSom dataplattform skal Dapla stimulerere til økt kvalitet på statistikk og forskning, samtidig som den gjør organisasjonen mer tilpasningsdyktig i møte med fremtiden.\n\n\nDen nye skybaserte dataplattformen (Dapla) skal bli viktig for å effektivisere arbeids-og produksjonsprosesser, den skal sikre effektiv lagring og gjenfinning av data og metadata, og støtte opp under deling av data på tvers av statistikkområder.\n\nKilde: Langtidsplan for SSB (2022-2024)\n\n\n\nMålet med Dapla er å tilby tjenester og verktøy som lar statistikkprodusenter og forskere produsere resultater på en sikker og effektiv måte."
  },
  {
    "objectID": "systemoversikt.html",
    "href": "systemoversikt.html",
    "title": "Systemoversikt",
    "section": "",
    "text": "Systemoversikt\nHvilke komponenter er plattformen bygd opp på? Forklart på lettest mulig måte."
  },
  {
    "objectID": "nytt-ssbproject.html",
    "href": "nytt-ssbproject.html",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "I dette kapittelet forklarer vi hvordan du oppretter et ssb-project og hva det innebærer. ssb-project er et CLI1 for å raskt komme i gang med koding på Dapla, hvor en del SSB-spesifikke beste-prakiser er ivaretatt. Kode som naturlig hører sammen, f.eks. koden til et produksjonsløp for en statistikk, er målgruppen for dette programmet. Kort fortalt kan du kjøre denne kommandoen i en terminal\nssb-project create stat-testprod\nog du vil få en mappe som heter stat-testprod med følgende innhold:\n\nStandard mappestruktur En standard mappestruktur gjør det lettere å dele og samarbeide om kode, som igjen reduserer sårbarheten knyttet til at få personer kjenner koden.\nVirtuelt miljø Virtuelle miljøer isolerer og lagrer informasjon knyttet til kode. For eksempel hvilken versjon av Python du bruker og tilhørende pakkeversjoner. Det er viktig for at publiserte tall skal være reproduserbare. Verktøyet for å lage virtuelt miljø er Poetry.\nVersjonshåndtering med Git Initierer versjonshåndtering med Git og legger til SSBs anbefalte .gitignore og .gitattributes. Det sikrer at du ikke versjonhåndterer filer/informasjon som ikke skal versjonshåndteres.\n\nI tillegg lar ssb-project deg opprette et GitHub-repo hvis du ønsker. Les mer om hvordan du kan ta i bruk dette verktøyet under.\n\n\n\n\n\n\nNote\n\n\n\nDokumentasjonen for ssb-project finnes her: https://statisticsnorway.github.io/ssb-project-cli/. Det oppdateres hver gang en ny versjon av ssb-project slippes.\n\n\n\n\nFør du kan ta i bruk ssb-project så er det et par ting som må være på plass:\n\nDu må ha opprettet en git-bruker og git-epost lokalt der du skal kalle på programmet (les mer om hvordan her).\nHvis du ønsker at ssb-project også skal opprette et GitHub-repo for deg må du også følgende være på plass:\n\nDu må ha en GitHub-bruker (les hvordan her)\nSkru på 2-faktor autentifisering for GitHub-brukeren din (les hvordan her)\nVære koblet mot SSBs organisasjon statisticsnorway på GitHub (les hvordan her)\nOpprette Personal Access Token (PAT) og godkjenne det for bruk mot statisticsnorway (les hvordan her)\n\n\nDet er også å anbefale at du lagrer PAT lokalt slik at du ikke trenger å forholde deg til det når jobber med Git og GitHub. Hvis du har alt dette på plass så kan du bare fortsette å følge de neste kapitlene.\n\n\n\n\n\n\n\n\n\nHar du Github bruker? Noe funksjonalitet i ssb-project krever det. Finn ut hvordan ved å lese forrige kapittel.\n\n\n\nssb-project lar deg opprette en prosjekt-mappe med og uten GitHub-repo. La oss ta for oss hver av alternativene.\n\n\nFor å opprette et nytt ssb-project uten GitHub-repo gjør du følgende:\n\nÅpne en terminal. De fleste vil gjøre dette i Jupyterlab på bakke eller sky og da kan de bare trykke på det blå ➕-tegnet i Jupyterlab og velge Terminal.\nFør vi kjører programmet må vi være obs på at ssb-project vil opprette en ny mappe der vi står. Gå derfor til den mappen du ønsker å ha den nye prosjektmappen. For å opprette et prosjekt som heter stat-testprod så skriver du følgende i terminalen:\n\nssb-project create stat-testprod\n\n\nHvis du stod i hjemmemappen din på når du skrev inn kommandoen over i terminalen, så har du fått mappestrukturen som vises i Figur 1. 2. Den inneholder følgende :\n\n.git-mappe som blir opprettet for å versjonshåndtere med Git.\nsrc-mappe som skal inneholde all koden som utgjør produksjonsløpet.\ntests-mappe som inneholder tester du skriver for koden din.\nLICENCE-fil som skal benyttes for public-repos i SSB.\npoetry.lock-fil som inneholder alle versjoner av Python-pakker som blir brukt.\nREADME.md-fil som brukes for tekstlig innhold på GitHub-siden for prosjektet.\n\n\n\n\n\n\nFigur 1: Mappen som ble opprettet av ssb-project.\n\n\n\n\n\n\n\nOver så opprettet vi et ssb-project uten å opprette et GitHub-repo. Hvis du ønsker å opprette et GitHub-repo også må du endre kommandoen over til:\nssb-project create stat-testprod --github --github-token='blablabla'\nKommandoen over oppretter en mappestruktur slik vi så tidligere, men også et ssb-project som heter stat-testprod med et GitHub-repo med samme navn. Som du ser så må vi da sende med opsjonen --github og PAT med opsjonen --github-token='blablabla'. Repoet i GitHub ser da ut som i Figur 2.\n\n\n\nFigur 2: GitHub-repo som er opprettet av ssb-project\n\n\n\n\n\n\n\n\nNår du har opprettet et nytt ssb-project, eller bygget et eksisterende prosjekt, så kan det ta rundt 30 sekunder før kernelen viser seg i Jupterlab-launcher. Vær tålmodig!\n\n\n\n\n\n\n\nNår du har opprettet et ssb-project så kan du installere de python-pakkene du trenger fra PyPI. Hvis du for eksempel ønsker å installere Pandas, et populært data wrangling bibliotek, så kan du gjøre følgende:\n\nÅpne en terminal i Jupyterlab.\nGå inn i prosjektmappen din ved å skrive\n\ncd &lt;sti til prosjektmappe&gt;\n\nLag en branch/utviklingsg som f.eks. heter install-pandas:\n\ngit checkout -b install-pandas\n\nInstaller Pandas ved å skrive følgende\n\npoetry add pandas\n\n\n\nFigur 3: Installasjon av Pandas med ssb-project\n\n\nFigur 3 viser hvordan dette vil se ut i en Jupyterlab-terminal. Kommandoen for å installere noe er poetry add etterfulgt av pakkenavnet. Vi ser også at den automatisk legger til Pandas-versjonen i filen poetry.lock. Les mer om hvordan man installerer pakker her.\n\n\n\nNår du nå har installert en pakke så har filen poetry.lock endret seg. La oss for eksempelets skyld anta at du ønsker å bruke Git til å dokumentere denne hendelsen, og dele det med en kollega via GitHub. Hvis vi har opprettet et ssb-project med et GitHub-repo så kan vi gjøre akkurat dette:\n\nVi kan stage alle endringer med følgende kommando i terminalen når vi står i prosjektmappen:\n\ngit add -A\n\nVidere kan commit en endring, dvs. ta et stillbilde av koden i dette øyeblikket, ved å skrive følgende:\n\ngit commit -m \"Installert pandas\"\n\nPush det opp til GitHub3. Anta at vi gjorde dette i branchen install-pandas som ble opprettet tidligere. Da kan vi skrive følgende:\n\ngit push --set-upstream origin install-pandas\nMer kommer her.\n\n\n\nNår vi skal samarbeide med andre om kode så gjør vi dette via GitHub. Når du pusher koden din til GitHub, så kan samarbeidspartnere pulle ned koden og jobbe videre med den. Men når de henter ned koden så vil de bare hente ned selve koden, ikke pakker og Python-versjonen som ble brukt. De må installere alt som du hadde installert. I tillegg trenger de en kernel hvis de skal jobbe i Jupyterlab. ssb-project gjør det svært enkelt å bygge opp det du trenger, siden det virtuelle miljøet har all informasjon om hva som trengs.\nFor at samarbeidspartneren din skal kunne bygge miljøet på nytt, må de ha gjort en minimal konfigurering av Git. Les mer om hvordan du frem for å gjøre dette her.\nFor å bygge opp et eksisterende miljø gjør du følgende:\n\nFørst må du kopiere prosjektet ned lokalt, eller klone repoet med git-terminologi\n\ngit clone https://github.com/statisticsnorway/&lt;prosjektnavn&gt;\n\nGå inn i mappen du klonet\n\ncd &lt;prosjektnavn&gt;\n\nSkape et virtuelt miljø og installere en tilsvarende Jupyter kernel med\n\nssb-project build\n\n\n\nDet vil være tilfeller hvor man ønsker å slette et ssb-project, enten fordi man ikke trenger koden lenger eller fordi man bare testet litt.\n\n\nHvis man jobber med flere prosjekter så kan det fort bli mange Jupyter kerneler hengende igjen. Derfor er det også mulighet å kjøre\nssb-project clean stat-testprod\nsom sletter Jupyter-kernelen og de installerte pakkene i prosjektet. Hvis du også ønsker å slette selve mappen med kode må du gjøre det manuelt4:\nrm -rf ~/stat-testprod/\nProsjektmappen over lå direkte i hjemmemappen min og hjemmemappen på Linux kan alltid referes til med et tilda-tegn ~.\n\n\n\nGitHub-repoer som er opprettet under SSB sin organinasjons statisticsnorway på GitHub kan ikke slettes, bare arkiveres. Grunnen er at hvis man oppdager en sårbarhet senere så er det viktig å kunne se repoet for å forstå hva som har skjedd.\nHvis du ikke trenger et GitHub-repo lenger kan man arkivere repoet. Det gjør du på følgende måte:\n\nGi inn i repoet Settings slik som vist med rød pil i Figur 4.\n\n\n\n\nFigur 4: Settings for repoet.\n\n\n\nUnder General scroller du deg ned til delen som heter Danger Zone og velger Archive this repository, slik som vist på Figur 5.\n\n\n\n\nFigur 5: Arkivering av et repo.\n\n\n\nI dialogboksen som dukker opp fyller du inn reponavnet som beskrevet og trykker på I understand the consequences, archive this repository, som vist i Figur 6.\n\n\n\n\nFigur 6: Bekreftelse av arkiveringen.\n\n\nNår det er gjort så er repoet lesbart, men man kan ikke jobbe med det. Men som vi ser av @#fig-github-repo-settings-archive-warning kan man omgjøre arkiveringen senere hvis det skulle være ønskelig.\n\n\n\n\nVi har foreløpig ikke integret R i ssb-project. Grunnen er at det mest populære virtuelle miljø-verktøet for R, renv, kun tilbyr å passe på versjoner av R-pakker og ikke selve R-installasjonen. Det er en svakhet som trolig gjør det vanskeligere enn nødvendig å gjenskape tidligere publiserte resultater med ssb-project. I tillegg klarer den ikke å gjenkjenne pakker som blir brukt i ipynb-filer.\nPlanen er å finne et annet verktøy enn renv som kan også reprodusere R-versjonen. Team Statistikktjenester ser nærmere på hvilke alternativer som finnes og vil tilby noe i fremtiden.\nI mellomtiden kan man bruke renv slik det er beskrevet her for skymiljøet, og med denne modifiseringen for bakkemiljøet."
  },
  {
    "objectID": "nytt-ssbproject.html#forberedelser",
    "href": "nytt-ssbproject.html#forberedelser",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "Før du kan ta i bruk ssb-project så er det et par ting som må være på plass:\n\nDu må ha opprettet en git-bruker og git-epost lokalt der du skal kalle på programmet (les mer om hvordan her).\nHvis du ønsker at ssb-project også skal opprette et GitHub-repo for deg må du også følgende være på plass:\n\nDu må ha en GitHub-bruker (les hvordan her)\nSkru på 2-faktor autentifisering for GitHub-brukeren din (les hvordan her)\nVære koblet mot SSBs organisasjon statisticsnorway på GitHub (les hvordan her)\nOpprette Personal Access Token (PAT) og godkjenne det for bruk mot statisticsnorway (les hvordan her)\n\n\nDet er også å anbefale at du lagrer PAT lokalt slik at du ikke trenger å forholde deg til det når jobber med Git og GitHub. Hvis du har alt dette på plass så kan du bare fortsette å følge de neste kapitlene."
  },
  {
    "objectID": "nytt-ssbproject.html#opprett-ssb-project",
    "href": "nytt-ssbproject.html#opprett-ssb-project",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "Har du Github bruker? Noe funksjonalitet i ssb-project krever det. Finn ut hvordan ved å lese forrige kapittel.\n\n\n\nssb-project lar deg opprette en prosjekt-mappe med og uten GitHub-repo. La oss ta for oss hver av alternativene.\n\n\nFor å opprette et nytt ssb-project uten GitHub-repo gjør du følgende:\n\nÅpne en terminal. De fleste vil gjøre dette i Jupyterlab på bakke eller sky og da kan de bare trykke på det blå ➕-tegnet i Jupyterlab og velge Terminal.\nFør vi kjører programmet må vi være obs på at ssb-project vil opprette en ny mappe der vi står. Gå derfor til den mappen du ønsker å ha den nye prosjektmappen. For å opprette et prosjekt som heter stat-testprod så skriver du følgende i terminalen:\n\nssb-project create stat-testprod\n\n\nHvis du stod i hjemmemappen din på når du skrev inn kommandoen over i terminalen, så har du fått mappestrukturen som vises i Figur 1. 2. Den inneholder følgende :\n\n.git-mappe som blir opprettet for å versjonshåndtere med Git.\nsrc-mappe som skal inneholde all koden som utgjør produksjonsløpet.\ntests-mappe som inneholder tester du skriver for koden din.\nLICENCE-fil som skal benyttes for public-repos i SSB.\npoetry.lock-fil som inneholder alle versjoner av Python-pakker som blir brukt.\nREADME.md-fil som brukes for tekstlig innhold på GitHub-siden for prosjektet.\n\n\n\n\n\n\nFigur 1: Mappen som ble opprettet av ssb-project.\n\n\n\n\n\n\n\nOver så opprettet vi et ssb-project uten å opprette et GitHub-repo. Hvis du ønsker å opprette et GitHub-repo også må du endre kommandoen over til:\nssb-project create stat-testprod --github --github-token='blablabla'\nKommandoen over oppretter en mappestruktur slik vi så tidligere, men også et ssb-project som heter stat-testprod med et GitHub-repo med samme navn. Som du ser så må vi da sende med opsjonen --github og PAT med opsjonen --github-token='blablabla'. Repoet i GitHub ser da ut som i Figur 2.\n\n\n\nFigur 2: GitHub-repo som er opprettet av ssb-project\n\n\n\n\n\n\n\n\nNår du har opprettet et nytt ssb-project, eller bygget et eksisterende prosjekt, så kan det ta rundt 30 sekunder før kernelen viser seg i Jupterlab-launcher. Vær tålmodig!"
  },
  {
    "objectID": "nytt-ssbproject.html#installere-pakker",
    "href": "nytt-ssbproject.html#installere-pakker",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "Når du har opprettet et ssb-project så kan du installere de python-pakkene du trenger fra PyPI. Hvis du for eksempel ønsker å installere Pandas, et populært data wrangling bibliotek, så kan du gjøre følgende:\n\nÅpne en terminal i Jupyterlab.\nGå inn i prosjektmappen din ved å skrive\n\ncd &lt;sti til prosjektmappe&gt;\n\nLag en branch/utviklingsg som f.eks. heter install-pandas:\n\ngit checkout -b install-pandas\n\nInstaller Pandas ved å skrive følgende\n\npoetry add pandas\n\n\n\nFigur 3: Installasjon av Pandas med ssb-project\n\n\nFigur 3 viser hvordan dette vil se ut i en Jupyterlab-terminal. Kommandoen for å installere noe er poetry add etterfulgt av pakkenavnet. Vi ser også at den automatisk legger til Pandas-versjonen i filen poetry.lock. Les mer om hvordan man installerer pakker her."
  },
  {
    "objectID": "nytt-ssbproject.html#push-til-github",
    "href": "nytt-ssbproject.html#push-til-github",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "Når du nå har installert en pakke så har filen poetry.lock endret seg. La oss for eksempelets skyld anta at du ønsker å bruke Git til å dokumentere denne hendelsen, og dele det med en kollega via GitHub. Hvis vi har opprettet et ssb-project med et GitHub-repo så kan vi gjøre akkurat dette:\n\nVi kan stage alle endringer med følgende kommando i terminalen når vi står i prosjektmappen:\n\ngit add -A\n\nVidere kan commit en endring, dvs. ta et stillbilde av koden i dette øyeblikket, ved å skrive følgende:\n\ngit commit -m \"Installert pandas\"\n\nPush det opp til GitHub3. Anta at vi gjorde dette i branchen install-pandas som ble opprettet tidligere. Da kan vi skrive følgende:\n\ngit push --set-upstream origin install-pandas\nMer kommer her."
  },
  {
    "objectID": "nytt-ssbproject.html#bygg-eksisterende-ssb-project",
    "href": "nytt-ssbproject.html#bygg-eksisterende-ssb-project",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "Når vi skal samarbeide med andre om kode så gjør vi dette via GitHub. Når du pusher koden din til GitHub, så kan samarbeidspartnere pulle ned koden og jobbe videre med den. Men når de henter ned koden så vil de bare hente ned selve koden, ikke pakker og Python-versjonen som ble brukt. De må installere alt som du hadde installert. I tillegg trenger de en kernel hvis de skal jobbe i Jupyterlab. ssb-project gjør det svært enkelt å bygge opp det du trenger, siden det virtuelle miljøet har all informasjon om hva som trengs.\nFor at samarbeidspartneren din skal kunne bygge miljøet på nytt, må de ha gjort en minimal konfigurering av Git. Les mer om hvordan du frem for å gjøre dette her.\nFor å bygge opp et eksisterende miljø gjør du følgende:\n\nFørst må du kopiere prosjektet ned lokalt, eller klone repoet med git-terminologi\n\ngit clone https://github.com/statisticsnorway/&lt;prosjektnavn&gt;\n\nGå inn i mappen du klonet\n\ncd &lt;prosjektnavn&gt;\n\nSkape et virtuelt miljø og installere en tilsvarende Jupyter kernel med\n\nssb-project build"
  },
  {
    "objectID": "nytt-ssbproject.html#rydd-opp-etter-deg",
    "href": "nytt-ssbproject.html#rydd-opp-etter-deg",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "Det vil være tilfeller hvor man ønsker å slette et ssb-project, enten fordi man ikke trenger koden lenger eller fordi man bare testet litt.\n\n\nHvis man jobber med flere prosjekter så kan det fort bli mange Jupyter kerneler hengende igjen. Derfor er det også mulighet å kjøre\nssb-project clean stat-testprod\nsom sletter Jupyter-kernelen og de installerte pakkene i prosjektet. Hvis du også ønsker å slette selve mappen med kode må du gjøre det manuelt4:\nrm -rf ~/stat-testprod/\nProsjektmappen over lå direkte i hjemmemappen min og hjemmemappen på Linux kan alltid referes til med et tilda-tegn ~.\n\n\n\nGitHub-repoer som er opprettet under SSB sin organinasjons statisticsnorway på GitHub kan ikke slettes, bare arkiveres. Grunnen er at hvis man oppdager en sårbarhet senere så er det viktig å kunne se repoet for å forstå hva som har skjedd.\nHvis du ikke trenger et GitHub-repo lenger kan man arkivere repoet. Det gjør du på følgende måte:\n\nGi inn i repoet Settings slik som vist med rød pil i Figur 4.\n\n\n\n\nFigur 4: Settings for repoet.\n\n\n\nUnder General scroller du deg ned til delen som heter Danger Zone og velger Archive this repository, slik som vist på Figur 5.\n\n\n\n\nFigur 5: Arkivering av et repo.\n\n\n\nI dialogboksen som dukker opp fyller du inn reponavnet som beskrevet og trykker på I understand the consequences, archive this repository, som vist i Figur 6.\n\n\n\n\nFigur 6: Bekreftelse av arkiveringen.\n\n\nNår det er gjort så er repoet lesbart, men man kan ikke jobbe med det. Men som vi ser av @#fig-github-repo-settings-archive-warning kan man omgjøre arkiveringen senere hvis det skulle være ønskelig."
  },
  {
    "objectID": "nytt-ssbproject.html#hva-med-r",
    "href": "nytt-ssbproject.html#hva-med-r",
    "title": "Nytt ssb-project",
    "section": "",
    "text": "Vi har foreløpig ikke integret R i ssb-project. Grunnen er at det mest populære virtuelle miljø-verktøet for R, renv, kun tilbyr å passe på versjoner av R-pakker og ikke selve R-installasjonen. Det er en svakhet som trolig gjør det vanskeligere enn nødvendig å gjenskape tidligere publiserte resultater med ssb-project. I tillegg klarer den ikke å gjenkjenne pakker som blir brukt i ipynb-filer.\nPlanen er å finne et annet verktøy enn renv som kan også reprodusere R-versjonen. Team Statistikktjenester ser nærmere på hvilke alternativer som finnes og vil tilby noe i fremtiden.\nI mellomtiden kan man bruke renv slik det er beskrevet her for skymiljøet, og med denne modifiseringen for bakkemiljøet."
  },
  {
    "objectID": "nytt-ssbproject.html#footnotes",
    "href": "nytt-ssbproject.html#footnotes",
    "title": "Nytt ssb-project",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nCLI = Command-Line-Interface. Dvs. et program som er skrevet for å brukes terminalen ved hjelp av enkle kommandoer.↩︎\nFiler og mapper som starter med punktum er skjulte med mindre man ber om å se dem. I Jupyterlab kan disse vises i filutforskeren ved å velge View fra menylinjen, og deretter velge Show hidden files. I en terminal skriver man ls -a for å se de.↩︎\nÅ pushe til GitHub uten å sende ved Personal Access Token fordrer at du har lagret det lokalt så Git kan finne det. Her et eksempel på hvordan det kan gjøres.↩︎\nDette kan også gjøres ved å høyreklikke på mappen i Jupyterlab sin filutforsker og velge Delete.↩︎"
  },
  {
    "objectID": "github-app-integrasjon.html",
    "href": "github-app-integrasjon.html",
    "title": "Koble prosjektet til Github",
    "section": "",
    "text": "For at automatiseringsløsningen på Dapla skal kunne settes opp automatisk må denne ha tilgang til å lese fra prosjektets IAC-repo1. Dette avsnittet vil beskrive denne prosessen. Merk at dette er en engangsjobb som må gjøres av prosjektets kildedataansvarlige.\n\n\n\n\n\n\nViktig: Prosjektets kildedataansvarlige også må ha administrator-rettigheter til IAC-repoet i Github.\n\n\n\n\nLogg inn på Google Cloud Console og velg det prosjektet som skal konfigureres øverst venstre hjørte. Søk opp Cloud Build i søkefeltet og trykk på det valget som kommer opp.\nDet skal nå være en venstremeny tilgjengelig med tittel Cloud Build. Trykk på menyvalget som heter Triggers (Figur 1)\n\n\n\n\nFigur 1: Bilde av venstremeny\n\n\n\nI nedtrekkslisten Region sørg for at europe-north1 er valgt (Figur 2)\n\n\n\n\nFigur 2: Velg korrekt region\n\n\n\nTrykk deretter på en link som heter CONNECT REPOSITORY ca. midt på siden.\n\n\n\n\nFigur 3: Oversikt over triggers\n\n\n\nNå vil det dukke opp et vindu på høyre side med overskrift Connect repository (Figur 4). Velg GitHub (Cloud Build GitHub App) og trykk på CONTINUE\n\n\n\n\nFigur 4: Vindu for å velge Cloud Build Github App\n\n\n\nEt pop-up vindu tilsvarende Figur 5 vil komme opp. Trykk på Authorize. Vinduet vil etter hvert lukke seg og man kommer videre til et steg som heter Select repository (Figur 6)\n\n\n\n\nFigur 5: Pop-up vindu for Github\n\n\n\n\n\nFigur 6: Valg av Github repository\n\n\n\nTrykk på nedtrekkslisten Repository og skriv inn teamets navn. Huk av boksen ved teamets IAC-repo og trykk OK.\n\n\n\n\nFigur 7: Gi Google Build tilgang til Github repository\n\n\n\nKryss så av i sjekkboksen som i (Figur 8) og trykk CONNECT.\n\n\n\n\nFigur 8: Bekreft nytt Github repository\n\n\n\nTil slutt vil skjermbildet se ut som vist i Figur 9. Det siste steget Create a trigger kan du hoppe over. Dette vil bli satt opp av automatiseringsløsningen senere. Trykk på knappen DONE\n\n\n\n\nFigur 9: Siste steg - Create a trigger"
  },
  {
    "objectID": "github-app-integrasjon.html#footnotes",
    "href": "github-app-integrasjon.html#footnotes",
    "title": "Koble prosjektet til Github",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nIAC-repo er et en kodebase i Github på formen https://github.com/statisticsnorway/team-navn-iac.↩︎"
  },
  {
    "objectID": "overføring-av-data.html",
    "href": "overføring-av-data.html",
    "title": "Overføring av data",
    "section": "",
    "text": "For å overføre data mellom bakke og sky brukes Data Transfer, som er en tjeneste i Google Cloud Console. Denne tjenesten kan brukes til å flytte data både til og fra Linuxstammen og Dapla, og er tilgjengelig for teamets kildedataansvarlige.\nFor å få tilgang til å overføre filer må man be om dette ved opprettelsen av teamet. Ber man om det skjer følgende:\n\nEn mappe blir opprettet på Linux i prodsonen under /ssb/cloud_sync/\nEt Google Project blir opprettet med navn &lt;team navn&gt;-ts.\n\nDette Google-prosjektet er ikke det samme som der du lagrer annen data. Det har navnet &lt;team navn&gt;-ts, og filstiene på bakken og sky vises i Figur 1.\n\n\n\nFigur 1: Hvordan Transfer Service kan flytte filer mellom bakke og sky.\n\n\nTeamets kildedataansvarlige vil være spesifisert som en del av å opprette et Dapla-team.\n\n\nEnten man skal overføre filer opp til sky eller ned til bakken så bruker man den samme Data Transfer tjenesten. For å få tilgang til denne må man først logge seg inn i Google Cloud Console. Sjekk at du er logget inn med din SSB-konto (xxx@ssb.no).\nØverst på siden, til høyre for teksten Google Cloud finnes det en prosjektvelger, og her er det viktig å velge korrekt Google prosjekt. Hvis du trykker på prosjektvelgeren vil det åpnes opp et nytt vindu. Sjekk at det står SSB.NO øverst i dette vinduet. Trykk deretter på fanen ALL for å få opp alle tilgjengelige Google-prosjekter under organisasjonen ssb.no (Figur 2)\n\n\n\nFigur 2: Prosjektvelgeren i Google Cloud Console\n\n\nUnder ssb.no vil det ligge flere mapper. Åpne mappen som heter production og let frem en undermappe som har navnet på ditt Dapla-team. Strukturen skal se slik ut:\n    ssb.no\n    ├── production\n        └── &lt;teamnavn&gt;\n            ├── prod-&lt;teamnavn&gt;\n            └── &lt;teamnavn&gt;-ts\nDet underste nivået (prod-&lt;teamnavn&gt; og &lt;teamnavn&gt;-ts) viser prosjektene, nivået i mellom er mapper, og toppnivået er organisasjonen (ssb.no). Prosjektet &lt;teamnavn&gt;-ts er et separat prosjekt som bare teamets kildedataansvarlige har tilgang til, og det er her tjenesten Data Transfer skal settes opp.\n\nVelg derfor prosjektet &lt;teamnavn&gt;-ts.\nI søkefeltet til Google Cloud Console, skriv Data transfer og trykk på det valget som kommer opp.\nFørste gang man kommer inn på siden til Transfer Services vil man bli vist en blå knapp med teksten Set Up Connection. Når du trykker på denne vil det dukke opp et nytt felt hvor du får valget Create Pub-Sub Resources. Dette er noe som bare trengs å gjøre én gang. Trykk på den blå CREATE knappen, og deretter trykk på Close lenger nede.\nI navigasjonsmenyen til venstre trykk Transfer jobs, og deretter trykk på + Create transfer job øverst på siden for å opprette en ny overføringsjobb.\n\n\n\nFølgende oppskrift tar utgangspunkt i siden Create a transfer job (Figur 3):\n\n\n\nFigur 3: Opprett overføringsjobb i Google Cloud Console\n\n\n\nVelg POSIX filesystem under “Source type” og Google cloud storage under “Destination type” (eller motsatt hvis overføringsjobben skal gå fra Dapla til Linuxstammen). Trykk Next step\nNedtrekkslisten “Agent pool” skal normalt bare ha ett valg: transfer_service_default. Velg denne.\nI feltet “Source directory path” skal man kun skrive data/tilsky siden overføringsagenten kun har tilgang til mapper som ligger relativt plassert under /ssb/cloud_sync/&lt;teamnavn&gt;/. Trykk Next step\nVelg en destinasjon for overføringsjobben. Trykk på Browse og velg bøtten med navn som passer til ssb-prod-&lt;teamnavn&gt;-data-synk-opp. Vi anbefaler at du også oppretter en mappe inne i denne bøtten. Det gjøres ved å trykke på mappeikonet med et +-tegn foran. Skriv inn et passende mappenavn og trykk Select i bunnen av siden. Trykk deretter Next step\nNeste steg “Choose how and when to run this job” er opp til brukeren å bestemme. Hvis man f.eks. velger at Data Transfer skal overføre data en gang i uken, vil den kun starte en overføring hvis det finnes nye data. Trykk Next step\nBeskriv overføringsjobben, f.eks: “Flytter data for  til sky.”. Resten av feltene er opp til brukeren å bestemme. Standardverdiene er OK.\n\nTrykk til slutt på den blå Create-knappen. Du vil kunne se kjørende jobber under menyen Transfer jobs.\nFor å sjekke om data har blitt overført, skriv inn cloud storage i søkefeltet øverst på siden og trykk på det første valget som kommer opp. Her vil du finne en oversikt over alle teamets bøtter, deriblant en med navn ssb-prod-&lt;team-name&gt;-data-synk-opp. Når overføringsjobben er ferdig vil du kunne finne igjen dataene i den mappen som ble definert i stegene overnfor.\n\n\n\nOverføringsjobben settes opp nesten identisk med Overføring fra Linuxstammen til Dapla med unntak av følgende:\n\nSteg 1: Velg Google cloud storage under “Source type” og POSIX filesystem under “Destination type”\nSteg 2: Velg bøtten ssb-prod-&lt;team-name&gt;-data-synk-ned\nStep 3: Velg transfer_service_default som “Agent pool” og skriv data/frasky inn i feltet for “Destination directory path”.\n\nFor å se om data har blitt overført til Linuxstammen må du nå gå til mappen /ssb/cloud_sync/&lt;team-name&gt;/data/frasky fra FileZilla.\nHusk: Du kan alltids gå tilbake og se på tidligere fullførte jobber, og starte en overføringsjobb manuelt fra menyen Transfer jobs.\n\n\n\n\nNår du har satt opp en, enten for å overføre fra sky eller til sky, kan du skrive ut data til mappen eller bøtten som du har bedt Transfer Service om å overføre data fra.\nHvis du skal overføre data fra bakken/prodsonen til sky, så må teamets kildedataansvarlige skrive ut data til Linux-mappen /ssb/cloud_sync/&lt;team navn&gt;/data/tilsky, og det vil ende opp i Dapla-bøtta gs://ssb-prod-&lt;team navn&gt;-data-synk-opp Dette kan du gjøre med alle programmeringsverktøy som har en kobling til Linux-stammene der dataene ligger. For eksempel:\n\nSAS EG\nSAS-installasjon på Linux\nJupyterlab i prodsonen\nRstudio på sl-stata-03\n\nSkal du flytte data fra Dapla til bakken/prodsonen, så må teamets kildedataansvarlige skrive ut data til gs://ssb-prod-&lt;team navn&gt;-data-synk-opp-bøtta på Dapla. Det er noe man typisk gjør fra Jupyterlab på Dapla."
  },
  {
    "objectID": "overføring-av-data.html#sette-opp-overføringsjobber",
    "href": "overføring-av-data.html#sette-opp-overføringsjobber",
    "title": "Overføring av data",
    "section": "",
    "text": "Enten man skal overføre filer opp til sky eller ned til bakken så bruker man den samme Data Transfer tjenesten. For å få tilgang til denne må man først logge seg inn i Google Cloud Console. Sjekk at du er logget inn med din SSB-konto (xxx@ssb.no).\nØverst på siden, til høyre for teksten Google Cloud finnes det en prosjektvelger, og her er det viktig å velge korrekt Google prosjekt. Hvis du trykker på prosjektvelgeren vil det åpnes opp et nytt vindu. Sjekk at det står SSB.NO øverst i dette vinduet. Trykk deretter på fanen ALL for å få opp alle tilgjengelige Google-prosjekter under organisasjonen ssb.no (Figur 2)\n\n\n\nFigur 2: Prosjektvelgeren i Google Cloud Console\n\n\nUnder ssb.no vil det ligge flere mapper. Åpne mappen som heter production og let frem en undermappe som har navnet på ditt Dapla-team. Strukturen skal se slik ut:\n    ssb.no\n    ├── production\n        └── &lt;teamnavn&gt;\n            ├── prod-&lt;teamnavn&gt;\n            └── &lt;teamnavn&gt;-ts\nDet underste nivået (prod-&lt;teamnavn&gt; og &lt;teamnavn&gt;-ts) viser prosjektene, nivået i mellom er mapper, og toppnivået er organisasjonen (ssb.no). Prosjektet &lt;teamnavn&gt;-ts er et separat prosjekt som bare teamets kildedataansvarlige har tilgang til, og det er her tjenesten Data Transfer skal settes opp.\n\nVelg derfor prosjektet &lt;teamnavn&gt;-ts.\nI søkefeltet til Google Cloud Console, skriv Data transfer og trykk på det valget som kommer opp.\nFørste gang man kommer inn på siden til Transfer Services vil man bli vist en blå knapp med teksten Set Up Connection. Når du trykker på denne vil det dukke opp et nytt felt hvor du får valget Create Pub-Sub Resources. Dette er noe som bare trengs å gjøre én gang. Trykk på den blå CREATE knappen, og deretter trykk på Close lenger nede.\nI navigasjonsmenyen til venstre trykk Transfer jobs, og deretter trykk på + Create transfer job øverst på siden for å opprette en ny overføringsjobb.\n\n\n\nFølgende oppskrift tar utgangspunkt i siden Create a transfer job (Figur 3):\n\n\n\nFigur 3: Opprett overføringsjobb i Google Cloud Console\n\n\n\nVelg POSIX filesystem under “Source type” og Google cloud storage under “Destination type” (eller motsatt hvis overføringsjobben skal gå fra Dapla til Linuxstammen). Trykk Next step\nNedtrekkslisten “Agent pool” skal normalt bare ha ett valg: transfer_service_default. Velg denne.\nI feltet “Source directory path” skal man kun skrive data/tilsky siden overføringsagenten kun har tilgang til mapper som ligger relativt plassert under /ssb/cloud_sync/&lt;teamnavn&gt;/. Trykk Next step\nVelg en destinasjon for overføringsjobben. Trykk på Browse og velg bøtten med navn som passer til ssb-prod-&lt;teamnavn&gt;-data-synk-opp. Vi anbefaler at du også oppretter en mappe inne i denne bøtten. Det gjøres ved å trykke på mappeikonet med et +-tegn foran. Skriv inn et passende mappenavn og trykk Select i bunnen av siden. Trykk deretter Next step\nNeste steg “Choose how and when to run this job” er opp til brukeren å bestemme. Hvis man f.eks. velger at Data Transfer skal overføre data en gang i uken, vil den kun starte en overføring hvis det finnes nye data. Trykk Next step\nBeskriv overføringsjobben, f.eks: “Flytter data for  til sky.”. Resten av feltene er opp til brukeren å bestemme. Standardverdiene er OK.\n\nTrykk til slutt på den blå Create-knappen. Du vil kunne se kjørende jobber under menyen Transfer jobs.\nFor å sjekke om data har blitt overført, skriv inn cloud storage i søkefeltet øverst på siden og trykk på det første valget som kommer opp. Her vil du finne en oversikt over alle teamets bøtter, deriblant en med navn ssb-prod-&lt;team-name&gt;-data-synk-opp. Når overføringsjobben er ferdig vil du kunne finne igjen dataene i den mappen som ble definert i stegene overnfor.\n\n\n\nOverføringsjobben settes opp nesten identisk med Overføring fra Linuxstammen til Dapla med unntak av følgende:\n\nSteg 1: Velg Google cloud storage under “Source type” og POSIX filesystem under “Destination type”\nSteg 2: Velg bøtten ssb-prod-&lt;team-name&gt;-data-synk-ned\nStep 3: Velg transfer_service_default som “Agent pool” og skriv data/frasky inn i feltet for “Destination directory path”.\n\nFor å se om data har blitt overført til Linuxstammen må du nå gå til mappen /ssb/cloud_sync/&lt;team-name&gt;/data/frasky fra FileZilla.\nHusk: Du kan alltids gå tilbake og se på tidligere fullførte jobber, og starte en overføringsjobb manuelt fra menyen Transfer jobs."
  },
  {
    "objectID": "overføring-av-data.html#skrive-ut-data",
    "href": "overføring-av-data.html#skrive-ut-data",
    "title": "Overføring av data",
    "section": "",
    "text": "Når du har satt opp en, enten for å overføre fra sky eller til sky, kan du skrive ut data til mappen eller bøtten som du har bedt Transfer Service om å overføre data fra.\nHvis du skal overføre data fra bakken/prodsonen til sky, så må teamets kildedataansvarlige skrive ut data til Linux-mappen /ssb/cloud_sync/&lt;team navn&gt;/data/tilsky, og det vil ende opp i Dapla-bøtta gs://ssb-prod-&lt;team navn&gt;-data-synk-opp Dette kan du gjøre med alle programmeringsverktøy som har en kobling til Linux-stammene der dataene ligger. For eksempel:\n\nSAS EG\nSAS-installasjon på Linux\nJupyterlab i prodsonen\nRstudio på sl-stata-03\n\nSkal du flytte data fra Dapla til bakken/prodsonen, så må teamets kildedataansvarlige skrive ut data til gs://ssb-prod-&lt;team navn&gt;-data-synk-opp-bøtta på Dapla. Det er noe man typisk gjør fra Jupyterlab på Dapla."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Velkommen",
    "section": "",
    "text": "Denne boken er ment som enkel-å-bruke manual for å ta i bruk SSBs nye dataplattform Dapla. Plattformen forsøker å gjøre statistikkprodusenter og forskere så selvhjulpne som mulig. Målsetningen er at tjenestene som tilbys skal kunne tas i bruk på en enkel og intuitiv måte. Men uansett hvor lett tilgjengelig tjenester er, og hvor mye arbeid som er lagt å gjøre løsninger brukervennlige, så trenger de fleste i SSB en klar og tydelig veiledning for hvordan de skal brukes og i hvilken større sammenheng tjenestene inngår. Dapla-manualen er ment å være en sånn støtte i statistikkernes hverdag. Uansett om man lurer på hvordan man logger seg inn på plattformen, eller om man ønsker informasjon om kjøremiljøet for mer kompliserte maskinlæringsmodeller, så skal man finne veiledning i denne manualen. Målgruppen er både nybegynneren og den mer erfarne.\n\n\n\n\n\n\nAlle ansatte i SSB kan bidra til manualen ved å følge instruksjonene her.\n\n\n\n\n\nDapla-manualen er initiert og skrevet av Team Statistikktjenester i SSB. Bidragsytere er Øyvind Bruer-Skarsbø, Miles Winther, Bjørn Andre Skaar, Anders Lunde og Damir Medakovic. Ved behov for oppdateringer og nytt innhold håper vi at alle i SSB kan bidra.\n\n\n\nAlle i SSB kan bidra til denne manualen. Endringer må godkjennes av noen i Team Statistikktjenester, si gjerne i fra at det ligger en PR å se på.\n\n\n\n\n\n\nWarning\n\n\n\nDenne nettsiden er åpen og hvem som helst kan lese det som er skrevet her. Hold det i tankene når du skriver.\n\n\n\n\n\nMan trenger basis git kompetanse, det ligger en fin beskrivelse av det på Beste Praksis siden fra KVAKK.\nMan trenger en konto på Github, det kan man opprette ved å følge instruksjonene her.\nMan kan lære seg Markdown (enkel tekst formatering), en fin oversikt finnes her.\nVerktøyet Quarto burde installeres for å kunne se endringene slik som de ser ut på nettsiden. Installasjon instruksjoner finnes her.\n\n\n\n\n\nKlone repoet git clone https://github.com/statisticsnorway/dapla-manual.git\nLage en ny gren\nGjøre endringen\nKjør quarto preview dapla-manual og følge lenken for å sjekke at alt ser bra ut på nettsiden\nÅpne en PR\nBe noen å gjennomgå endringen og eventuelt godkjenne den\nMerge PRen\nEtter et par minutter skal din endring være synlig!"
  },
  {
    "objectID": "index.html#bidragsytere",
    "href": "index.html#bidragsytere",
    "title": "Velkommen",
    "section": "",
    "text": "Dapla-manualen er initiert og skrevet av Team Statistikktjenester i SSB. Bidragsytere er Øyvind Bruer-Skarsbø, Miles Winther, Bjørn Andre Skaar, Anders Lunde og Damir Medakovic. Ved behov for oppdateringer og nytt innhold håper vi at alle i SSB kan bidra."
  },
  {
    "objectID": "index.html#bidra-til-dapla-manualen",
    "href": "index.html#bidra-til-dapla-manualen",
    "title": "Velkommen",
    "section": "",
    "text": "Alle i SSB kan bidra til denne manualen. Endringer må godkjennes av noen i Team Statistikktjenester, si gjerne i fra at det ligger en PR å se på.\n\n\n\n\n\n\nWarning\n\n\n\nDenne nettsiden er åpen og hvem som helst kan lese det som er skrevet her. Hold det i tankene når du skriver.\n\n\n\n\n\nMan trenger basis git kompetanse, det ligger en fin beskrivelse av det på Beste Praksis siden fra KVAKK.\nMan trenger en konto på Github, det kan man opprette ved å følge instruksjonene her.\nMan kan lære seg Markdown (enkel tekst formatering), en fin oversikt finnes her.\nVerktøyet Quarto burde installeres for å kunne se endringene slik som de ser ut på nettsiden. Installasjon instruksjoner finnes her.\n\n\n\n\n\nKlone repoet git clone https://github.com/statisticsnorway/dapla-manual.git\nLage en ny gren\nGjøre endringen\nKjør quarto preview dapla-manual og følge lenken for å sjekke at alt ser bra ut på nettsiden\nÅpne en PR\nBe noen å gjennomgå endringen og eventuelt godkjenne den\nMerge PRen\nEtter et par minutter skal din endring være synlig!"
  },
  {
    "objectID": "gjenopprette-data.html",
    "href": "gjenopprette-data.html",
    "title": "Gjenopprette data fra bøtter",
    "section": "",
    "text": "Alle bøtter har automatisk versjonering. Dette gjør det mulig å tilbakeføre filer til en tidligere versjon eller gjenopprette filer som er slettet ved et uhell.\nLogg inn på Google Cloud Console og søk opp “Cloud Storage” i søkefeltet. Klikk på den bøtten hvor filen er lagret under “Buckets”.\n\n\nFra Cloud Storage skjermbildet kan man navigere seg frem til den mappen hvor filen tidligere er lagret og skru på radioknappen “Show deleted data” (Figur 1)\n\n\n\nFigur 1: Skru på visning av slettede filer\n\n\nNå vil man kunne se slettede filer i kursiv med teksten (Deleted) på slutten. Kolonnen “Version history” vil også vise hvor mange tidligere versjoner som finnes av denne filen. Trykk på filnavnet du ønsker å gjenopprette og velg deretter fanen “Version history”. I listen av versjoner til denne filen har man mulighet til å gjenopprette til en tidligere versjon ved å klikke på “Restore” (Figur 2).\n\n\n\nFigur 2: Gjenoppretting av en slettet fil\n\n\n\n\n\nFra Cloud Storage skjermbildet kan man navigere seg frem til den mappen hvor filen er lagret, og trykke på filnavnet. Velg deretter fanen “Version history”. I listen av versjoner til denne filen har man mulighet til å gjenopprette til en tidligere versjon ved å klikke på “Restore” (Figur 3).\n\n\n\nFigur 3: Versjonshistorikk til en fil"
  },
  {
    "objectID": "gjenopprette-data.html#gjenopprette-en-slettet-fil",
    "href": "gjenopprette-data.html#gjenopprette-en-slettet-fil",
    "title": "Gjenopprette data fra bøtter",
    "section": "",
    "text": "Fra Cloud Storage skjermbildet kan man navigere seg frem til den mappen hvor filen tidligere er lagret og skru på radioknappen “Show deleted data” (Figur 1)\n\n\n\nFigur 1: Skru på visning av slettede filer\n\n\nNå vil man kunne se slettede filer i kursiv med teksten (Deleted) på slutten. Kolonnen “Version history” vil også vise hvor mange tidligere versjoner som finnes av denne filen. Trykk på filnavnet du ønsker å gjenopprette og velg deretter fanen “Version history”. I listen av versjoner til denne filen har man mulighet til å gjenopprette til en tidligere versjon ved å klikke på “Restore” (Figur 2).\n\n\n\nFigur 2: Gjenoppretting av en slettet fil"
  },
  {
    "objectID": "gjenopprette-data.html#gjenopprette-en-fil-til-en-tidligere-versjon",
    "href": "gjenopprette-data.html#gjenopprette-en-fil-til-en-tidligere-versjon",
    "title": "Gjenopprette data fra bøtter",
    "section": "",
    "text": "Fra Cloud Storage skjermbildet kan man navigere seg frem til den mappen hvor filen er lagret, og trykke på filnavnet. Velg deretter fanen “Version history”. I listen av versjoner til denne filen har man mulighet til å gjenopprette til en tidligere versjon ved å klikke på “Restore” (Figur 3).\n\n\n\nFigur 3: Versjonshistorikk til en fil"
  },
  {
    "objectID": "statistikkbanken.html",
    "href": "statistikkbanken.html",
    "title": "Statistikkbanken",
    "section": "",
    "text": "Pakken “dapla-statbank-client” kan brukes til å overføre tabeller til Statistikkbanken fra Jupyterlab i prodsonen og på Dapla. Den henter også “filbeskrivelsen” som beskriver formen dataene skal ha når de sendes inn til Statistikkbanken. Og den kan også hente publiserte data fra Statistikkbanken. Pakken er en python-pakke som baserer seg på at dataene (deltabellene) lastes inn i en eller flere pandas DataFrames før overføring. Ved å hente ned “filbeskrivelsen” kan man validere dataene sine (dataframene) mot denne lokalt, uten å sende dataene til Statistikkbanken. Dette kan være til hjelp under setting av formen på dataene. Å hente publiserte data fra Statistikkbanken kan gjøres gjennom løse funksjoner, eller via “klienten”.\nLenker: - Pakken ligger her på Pypi. Og kan installeres via poetry med: poetry add dapla-statbank-client - Kodebasen for pakken ligger her, readme-en gir en teknisk innføring som du kan følge og kopiere kode fra, og om du finner noe du vil rapportere om bruken av pakken så gjør det gjerne under “issues” på github-sidene. - Noe demokode ligger i repoet, og kan være ett godt utgangspunkt å kopiere og endre fra.\n\n\nStatistikkbanken opererer med ulike databaser for testing og produksjonssetting. Om du vil at tallene faktisk skal oppdateres på nettsidene så må du sende til Statistikkbankens “PROD”-database. Om du kun vil teste innsending skal du sende til databasen “TEST”. Disse databasene har de samme lastebrukerne, men passordene er ulike. Om du vil teste innsending må du derfor skaffe deg “test-passordet” til den lastebrukeren som du har tilgjengelig. For å gjøre tester via pakken må du være i staging på dapla: https://jupyter.dapla-staging.ssb.no/ eller staging i prodosonen: https://sl-jupyter-t.ssb.no/ Om du faktisk vil sende inn data til publisering i Statistikkbanken må du være i dapla-prod (den vanlige): https://jupyter.dapla.ssb.no/ eller i prodsonen på: https://sl-jupyter-p.ssb.no/ For å teste er det fint å skaffe seg noe data fra fjorårets publisering på et produksjonsløp man kjenner fra før. Evt. kan man hente data fra Statistikkbanken og sende disse tilbake snudd rett, og med riktig antall prikke-kolonner lagt til.\n\n\n\nSe mer detaljer i readme-en på prosjektets kodebase.\n\n\nFor å kunne bruke pakken må du importere klienten:\nfrom statbank import StatbankClient\nSå initialiserer du klienten med de innstillingene som oftest er faste på tvers av alle innsendingene fra ett produksjonsløp:\nstatcli = StatbankClient(loaduser=\"LAST360\", date=\"2050-01-01\", overwrite=True, approve=2)\nHer vil du bli bedt om å skrive inn passordet til lastebrukeren.\n\n\n\nOm du har dataene klare er det bare å overføre, men du må vite navnet på deltabell-dat-filene. (Statistikkbanken lagrer disse ned igjen som dat-filer.)\ndata_07495 = {\"kargrs01fylker1.dat\" : df_07495_fylker,\n              \"kargrs01landet1.dat\" : df_07495_landet,}\nstatcli.transfer(data_07495, tableid=\"07495\")\nEtter innsending kommer det en link til Statistikkbankens GUI for å følge med på om innsendingen gikk bra hos dem. Om det var det du ønsket, så er du nå ferdig… Men det finnes mer funksjonalitet her…\n\n\n\nFor å hente filbeskrivelsen til en hovedtabell bruker du denne metoden under klienten:\nfilbeskrivelse = statcli.get_description(tableid=\"07495\")\nprint(filbeskrivelse)\nMed filbeskrivelsen kan du lett få en mal på dictionaryet du må plassere dataene i:\nfilbeskrivelse.transferdata_template()\nDu kan også validere dataene dine mot filbeskrivelsen:\nfilbeskrivelse.validate(data_07495, tableid=\"07495\")\n\n\n\n\nDet tas noe ekstra ansvar i pakken rundt avrunding av desimaltall, da filbeskrivelsen inneholder informasjon om hvor mange desimaler som blir lagret per kolonne, kan vi runde av til rett antall samtidig som vi tar ett bevisst valg rundt “hvilken vei vi skal runde av”. På barneskolen lærte vi at ved 2,5 avrundet til 0 desimaler, så runder vi opp til 3. Det samme skjer i utgangspunktet i SAS og Excel. Python og R runder derimot “mot nærmeste partall”, så fra 2,5 blir det rundet til 2, men fra 1,5 blir det også rundet til 2. Dette er for å forhindre bias i en retning. Dvs. om alle tall rundes opp, vil en hel kolonne med tall “dras oppover”, ved å gjøre annenhver opp og ned, vil ikke helheten bli “dratt en spesifikk vei”. Siden “round to even” ikke er det folk er vandte til, gjør vi derfor noe annet i denne pakken, enn det som er vanlig oppførsel i Python. Vi runder opp. Om du bruker følgende metoden under filbeskrivelsen på dataene, så vil denne runde oppover, samtidig som den konverterer til en streng for å bevare formateringen.\ndata_07495 = filbeskrivelse.round_data(data_07495)  # For å ta vare på endringene, så må du skrive tilbake over variabelen\n\n\n\n\nEn date-widget for å visuelt endre til en valid dato.\nLagring av overføring og filbeskrivelses-objekter til json-filer\nUthenting av logg fra klienten\nHenting av publisert data fra statbanken\nRotering av tabeller fra statbanken"
  },
  {
    "objectID": "statistikkbanken.html#testoverføring-fra-staging---faktisk-oppdatering-fra-prod",
    "href": "statistikkbanken.html#testoverføring-fra-staging---faktisk-oppdatering-fra-prod",
    "title": "Statistikkbanken",
    "section": "",
    "text": "Statistikkbanken opererer med ulike databaser for testing og produksjonssetting. Om du vil at tallene faktisk skal oppdateres på nettsidene så må du sende til Statistikkbankens “PROD”-database. Om du kun vil teste innsending skal du sende til databasen “TEST”. Disse databasene har de samme lastebrukerne, men passordene er ulike. Om du vil teste innsending må du derfor skaffe deg “test-passordet” til den lastebrukeren som du har tilgjengelig. For å gjøre tester via pakken må du være i staging på dapla: https://jupyter.dapla-staging.ssb.no/ eller staging i prodosonen: https://sl-jupyter-t.ssb.no/ Om du faktisk vil sende inn data til publisering i Statistikkbanken må du være i dapla-prod (den vanlige): https://jupyter.dapla.ssb.no/ eller i prodsonen på: https://sl-jupyter-p.ssb.no/ For å teste er det fint å skaffe seg noe data fra fjorårets publisering på et produksjonsløp man kjenner fra før. Evt. kan man hente data fra Statistikkbanken og sende disse tilbake snudd rett, og med riktig antall prikke-kolonner lagt til."
  },
  {
    "objectID": "statistikkbanken.html#kode-eksempler",
    "href": "statistikkbanken.html#kode-eksempler",
    "title": "Statistikkbanken",
    "section": "",
    "text": "Se mer detaljer i readme-en på prosjektets kodebase.\n\n\nFor å kunne bruke pakken må du importere klienten:\nfrom statbank import StatbankClient\nSå initialiserer du klienten med de innstillingene som oftest er faste på tvers av alle innsendingene fra ett produksjonsløp:\nstatcli = StatbankClient(loaduser=\"LAST360\", date=\"2050-01-01\", overwrite=True, approve=2)\nHer vil du bli bedt om å skrive inn passordet til lastebrukeren.\n\n\n\nOm du har dataene klare er det bare å overføre, men du må vite navnet på deltabell-dat-filene. (Statistikkbanken lagrer disse ned igjen som dat-filer.)\ndata_07495 = {\"kargrs01fylker1.dat\" : df_07495_fylker,\n              \"kargrs01landet1.dat\" : df_07495_landet,}\nstatcli.transfer(data_07495, tableid=\"07495\")\nEtter innsending kommer det en link til Statistikkbankens GUI for å følge med på om innsendingen gikk bra hos dem. Om det var det du ønsket, så er du nå ferdig… Men det finnes mer funksjonalitet her…\n\n\n\nFor å hente filbeskrivelsen til en hovedtabell bruker du denne metoden under klienten:\nfilbeskrivelse = statcli.get_description(tableid=\"07495\")\nprint(filbeskrivelse)\nMed filbeskrivelsen kan du lett få en mal på dictionaryet du må plassere dataene i:\nfilbeskrivelse.transferdata_template()\nDu kan også validere dataene dine mot filbeskrivelsen:\nfilbeskrivelse.validate(data_07495, tableid=\"07495\")"
  },
  {
    "objectID": "statistikkbanken.html#problematikk-knyttet-til-avrunding",
    "href": "statistikkbanken.html#problematikk-knyttet-til-avrunding",
    "title": "Statistikkbanken",
    "section": "",
    "text": "Det tas noe ekstra ansvar i pakken rundt avrunding av desimaltall, da filbeskrivelsen inneholder informasjon om hvor mange desimaler som blir lagret per kolonne, kan vi runde av til rett antall samtidig som vi tar ett bevisst valg rundt “hvilken vei vi skal runde av”. På barneskolen lærte vi at ved 2,5 avrundet til 0 desimaler, så runder vi opp til 3. Det samme skjer i utgangspunktet i SAS og Excel. Python og R runder derimot “mot nærmeste partall”, så fra 2,5 blir det rundet til 2, men fra 1,5 blir det også rundet til 2. Dette er for å forhindre bias i en retning. Dvs. om alle tall rundes opp, vil en hel kolonne med tall “dras oppover”, ved å gjøre annenhver opp og ned, vil ikke helheten bli “dratt en spesifikk vei”. Siden “round to even” ikke er det folk er vandte til, gjør vi derfor noe annet i denne pakken, enn det som er vanlig oppførsel i Python. Vi runder opp. Om du bruker følgende metoden under filbeskrivelsen på dataene, så vil denne runde oppover, samtidig som den konverterer til en streng for å bevare formateringen.\ndata_07495 = filbeskrivelse.round_data(data_07495)  # For å ta vare på endringene, så må du skrive tilbake over variabelen"
  },
  {
    "objectID": "statistikkbanken.html#annen-funksjonalitet-du-kan-lese-om-i-readme-en-er",
    "href": "statistikkbanken.html#annen-funksjonalitet-du-kan-lese-om-i-readme-en-er",
    "title": "Statistikkbanken",
    "section": "",
    "text": "En date-widget for å visuelt endre til en valid dato.\nLagring av overføring og filbeskrivelses-objekter til json-filer\nUthenting av logg fra klienten\nHenting av publisert data fra statbanken\nRotering av tabeller fra statbanken"
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Forord",
    "section": "",
    "text": "Denne boken tar sikte på å gi SSB-ansatte mulighet til å ta i bruk grunnleggende funksjonalitet på DAPLA uten hjelp fra eksperter. Boken er bygget opp som den reisen vi mener en statistikker skal gjennom når de flytter sin produksjon fra bakke til sky1. Første del inneholder en del grunnleggende kunnskap som vi mener er viktig å ha før man skal starte å jobbe i skyen. Andre del forklarer hvordan man søker om å opprette et Dapla-team, en forutsetning for å drive databehandling på plattformen. Det vil ofte være første steget i ta i bruk plattformen, siden det er slik man får et sted å lagre data. Her forklarer vi hvilke tjenester som inkluderes i et statistikkteam og hvordan man bruker og administerer dem. Den tredje delen tar utgangspunkt i at man skal starte å kode opp sin statistikkproduksjon eller kjøre eksisterende kode. ssb-project er et verktøy som er utviklet i SSB for å gjøre denne prosessen så enkel som mulig. Da kan brukerne implementere det som anses som god praksis i SSB med noen få tastetrykk, samtidig som vi også forklarer mer detaljert hva som skjer under panseret.\nDet er tilrettelagt for en treningsarena i bakkemiljøet. Dette miljøet er nesten identisk med det som møter deg på Dapla, med unntak av at du her har tilgang til mange av de gamle systemene og mye mindre hestekrefter i maskinene. Ideen er at SSB-ere ofte vil ønske å lære seg de nye verktøyene2 i kjente og kjære omgivelser først, og deretter flytte et ferdig skrevet produksjonsløp til Dapla. Del 4 av denne boken beskriver mer utfyllende hvordan dette miljøet skiller seg fra Dapla, og hvordan man gjør en del vanlige operasjoner mot de gamle bakkesystemene.\nSiste delen av boken kaller vi Avansert og tar for seg ulike emner som mer avanserte brukere typisk trenger informasjon om. Her finner man blant annet informasjon om hvilke databaser man kan bruke og hvilke formål de er egnet for. Her beskrives også hvordan man kan bruke andre IDE-er enn Jupyterlab hvis man ønsker det. Tjenester for schedulerte kjøringer av Notebooks blir også diskutert.\nForhåpentligvis senker denne boken terskelen for å ta i bruk Dapla. Kommentarer og ønsker vedrørende boken tas imot med åpne armer.\nGod fornøyelse😁"
  },
  {
    "objectID": "preface.html#footnotes",
    "href": "preface.html#footnotes",
    "title": "Forord",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI denne boken omtaler vi den gamle produksjonssonen, ofte kalt prodsonen, som bakke, og det nye skymiljøet Google Cloud som sky. Det er ikke helt presist men duger for formålene i denne boken.↩︎\nDet som omtales som nye verktøy er vil som regel bety R, Python, Git, GitHub og Jupyterlab.↩︎"
  },
  {
    "objectID": "datatilstander.html",
    "href": "datatilstander.html",
    "title": "Datatilstander",
    "section": "",
    "text": "Datatilstander\n\nDet er naturlig at hovedfokus i SSBs kvalitetsarbeid er rettet mot statistikkene. Samtidig har bådeforventninger og krav til SSBs evne til å dele data økt betydelig de senere år. Det betyr at i tillegg til å produsere hovedproduktet «statistikk», så vil mange statistikkteam ha økt fokus på å produseregjenbrukbare datasett med høy kvalitet. En viktig forutsetning for gjenbruk er at de som vil brukedataene, kan vite hvilke endringer dataene har gjennomgått. Det må også være mulig for andre å finne og forstå dataene. Kvalitetssikret bruk av data i SSB og gjenbruk i og utenfor SSB fordrer godemetadata. Definisjoner av datatilstander og andre statistikkbegreper må derfor i størst mulig gradvære avstemt med internasjonale statistiske rammeverk og definisjoner.\nBegrepet «etterprøvbarhet» brukes flere steder i notatet, og det legges til grunn at vi bør ha som et ideal å produsere statistikk på en slik måte at ettertiden eller en uavhengig instans med tilgang til dataene og vår dokumentasjon vil komme til samme statistiske resultater som oss selv.\nTilstandene som beskrives er kildedata, inndata, klargjorte data, statistikk og utdata. De tre første er i hovedsak mikrodata som gir informasjon om enkeltenheter, mens statistikk og utdata i hovedsak er aggregerte data.\n\n(Standardutvalget 2021, 5)\n\n\n\n\n\nReferanser\n\nStandardutvalget. 2021. “Datatilstander i SSB.” Statistisk sentralbyrå. https://ssbno.sharepoint.com/sites/Internedokumenter/Delte%20dokumenter/Forms/AllItems.aspx?id=%2Fsites%2FInternedokumenter%2FDelte%20dokumenter%2FInterne%20dokumenter%202021%2F2021%2D17%20Datatilstander%20i%20SSB%20%2Epdf&parent=%2Fsites%2FInternedokumenter%2FDelte%20dokumenter%2FInterne%20dokumenter%202021."
  },
  {
    "objectID": "slette-data.html",
    "href": "slette-data.html",
    "title": "Slette data fra bøtter",
    "section": "",
    "text": "Slette data fra bøtter\nSletting av filer og mapper fra bøtter kan gjøres fra Google Cloud Console. Søk opp “Cloud Storage” i søkefeltet og klikk på den bøtten hvor filen er lagret under “Buckets”.\nKryss av filen/katalogen som du ønsker å slette og trykk “Delete” (Figur 1)\n\n\n\nFigur 1: Sletting av en fil\n\n\nSiden bøtter på Dapla har versjonering får man opp en dialogboks som informerer om at objektet (dvs. filen) er versjonert (Figur 2). Trykk på “Delete”.\n\n\n\nFigur 2: Bekreft sletting av fil\n\n\nSlettingen kan ta noe tid. Når denne er ferdig vil filen være slettet, men den kan fortsatt gjenopprettes. Hvis du ønsker at filen skal slettes permanent, gjør følgende:\n\nSkru på visning av slettede filer med å bruke radioknappen “Show deleted data” (Figur 3)\n\n\n\n\nFigur 3: Skru på visning av slettede filer\n\n\n\nFinn frem til den slettede filen og trykk på linken “1 noncurrent version” eller tilsvarende (Figur 4). Dette vil ta deg direkte til en side som viser filens versjonshistorikk.\n\n\n\n\nFigur 4: Velg versjonshistorikk\n\n\n\nVelg alle versjoner som vist på Figur 5 og trykk “Delete”\n\n\n\n\nFigur 5: Slett alle versioner\n\n\n\nTil slutt må man bekrefte at man ønsker å slette alle versioner (Figur 6) med å skrive inn DELETE og trykke på den blå “Delete”-knappen:\n\n\n\n\nFigur 6: Bekreft sletting av alle versjoner"
  },
  {
    "objectID": "opprette-dapla-team.html",
    "href": "opprette-dapla-team.html",
    "title": "Opprette Dapla-team",
    "section": "",
    "text": "Opprette Dapla-team\nFor å komme i gang med å opprette et Dapla-team trengs det en oversikt over teamets medlemmer og hvilke tilgangsgrupper medlemmene skal være med i. Det trengs også informasjon om hvilke Dapla-tjenester som er aktuelle for teamet å ta i bruk. Derfor har det blitt opprettet en egen veileder for dette kalt Dapla Start.\n\n\n\n\n\n\nGå til Dapla Start for starte bestilling av et nytt Dapla-team.\n\n\n\nNår teamet er opprettet får alle medlemmene tilgang til sitt eget prosjekt i Google Cloud Platform (GCP), som er SSBs leverandør av skytjenester. Videre får hvert prosjekt et sett med tjenester og tilganger som knyttes til teamet. Det opprettes også datalagringsområder (kalt bøtter) som bare kan aksesseres av brukere som er med i teamets tilgangsgrupper.\nDapla-teamet vil også få sin egen gruppe i SSBs Active Directory slik at medlemskapet i gruppen kan administreres av Kundeservice."
  },
  {
    "objectID": "bakke-til-sky.html",
    "href": "bakke-til-sky.html",
    "title": "Fra bakke til sky",
    "section": "",
    "text": "Fra bakke til sky"
  },
  {
    "objectID": "arkivering.html",
    "href": "arkivering.html",
    "title": "Arkivering",
    "section": "",
    "text": "Arkivering"
  },
  {
    "objectID": "gcc.html",
    "href": "gcc.html",
    "title": "Google Cloud Console",
    "section": "",
    "text": "Google Cloud er SSBs leverandør av skytjenester som Dapla er bygget på.\nGoogle Cloud Console er et web-basert grensesnitt for å administrere ressurser og tjenester på Google Cloud. For å bruke denne må man ha en Google-konto. Alle i SSB har en konto knyttet opp mot Google.\n\n\n\n\n\n\nGå til Google Cloud Console og logg på med din SSB-bruker."
  },
  {
    "objectID": "gcc.html#prosjektvelger",
    "href": "gcc.html#prosjektvelger",
    "title": "Google Cloud Console",
    "section": "Prosjektvelger",
    "text": "Prosjektvelger\nØverst på siden, til høyre for teksten Google Cloud finnes det en prosjektvelger. Her er det viktig å velge ditt teams Google prosjekt, ettersom teamets ressurser kun er tilgjengelige innenfor prosjektet. Hvis du trykker på prosjektvelgeren vil det åpnes opp et nytt vindu. Sjekk at det står SSB.NO øverst i dette vinduet.(Figur 1).\n\n\n\nFigur 1: Prosjektvelgeren i Google Cloud Console\n\n\n\nVelg prosjekt\nHer vises det hvordan man velger et prosjekt I GCC. Eksempelet benytter Dapla teamet demo stat b og fortsetter fra (Figur 1).\n\nSkrive teamnavn i søkefeltet, resultatene burde se ut som i (Figur 2).\nTrykk på lenken prod-demo-stat-b, som markert med rød pil i (Figur 2).\n\n\n\n\n\n\n\nI ID kolonnen ser man prosjektet ID (Figur 2).\n\n\n\n\n\n\nFigur 2: Søk i Prosjektvelgeren\n\n\nHar man gjort alle stegene rett vil det i venstre hjørne se ut som i (Figur 3).\n\n\n\nFigur 3: Aktivt prosjekt i GCC"
  },
  {
    "objectID": "jobbe-med-data.html",
    "href": "jobbe-med-data.html",
    "title": "Jobbe med data",
    "section": "",
    "text": "Når vi oppretter et dapla-team så får vi tildelt et eget området for lagring av data. For å kunne lese og skrive data fra Jupyter til disse områdene må vi autentisere oss, siden Jupyter og lagringsområdet er to separate sikkerhetsoner.\nFigur 1 viser dette klarer skillet mellom hvor vi koder og hvor dataene ligger på Dapla1. I dette kapitlet beskriver vi nærmere hvordan du kan jobbe med dataene dine på Dapla.\n\n\n\nFigur 1: Tydelig skille mellom kodemiljø og datalagring på Dapla.\n\n\n\n\nFor å gjøre det enklere å jobbe data på tvers av Jupyter og lagringsområdet er det laget noen egne SSB-utviklede biblioteker for å gjøre vanlige operasjoner mot lagringsområdet. Siden både R og Python skal brukes på Dapla, så er det laget to biblioteker, en for hver av disse språkene. fellesr er biblioteket for R, og dapla-toolbelt er biblioteket for Python.\n\n\ndapla-toolbelt er en en pakke som lar deg enkelt lese og skrive til lagringsområdet uten å måtte autentifisere deg manuelt. Den har en Pandas-aktig syntaks som forhåpentlig er gjenkjennbar for de fleste. Pakken er installert i alle Python-kernels på Dapla, så du trenger ikke å installere den selv hvis du åpner en notebook med Python3 for eksempel. For å importere hele biblioteket i en notebook skriver du bare\nimport dapla as dp\ndapla-toolbelt bruker en pakke som heter gcsfs for å kommunisere med lagringsområdet. gcsfs er en pakke som lar deg bruke Google Cloud Storage (GCS) som om det var en filsystem. Det betyr at du kan bruke samme syntaks som du bruker for å lese og skrive til filer på din egen maskin. Du kan lese mulighetene i gcsfs her. Et eksempel på hvordan de to pakkene kan brukes sammen ser du her:\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Example of how you can use gcsfs and dapla-toolbelt together\nfs.mkdir(\"gs://my-bucket/my-folder\")\nI koden over brukte jeg kommandoen mkdir fra gcsfs og FileClient fra dapla-toolbelt for å opprette en mappe i lagringsområdet.\nI kapitlene under finner du konkrete eksempler på hvordan du kan bruke dapla-toolbelt til å jobbe med data i SSBs lagringsområdet.\n\n\n\nR-pakken fellesr er under utvikling og gir mye av den samme funksjonaliteten som dapla-toolbelt gir for Python. I tillegg til å kunne lese og skrive til lagringsområdet på Dapla, så har fellesr også funksjoner for å jobbe med metadata på Dapla.\nfellesr er foreløpig ikke tilgjengeliggjort som en pakke som kan installeres. For å bruke pakken kan du gjøre følgende:\n\nKopiere scriptet DAPLA_funcs.R og legg den i en fil sammen med Notebooken din\nI en R-notebook som ligger i samme mappe som filen DAPLA_funcs.R starter du med å skrive\n\nsource(\"DAPLA_funcs.R\")\nDa er alle funksjonene tilgjengelig for deg i Notebooken din.\n\n\n\n\n\n\n\n\n\n\nEksempeldata\n\n\n\nDet finnes et område som alle SSB-ansatte har lese- og skrivetilgang til. Det er\ngs://ssb-prod-dapla-felles-data-delt/ i prod-miljøet på Dapla, og\ngs://ssb-staging-dapla-felles-data-delt/ i staging-miljøet. Eksemplene under bruker førstnevnte i koden, slik at alle kan kjøre koden selv.\nKode-eksemplene finnes for både R og Python, og du kan velge hvilken du skal se ved å trykke på den arkfanen du er interessert i.\n\n\nÅ liste ut innhold i et gitt mappe på Dapla er ganske enkelt. Under ser du hvordan du kan liste ut innholdet i følgende mappe:\ngs://ssb-prod-dapla-felles-data-delt/felles/veiledning/python/eksempler/purchases\n\nPython \n\n\nVi bruker modulen FileClient fra dapla-toolbelt for å liste ut innholdet i en mappe.\nfrom dapla import FileClient\n\n# Set path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\nFileClient.ls(f\"{bucket}/{folder}\")\nMed kommandoen over får du listet ut alle filnavn i mappen. Hvis du vil ha mer informasjon om filene så kan du bruke ls-kommandoen med detail = True, som under:\nFileClient.ls(f\"{bucket}/{folder}\", detail = True)\nSyntaksen med ls er veldig lik det man kjenner fra en Linux-terminal. Men når vi bruker detail = True blir metadata om filene returnert som en Python-liste med dictionaries. Det kan være svært nyttig når du f.eks. trenger å vite dato og tidspunkt for når en fil ble opprettet, eller når den sist ble oppdatert.\n\n\n# Loading functions into notebook\nsource(\"DAPLA_funcs.R\")\n\n# Path to folder\nbucket &lt;- \"ssb-prod-dapla-felles-data-delt/\"\nfolder &lt;- \"felles/veiledning/python/eksempler/purchases\"\n\n# List files in folder \nlist.files(paste0(bucket, folder))\n\n\n\n\n\n\nÅ skrive filer til et lagringsområde på Dapla er også ganske enkelt. Det ligner mye på den syntaksen vi er kjent med fra vanlige R- og Python-pakker, med noen små unntak.\n\n\nUnder lager vi en dataframe i en notebook og skriver den ut til en parquet-fil. Stien vi skriver til er\ngs://ssb-prod-dapla-felles-data-delt/felles/veiledning/python/eksempler/purchases:\n\nPython \n\n\nNår vi leser en Parquet-fil med dapla-toolbelt så bruker den pyarrow i bakgrunnen. Dette er en av de raskeste måtene å lese og skrive Parquet-filer på.\n\nimport dapla as dp\nimport pandas as pd\nimport numpy as np\n\n# Set path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Create pandas dataframe\npurchases = pd.DataFrame(np.random.randn(10, 5),\n                        columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Write pandas dataframe as parquet to GCS bucket\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{bucket}/{folder}/data.parquet\",\n                file_format = \"parquet\",)\nNår vi kalte write_pandas over så spesifiserte vi at filformatet skulle være parquet. Dette er default, så vi kunne også ha skrevet det slik:\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{bucket}/{folder}/data.parquet\")\nMen for de andre filformatene må vi altså spesifisere dette.\n\n\nKommer snart\n\n\n\n\n\n\nKommer snart eksempler på hvordan man kan skrive ut tekstfiler som CSV, JSON og XML.\n\nPython \n\n\ndapla-toolbelt kan skrive ut json, csv og posisjonsfiler (fixed-width-files/fwf) til lagringsområdet. Måten den gjør det på er å bruke Pandas sine funksjoner read_json, read_csv, read_fwf under panseret. Dette kan være nyttig å vite for skjønne hvordan dapla-toolbelt håndterer ulike strukturer i (spesielt hierarkiske) tekstfiler. Under ser du hvordan du kan skrive ut en dataframe til en json-fil.\nimport numpy as np\nimport pandas as pd\nfrom dapla import FileClient\n\n# Set path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Create a dataframe with Pandas\ndf = pd.DataFrame(np.random.randn(10, 5), columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Save dataframe as json with dapla-toolbelt\ndp.write_pandas(df = df,\n                gcs_path = f\"{bucket}/{folder}/test.json\",\n                file_format = \"json\")\nSom vi ser at syntaksen over så kunne vi skrevet ut til noe annet enn json ved å endre verdien i argumentet file_format.\n\n\nKommer snart.\n\n\n\n\n\n\nDet er ikke anbefalt å bruke xlsx-formatet, men her ser du hvordan det kan skrives ut. Mer kommer.\n\nPython \n\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Henter token for å kunne lese fra Dapla\ntoken = AuthClient.fetch_google_credentials()\n\n# Filsti\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\ndf.to_excel(f\"{bucket}/{folder}/test.xlsx\",\n           storage_options={\"token\": token})\n\n\nKommer snart\n\n\n\n\n\n\n\nÅ lese inn filer på med dapla-toolbelt er nesten like rett frem som med Pandas. Under finner du eksempler på hvordan du kan lese inn data til en Jupyter Notebooks på Dapla.\n\n\n\nPython \n\n\nimport dapla as dp\n\n# Set path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Read path into pandas dataframe \ndp.read_pandas(gcs_path= f\"{bucket}/{folder}/data.parquet\",\n               file_format = \"parquet\",\n               columns = None,)\nSom vi så med write_pandas så er file_format default satt til parquet, og default for columns = None, så vi kunne også ha skrevet det slik:\ndp.read_pandas(gcs_path= f\"{bucket}/{folder}/data.parquet\")\ncolumns-argumentet er en liste med kolonnenavn som vi ønsker å lese inn. Hvis vi ikke spesifiserer noen kolonner så vil alle kolonnene leses inn.\n\n\nKommer snart\n\n\n\n\n\n\nKommer mer snart. Python-koden under bygger på eksempelet over.\n\nPython \n\n\nimport dapla as dp\n\n# Path to write to\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Read in json-file from dapla-storage\ndf = dp.read_pandas(gcs_path = f\"{bucket}/{folder}/test3.json\",\n               file_format = \"json\")\n\n\nKommer snart\n\n\n\n\n\n\n\nPython \n\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Hent token\ntoken = AuthClient.fetch_google_credentials()\n\n# Les inn fil\ndf = pd.read_excel(\"gs://ssb-prod-arbmark-skjema-data-produkt/test_gcp.xlsx\",\n    storage_options={\"token\": token})\n\n\nKommer snart\n\n\n\n\n\n\n\nÅ slette filer fra lagringsområdet kan gjøres på flere måter. I kapitlet om sletting av data viste vi hvordan man gjør det med pek-og-klikk i Google Cloud Console. Under ser du hvordan du kan slette filer med dapla-toolbelt og gcsfs.\n\nPython \n\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\nfs.rm(f\"{bucket}/{from_folder}/df.json\")\n\n\nKommer snart\n\n\n\n\n\n\nÅ kopiere filer mellom mapper på et Linux-filsystem innebærer som regel bruke cp-kommandoen. På Dapla er det ikke så mye forskjell. Vi bruker en ligende tilnærming nå vi skal kopiere mellom bøtter eller mapper på lagringsområdet til SSB. Under ser du hvordan du kan kopiere en fil fra en mappe til en annen.\n\nPython \n\n\nLa oss begynne med et eksempel der vi kopierer en fil fra en mappe til en annen i samme bøtte.\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Path to folders\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\n# Copy file\nfs.cp(f\"{bucket}/{from_folder}/data.parquet\",\n      f\"{bucket}/{to_folder}/data_copy.parquet\")\nDet også fungere for å kopiere filer mellom bøtter.\nEt annet scenario vi ofte vil støte på er at vi ønsker å kopiere en fil fra vår Jupyter-filsystem til en mappe på lagringsområdet. Her kan vi bruke fs.put-metoden.\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Create a new file in your home directory called test.txt\nwith open('/home/jovyan/test.txt', 'w') as f:\n    f.write('Create a new text file!')\n\n#Path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Copy file from local to remote file system\nfs.put(lpath=f\"/home/jovyan/test.txt\", rpath=f\"{bucket}/{folder}/test.txt\")\nØnsker vi å kopiere en hel mappe fra lagringsområdet til Jupyter-filsystemet, kan vi bruke fs.get-metoden, med opsjonen recursive=True.\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Copy file\nfs.get(&lt;from_bucket&gt;,\n      \"/home/jovyan/sesongjustering/\",\n      recursive=True)\n\n\nKommer snart\n\n\n\n\n\n\n\nPython \n\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\nfs.mv(f\"{bucket}/{from_folder}/data.parquet\", f\"{bucket}/{to_folder}/data.parquet\")\n\n\nKommer snart\n\n\n\n\n\n\n\nPython \n\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n#Path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Create folder\nfs.mkdir(f\"{bucket}/{folder}/testmappe/\")\n\n\nKommer snart"
  },
  {
    "objectID": "jobbe-med-data.html#ssb-biblioteker",
    "href": "jobbe-med-data.html#ssb-biblioteker",
    "title": "Jobbe med data",
    "section": "",
    "text": "For å gjøre det enklere å jobbe data på tvers av Jupyter og lagringsområdet er det laget noen egne SSB-utviklede biblioteker for å gjøre vanlige operasjoner mot lagringsområdet. Siden både R og Python skal brukes på Dapla, så er det laget to biblioteker, en for hver av disse språkene. fellesr er biblioteket for R, og dapla-toolbelt er biblioteket for Python.\n\n\ndapla-toolbelt er en en pakke som lar deg enkelt lese og skrive til lagringsområdet uten å måtte autentifisere deg manuelt. Den har en Pandas-aktig syntaks som forhåpentlig er gjenkjennbar for de fleste. Pakken er installert i alle Python-kernels på Dapla, så du trenger ikke å installere den selv hvis du åpner en notebook med Python3 for eksempel. For å importere hele biblioteket i en notebook skriver du bare\nimport dapla as dp\ndapla-toolbelt bruker en pakke som heter gcsfs for å kommunisere med lagringsområdet. gcsfs er en pakke som lar deg bruke Google Cloud Storage (GCS) som om det var en filsystem. Det betyr at du kan bruke samme syntaks som du bruker for å lese og skrive til filer på din egen maskin. Du kan lese mulighetene i gcsfs her. Et eksempel på hvordan de to pakkene kan brukes sammen ser du her:\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Example of how you can use gcsfs and dapla-toolbelt together\nfs.mkdir(\"gs://my-bucket/my-folder\")\nI koden over brukte jeg kommandoen mkdir fra gcsfs og FileClient fra dapla-toolbelt for å opprette en mappe i lagringsområdet.\nI kapitlene under finner du konkrete eksempler på hvordan du kan bruke dapla-toolbelt til å jobbe med data i SSBs lagringsområdet.\n\n\n\nR-pakken fellesr er under utvikling og gir mye av den samme funksjonaliteten som dapla-toolbelt gir for Python. I tillegg til å kunne lese og skrive til lagringsområdet på Dapla, så har fellesr også funksjoner for å jobbe med metadata på Dapla.\nfellesr er foreløpig ikke tilgjengeliggjort som en pakke som kan installeres. For å bruke pakken kan du gjøre følgende:\n\nKopiere scriptet DAPLA_funcs.R og legg den i en fil sammen med Notebooken din\nI en R-notebook som ligger i samme mappe som filen DAPLA_funcs.R starter du med å skrive\n\nsource(\"DAPLA_funcs.R\")\nDa er alle funksjonene tilgjengelig for deg i Notebooken din."
  },
  {
    "objectID": "jobbe-med-data.html#liste-ut-innhold-i-mappe",
    "href": "jobbe-med-data.html#liste-ut-innhold-i-mappe",
    "title": "Jobbe med data",
    "section": "",
    "text": "Eksempeldata\n\n\n\nDet finnes et område som alle SSB-ansatte har lese- og skrivetilgang til. Det er\ngs://ssb-prod-dapla-felles-data-delt/ i prod-miljøet på Dapla, og\ngs://ssb-staging-dapla-felles-data-delt/ i staging-miljøet. Eksemplene under bruker førstnevnte i koden, slik at alle kan kjøre koden selv.\nKode-eksemplene finnes for både R og Python, og du kan velge hvilken du skal se ved å trykke på den arkfanen du er interessert i.\n\n\nÅ liste ut innhold i et gitt mappe på Dapla er ganske enkelt. Under ser du hvordan du kan liste ut innholdet i følgende mappe:\ngs://ssb-prod-dapla-felles-data-delt/felles/veiledning/python/eksempler/purchases\n\nPython \n\n\nVi bruker modulen FileClient fra dapla-toolbelt for å liste ut innholdet i en mappe.\nfrom dapla import FileClient\n\n# Set path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\nFileClient.ls(f\"{bucket}/{folder}\")\nMed kommandoen over får du listet ut alle filnavn i mappen. Hvis du vil ha mer informasjon om filene så kan du bruke ls-kommandoen med detail = True, som under:\nFileClient.ls(f\"{bucket}/{folder}\", detail = True)\nSyntaksen med ls er veldig lik det man kjenner fra en Linux-terminal. Men når vi bruker detail = True blir metadata om filene returnert som en Python-liste med dictionaries. Det kan være svært nyttig når du f.eks. trenger å vite dato og tidspunkt for når en fil ble opprettet, eller når den sist ble oppdatert.\n\n\n# Loading functions into notebook\nsource(\"DAPLA_funcs.R\")\n\n# Path to folder\nbucket &lt;- \"ssb-prod-dapla-felles-data-delt/\"\nfolder &lt;- \"felles/veiledning/python/eksempler/purchases\"\n\n# List files in folder \nlist.files(paste0(bucket, folder))"
  },
  {
    "objectID": "jobbe-med-data.html#skrive-ut-filer",
    "href": "jobbe-med-data.html#skrive-ut-filer",
    "title": "Jobbe med data",
    "section": "",
    "text": "Å skrive filer til et lagringsområde på Dapla er også ganske enkelt. Det ligner mye på den syntaksen vi er kjent med fra vanlige R- og Python-pakker, med noen små unntak.\n\n\nUnder lager vi en dataframe i en notebook og skriver den ut til en parquet-fil. Stien vi skriver til er\ngs://ssb-prod-dapla-felles-data-delt/felles/veiledning/python/eksempler/purchases:\n\nPython \n\n\nNår vi leser en Parquet-fil med dapla-toolbelt så bruker den pyarrow i bakgrunnen. Dette er en av de raskeste måtene å lese og skrive Parquet-filer på.\n\nimport dapla as dp\nimport pandas as pd\nimport numpy as np\n\n# Set path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Create pandas dataframe\npurchases = pd.DataFrame(np.random.randn(10, 5),\n                        columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Write pandas dataframe as parquet to GCS bucket\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{bucket}/{folder}/data.parquet\",\n                file_format = \"parquet\",)\nNår vi kalte write_pandas over så spesifiserte vi at filformatet skulle være parquet. Dette er default, så vi kunne også ha skrevet det slik:\ndp.write_pandas(df = purchases,\n                gcs_path = f\"{bucket}/{folder}/data.parquet\")\nMen for de andre filformatene må vi altså spesifisere dette.\n\n\nKommer snart\n\n\n\n\n\n\nKommer snart eksempler på hvordan man kan skrive ut tekstfiler som CSV, JSON og XML.\n\nPython \n\n\ndapla-toolbelt kan skrive ut json, csv og posisjonsfiler (fixed-width-files/fwf) til lagringsområdet. Måten den gjør det på er å bruke Pandas sine funksjoner read_json, read_csv, read_fwf under panseret. Dette kan være nyttig å vite for skjønne hvordan dapla-toolbelt håndterer ulike strukturer i (spesielt hierarkiske) tekstfiler. Under ser du hvordan du kan skrive ut en dataframe til en json-fil.\nimport numpy as np\nimport pandas as pd\nfrom dapla import FileClient\n\n# Set path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Create a dataframe with Pandas\ndf = pd.DataFrame(np.random.randn(10, 5), columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\n# Save dataframe as json with dapla-toolbelt\ndp.write_pandas(df = df,\n                gcs_path = f\"{bucket}/{folder}/test.json\",\n                file_format = \"json\")\nSom vi ser at syntaksen over så kunne vi skrevet ut til noe annet enn json ved å endre verdien i argumentet file_format.\n\n\nKommer snart.\n\n\n\n\n\n\nDet er ikke anbefalt å bruke xlsx-formatet, men her ser du hvordan det kan skrives ut. Mer kommer.\n\nPython \n\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Henter token for å kunne lese fra Dapla\ntoken = AuthClient.fetch_google_credentials()\n\n# Filsti\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\ndf.to_excel(f\"{bucket}/{folder}/test.xlsx\",\n           storage_options={\"token\": token})\n\n\nKommer snart"
  },
  {
    "objectID": "jobbe-med-data.html#lese-inn-filer",
    "href": "jobbe-med-data.html#lese-inn-filer",
    "title": "Jobbe med data",
    "section": "",
    "text": "Å lese inn filer på med dapla-toolbelt er nesten like rett frem som med Pandas. Under finner du eksempler på hvordan du kan lese inn data til en Jupyter Notebooks på Dapla.\n\n\n\nPython \n\n\nimport dapla as dp\n\n# Set path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Read path into pandas dataframe \ndp.read_pandas(gcs_path= f\"{bucket}/{folder}/data.parquet\",\n               file_format = \"parquet\",\n               columns = None,)\nSom vi så med write_pandas så er file_format default satt til parquet, og default for columns = None, så vi kunne også ha skrevet det slik:\ndp.read_pandas(gcs_path= f\"{bucket}/{folder}/data.parquet\")\ncolumns-argumentet er en liste med kolonnenavn som vi ønsker å lese inn. Hvis vi ikke spesifiserer noen kolonner så vil alle kolonnene leses inn.\n\n\nKommer snart\n\n\n\n\n\n\nKommer mer snart. Python-koden under bygger på eksempelet over.\n\nPython \n\n\nimport dapla as dp\n\n# Path to write to\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\n# Read in json-file from dapla-storage\ndf = dp.read_pandas(gcs_path = f\"{bucket}/{folder}/test3.json\",\n               file_format = \"json\")\n\n\nKommer snart\n\n\n\n\n\n\n\nPython \n\n\nimport pandas as pd\nfrom dapla import AuthClient\n\n# Hent token\ntoken = AuthClient.fetch_google_credentials()\n\n# Les inn fil\ndf = pd.read_excel(\"gs://ssb-prod-arbmark-skjema-data-produkt/test_gcp.xlsx\",\n    storage_options={\"token\": token})\n\n\nKommer snart"
  },
  {
    "objectID": "jobbe-med-data.html#slette-filer",
    "href": "jobbe-med-data.html#slette-filer",
    "title": "Jobbe med data",
    "section": "",
    "text": "Å slette filer fra lagringsområdet kan gjøres på flere måter. I kapitlet om sletting av data viste vi hvordan man gjør det med pek-og-klikk i Google Cloud Console. Under ser du hvordan du kan slette filer med dapla-toolbelt og gcsfs.\n\nPython \n\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler/purchases\"\n\nfs.rm(f\"{bucket}/{from_folder}/df.json\")\n\n\nKommer snart"
  },
  {
    "objectID": "jobbe-med-data.html#kopiere-filer",
    "href": "jobbe-med-data.html#kopiere-filer",
    "title": "Jobbe med data",
    "section": "",
    "text": "Å kopiere filer mellom mapper på et Linux-filsystem innebærer som regel bruke cp-kommandoen. På Dapla er det ikke så mye forskjell. Vi bruker en ligende tilnærming nå vi skal kopiere mellom bøtter eller mapper på lagringsområdet til SSB. Under ser du hvordan du kan kopiere en fil fra en mappe til en annen.\n\nPython \n\n\nLa oss begynne med et eksempel der vi kopierer en fil fra en mappe til en annen i samme bøtte.\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Path to folders\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\n# Copy file\nfs.cp(f\"{bucket}/{from_folder}/data.parquet\",\n      f\"{bucket}/{to_folder}/data_copy.parquet\")\nDet også fungere for å kopiere filer mellom bøtter.\nEt annet scenario vi ofte vil støte på er at vi ønsker å kopiere en fil fra vår Jupyter-filsystem til en mappe på lagringsområdet. Her kan vi bruke fs.put-metoden.\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Create a new file in your home directory called test.txt\nwith open('/home/jovyan/test.txt', 'w') as f:\n    f.write('Create a new text file!')\n\n#Path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Copy file from local to remote file system\nfs.put(lpath=f\"/home/jovyan/test.txt\", rpath=f\"{bucket}/{folder}/test.txt\")\nØnsker vi å kopiere en hel mappe fra lagringsområdet til Jupyter-filsystemet, kan vi bruke fs.get-metoden, med opsjonen recursive=True.\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n# Copy file\nfs.get(&lt;from_bucket&gt;,\n      \"/home/jovyan/sesongjustering/\",\n      recursive=True)\n\n\nKommer snart"
  },
  {
    "objectID": "jobbe-med-data.html#flytte-filer",
    "href": "jobbe-med-data.html#flytte-filer",
    "title": "Jobbe med data",
    "section": "",
    "text": "Python \n\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfrom_folder = \"felles/veiledning/python/eksempler/purchases\"\nto_folder = \"felles/veiledning/python/eksempler\"\n\nfs.mv(f\"{bucket}/{from_folder}/data.parquet\", f\"{bucket}/{to_folder}/data.parquet\")\n\n\nKommer snart"
  },
  {
    "objectID": "jobbe-med-data.html#opprette-mapper",
    "href": "jobbe-med-data.html#opprette-mapper",
    "title": "Jobbe med data",
    "section": "",
    "text": "Python \n\n\nfrom dapla import FileClient\nfs = FileClient.get_gcs_file_system()\n\n#Path to folder\nbucket = \"gs://ssb-prod-dapla-felles-data-delt\"\nfolder = \"felles/veiledning/python/eksempler\"\n\n# Create folder\nfs.mkdir(f\"{bucket}/{folder}/testmappe/\")\n\n\nKommer snart"
  },
  {
    "objectID": "jobbe-med-data.html#footnotes",
    "href": "jobbe-med-data.html#footnotes",
    "title": "Jobbe med data",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nI de tidligere systemene på bakken så var det ikke nødvendig med autentisering mellom kodemiljø og datalagringen↩︎"
  },
  {
    "objectID": "dapla-team.html",
    "href": "dapla-team.html",
    "title": "Dapla Team",
    "section": "",
    "text": "Dapla Team\nI forrige del beskrev vi noen grunnleggende ting rundt Dapla. I denne delen tar vi for oss hvordan du kan begynne å jobbe med skarpe data på plattformen.\nKapittelet som beskriver hvordan man logger seg inn på Dapla vil fungere uten at du må gjøre noen forberedelser. Er man koblet på SSB sitt nettverk så vil alle SSB-ansatte kunne gå inn på plattformen og kode i Python og R. Men du får ikke tilgang til SSBs område for datalagring på plattformen. I praksis vil det si at man kan generere data med kode, men man kan ikke jobbe med skarpe data.\nFor å få muligheten til å jobbe med skarpe data MÅ du først opprette et dapla-team. Dette er det første naturlige steget å ta når man skal begynne å jobbe med statistikkproduksjon på Dapla. I dette kapittelet vil vi forklare det du trenger å vite om det å opprette og jobbe innenfor et team."
  },
  {
    "objectID": "lage-nettsider.html",
    "href": "lage-nettsider.html",
    "title": "Lage nettsider",
    "section": "",
    "text": "Lage nettsider"
  },
  {
    "objectID": "dashboard.html",
    "href": "dashboard.html",
    "title": "Dash og dashboard",
    "section": "",
    "text": "Hvis en ønsker å lage ett dashbord som et brukergrensesnitt, så kan pakken Dash være et godt alternativ. Dash er ett rammeverk hvor man selv kan bygge opp applikasjoner i form av dashbord på en enklere måte, og det bygges oppå javascript pakker som plotly.js og react.js. Det er et produkt ved siden av og helintegrert med plotly, som også er en annen pakke i Python som gir oss interaktive grafer. Dash er et godt verktøy hvis en ønsker et dashbord som brukergrensesnitt for interaktiv visualisering av data. Dash kan kodes i Python og R, men også Julia og F#.\n\n\nI SSB kan man lage dashbord i virtuelle miljøer, gjerne satt opp med ssb-project, men man trenger helt spesifikke pakker for å få det oppe å gå. Mer info om å sette opp et eget miljø med ssb-project finner du her. Tabell under viser navn på pakkene, og gjeldende versjoner som er benyttet i skrivende stund av eksempel i neste avsnitt. Vel og merke kan nyere versjoner også fungere fint, noe man må prøve ut selv, men følgende versjoner fungerer fint.\n\n\n\nPakke\nVersjon (i skrivende stund)\n\n\n\n\ndash\n2.8.1\n\n\njupyter-dash\n0.4.2\n\n\njupyter-server-proxy\n3.2.2\n\n\njupyterlab-dash\n0.1.0a3\n\n\nipykernel\n6.21.3\n\n\ndash-bootstrap-components\n1.3.0 (hvis ønskelig)\n\n\n\nFor mer om håndtering av pakker i ett virtuelt miljø satt opp med ssb-project kan man se nærmere her. For å legge til disse pakkene kan man gjøre følgende i terminalen:\npoetry add dash\npoetry add jupyter-dash\npoetry add jupyter-server-proxy\npoetry add jupyterlab-dash\npoetry add ipykernel\nOg hvis en ønsker Dash-Bootstrap-Components:\npoetry add dash-bootstrap-components\nVel og merke så vil ikke denne pakken fungere uten at tilhørende filer er med i selve repoet. Dette fordi i hovedsak leter denne etter filer den trenger på internett. Pakken i seg selv har en fordel i at det er lettere å bygge opp utseende(layout) i dashbordet ditt, samt andre komponenter som ikke ligger i standard dash pakken.\n\n\n\nNoen ting er viktig å huske på at kommer i korrekt rekkefølge når en koder opp i JupyterLab. Her kommer ett veldig enkelt eksempel.\nFørste celle importerer vi alle nødvendige pakker\nfrom dash import html\nfrom jupyter_dash import JupyterDash\nfrom jupyter_dash.comms import _send_jupyter_config_comm_request\n_send_jupyter_config_comm_request()\nI Andre celle må følgende kjøres, men her er det VELDIG VIKTIG at man avventer 2-3 sekunder etter at forrige celle ble kjørt.\nJupyterDash.infer_jupyter_proxy_config()\nDeretter så er vi klare for å bygge opp selve dashbordet. så i Tredje celle kan en enkel kode for eksempel se slik ut:\napp = JupyterDash(__name__)\nporten = 8642 # Valgfritt fire sifret nummer\n\napp.layout = html.Div([\n    html.H1(\"Eget dashbord med Dash i SSB\")\n])\n\nif __name__ == \"__main__\":\n    app.run_server(debug=True, port=porten, mode=\"external\")\nDenne koden vil starte opp dashbordet i eget vindu i browseren ettersom mode=“external”. Dersom man vil ha et eget vindu i JupyterLab kan man sette denne til “jupyterlab”, eller hvis du vil at dashbordet skal dukke opp under cellen i notebooken din, så kan man sette denne til “inline”.\n\n\n\nDiverse som er verdt å se nærmere på når en bygger dashbord applikasjon med Dash. Det følger med mange gode eksempler for bruk av diverse komponenter i dokumentasjonene under her.\n\nStartsiden til Dash\nLage interaktive grafer i Python med Plotly\nDash Core Components\nDash HTML Components\nDash Bootstrap Components\n\nVel og merke henter denne diverse materialer fra internett, og vil ikke fungere uten tilgang. Det er heller anbefalt å ha de nødvendige filene lagret lokalt for bruk av denne pakken."
  },
  {
    "objectID": "dashboard.html#anbefalte-nødvendige-pakker",
    "href": "dashboard.html#anbefalte-nødvendige-pakker",
    "title": "Dash og dashboard",
    "section": "",
    "text": "I SSB kan man lage dashbord i virtuelle miljøer, gjerne satt opp med ssb-project, men man trenger helt spesifikke pakker for å få det oppe å gå. Mer info om å sette opp et eget miljø med ssb-project finner du her. Tabell under viser navn på pakkene, og gjeldende versjoner som er benyttet i skrivende stund av eksempel i neste avsnitt. Vel og merke kan nyere versjoner også fungere fint, noe man må prøve ut selv, men følgende versjoner fungerer fint.\n\n\n\nPakke\nVersjon (i skrivende stund)\n\n\n\n\ndash\n2.8.1\n\n\njupyter-dash\n0.4.2\n\n\njupyter-server-proxy\n3.2.2\n\n\njupyterlab-dash\n0.1.0a3\n\n\nipykernel\n6.21.3\n\n\ndash-bootstrap-components\n1.3.0 (hvis ønskelig)\n\n\n\nFor mer om håndtering av pakker i ett virtuelt miljø satt opp med ssb-project kan man se nærmere her. For å legge til disse pakkene kan man gjøre følgende i terminalen:\npoetry add dash\npoetry add jupyter-dash\npoetry add jupyter-server-proxy\npoetry add jupyterlab-dash\npoetry add ipykernel\nOg hvis en ønsker Dash-Bootstrap-Components:\npoetry add dash-bootstrap-components\nVel og merke så vil ikke denne pakken fungere uten at tilhørende filer er med i selve repoet. Dette fordi i hovedsak leter denne etter filer den trenger på internett. Pakken i seg selv har en fordel i at det er lettere å bygge opp utseende(layout) i dashbordet ditt, samt andre komponenter som ikke ligger i standard dash pakken."
  },
  {
    "objectID": "dashboard.html#eksempel-kode-i-jupyterlab",
    "href": "dashboard.html#eksempel-kode-i-jupyterlab",
    "title": "Dash og dashboard",
    "section": "",
    "text": "Noen ting er viktig å huske på at kommer i korrekt rekkefølge når en koder opp i JupyterLab. Her kommer ett veldig enkelt eksempel.\nFørste celle importerer vi alle nødvendige pakker\nfrom dash import html\nfrom jupyter_dash import JupyterDash\nfrom jupyter_dash.comms import _send_jupyter_config_comm_request\n_send_jupyter_config_comm_request()\nI Andre celle må følgende kjøres, men her er det VELDIG VIKTIG at man avventer 2-3 sekunder etter at forrige celle ble kjørt.\nJupyterDash.infer_jupyter_proxy_config()\nDeretter så er vi klare for å bygge opp selve dashbordet. så i Tredje celle kan en enkel kode for eksempel se slik ut:\napp = JupyterDash(__name__)\nporten = 8642 # Valgfritt fire sifret nummer\n\napp.layout = html.Div([\n    html.H1(\"Eget dashbord med Dash i SSB\")\n])\n\nif __name__ == \"__main__\":\n    app.run_server(debug=True, port=porten, mode=\"external\")\nDenne koden vil starte opp dashbordet i eget vindu i browseren ettersom mode=“external”. Dersom man vil ha et eget vindu i JupyterLab kan man sette denne til “jupyterlab”, eller hvis du vil at dashbordet skal dukke opp under cellen i notebooken din, så kan man sette denne til “inline”."
  },
  {
    "objectID": "dashboard.html#aktuell-dokumentasjon",
    "href": "dashboard.html#aktuell-dokumentasjon",
    "title": "Dash og dashboard",
    "section": "",
    "text": "Diverse som er verdt å se nærmere på når en bygger dashbord applikasjon med Dash. Det følger med mange gode eksempler for bruk av diverse komponenter i dokumentasjonene under her.\n\nStartsiden til Dash\nLage interaktive grafer i Python med Plotly\nDash Core Components\nDash HTML Components\nDash Bootstrap Components\n\nVel og merke henter denne diverse materialer fra internett, og vil ikke fungere uten tilgang. Det er heller anbefalt å ha de nødvendige filene lagret lokalt for bruk av denne pakken."
  },
  {
    "objectID": "altinn3.html",
    "href": "altinn3.html",
    "title": "Altinn 3",
    "section": "",
    "text": "Frem mot sommeren 2025 skal alle skjema-undersøkelser i SSB som gjennomføres på Altinn 2 flyttes over til Altinn 3. Skjemaer som flyttes til Altinn 3 vil motta sine data på Dapla, og ikke på bakken som tidligere. Datafangsten håndteres av Team SUV, mens statistikkseksjonene henter sine data fra Team SUV sitt lagringsområde på Dapla. I dette kapitlet beskriver vi nærmere hvordan statistikkseksjonene kan jobbe med Altinn3-data på Dapla. Kort oppsummert består det av disse stegene:\n\nStatistikkprodusenten avtaler overføring av skjema fra Altinn 2 til Altinn 3 med planleggere på S821, som koordinerer denne jobben.\nNår statistikkprodusentene får beskjed om at Altinn3-skjemaet skal sendes ut til oppgavegiverne, så må de opprette et Dapla-team.\nNår Dapla-teamet er opprettet, og første skjema er sendt inn, så ber de Team SUV om å gi statistikkteamet tilgang til dataene som har kommet inn fra Altinn 3. I tillegg ber de om at Team SUV gir tilgang til teamets Transfer Service instans. 1 Merk at det må gis separate tilganger for data i staging- og produksjonsmiljø.\nStatistikkprodusenten setter opp en automatisk overføring av skjemadata med Transfer Service, fra Team SUV sitt lagringsområde over til Dapla-teamet sin kildebøtte.\nStatistikkprodusentene kan begynne å jobbe med dataene i Dapla. Blant annet tilbyr Dapla en automatiseringstjeneste man kan bruke for å prosessere dataene fra kildedata til inndata2.\n\nUnder forklarer vi mer med mer detaljer hvordan man går frem for gjennomføre steg 4-5 over.\n\n\n\n\n\n\nAnsvar for kildedata\n\n\n\nSelv om Team SUV tar ansvaret for datafangst fra Altinn3, så er det statistikkteamet som har ansvaret for langtidslagring av dataene i sin kildebøtte. Det vil si at at statistikkteamet må sørge for at data overføres til sin kildebøtte, og at de kan ikke regne med at Team SUV tar vare på en backup av dataene.\n\n\n\n\nNår skjema-dataene kommer inn fra Altinn 3 til Team SUV sitt lagringsområde, så er det en del ting som er verdt å tenke på:\n\nHvordan er filene organisert hos Team SUV?\nFilstrukturen der Team SUV lagrer dataene som kommer inn fra Altinn 3, har en klar struktur. Du kan selv gå inn å kikke på dataene, enten fra Jupyter eller Google Cloud Console, og bli bedre kjent med strukturen3. Figur 1 viser en hvordan en typisk filsti ser ut på lagringsområdet til Team SUV. Det starter med navnet til bøtta som Team SUV har opprettet for skjemaet. Deretter viser den innvitteringsdato, deretter et teknisk navn, og til slutt selve skjemanavnet.\n\n\n\n\nFigur 1: Typisk filsti for et Altinn3-skjema.\n\n\n\nHvordan organisere dataene i din kildebøtte?\nNår vi bruker Transfer Service til å synkronisere innholdet i Team SUV sitt lagringsområde til Dapla-teamet sitt lagringsområde, så er det mest hensiktmessig å fortsette å bruke mapppe-strukturen som Team SUV har. Grunnen er at vi ikke kan legge på noe logikk som lager en ny struktur. Tjenesten bare kopierer over data. Men du kan kan lage et nytt toppnivå-mappe som du ønsker at dataene skal synkroniseres til. F.eks. at alle Altinn-dataene legger seg inn i en mappe som heter altinn. Det er spesielt nyttig hvis du har flere datakilder som skal ligge i samme kildebøtte.\nUnike skjemanavn\nSkjemanavnet du ser i Figur 1 er unike. Dvs. at hvis en oppgavegiver sender inn flere skjemaer på samme dag, så er fortsatt skjemanavnet unikt. Det er viktig å være klar over når man bruker automatiseringstjenesten for kildedata senere. Siden tjenesten trigges per fil, og man er avhengig av unike navn for å ikke skrive over filer, så er det nyttig å vite at man kan videreføre skjemanavnet i overgangen fra kildedata til inndata.\n\n\n\n\nNår vi skal overføre filer fra Team SUV sin bøtte til vår kildebøtte, så kan vi gjøre det manuelt fra Jupyter som forklart her.. Men det er en bedre løsning å bruke en tjeneste som gjør dette for deg. Transfer Service er en tjeneste som kan brukes til å synkronisere innholdet mellom bøtter på Dapla, samt mellom bakke og sky. Når du skal ta i bruk tjenesten for å overføre data mellom en bøtte fra Team SUV sitt prosjekt altinn-data-prod, til en kildedata-bøtte i Dapla-teamet ditt, så gjør du følgende:\n\nFølg denne beskrivelsen hvordan man setter opp overføringsjobber.\nEtter at du har trykket på Create Transfer Job velger du Google Cloud Storage på begge alternativene under Get Started. Deretter går du videre ved å klikke på Next Step.\nUnder Choose a source så skal du velge hvor du skal kopiere data fra. Trykk på Browse. I vinduet som dukker opp trykker du på 🔻-ikonet ved siden av Project ID. I søkevinduet som dukker opp søker du opp altinn-data-prod og trykker på navnet. Da får du listet opp alle bøttene i altinn-data-prod prosjektet. Til slutt trykker du på bøtta som Team SUV har opprettet for undersøkelsen4 og klikker Select til nederst på siden. Trykk deretter Next step for å gå videre.\nUnder Choose a destination så skal du velge hvor du skal kopiere data til. Dette steget er ganske likt som forrige, men du skal nå velge ditt eget projekt og kildebøtta der. Trykk på Browse. I vinduet som dukker opp trykker du på 🔻-ikonet ved siden av Project ID. I søkevinduet som dukker opp søker du opp prod-&lt;ditt teamnavn&gt; og trykker på navnet. Da får du listet opp alle bøttene i ditt team sitt prosjekt. Velg kildebøtta som har navnet ssb-prod-&lt;teamnavn&gt;-data-kilde. Hvis du ønsker å kopiere data til en undermappe i bøtta, så trykker du på &gt;-ikonet ved bøttenavnet og velger ønsket undermappe5. Til slutt trykker du på Select til nederst på siden. Trykk deretter Next step for å gå videre.\nUnder Choose how and when to run job velger du Run with custom frequency og Starting now i Batch-modus. Hvis du ønsker å overføre så ofte som mulig, så velger du Custom frequency og Repeat every 1 Hours. Til slutt trykker du Next Step nederst på siden.\nUnder Choose Settings så legger du til en kort beskrivelse av jobben du har opprettet. Under de andre valgene kan du gjøre følgende:\n\nUnder Advanced transfer Options trenger du ikke gjøre noen endringer.\nUnder When to overwrite kan du velge If different.\nUnder When to delete kan du velge Never.\nUnder Manifest trenger du ikke huke av.\nUnder Logging options velger du samme som vist i Figur 2.\n\n\n\n\n\nFigur 2: Valg av opsjoner for logging i Transfer Service\n\n\nTil slutt trykker du Create for å aktivere tjenesten. Den vil da sjekke Team SUV sin bøtte hver time og kopiere over alle filer som ikke allerede eksisterer statistikkteamets kildebøtte.\n\n\n\nNår du har satt opp Transfer Service til å kopiere over filer fra Team SUV sin bøtte til statistikkteamets kildebøtte, så vil det potensielt komme inn nye skjemaer hver time. Disse kan du lese inn i Jupyter og jobbe med hvis du er kildedata-ansvarlig i teamet. Hvis du ikke er det så må du vente til dataene er tilgjengeliggjort i produkt-bøtta til teamet.\nSiden få personer innehar rollen som kildedata-ansvarlig så er det laget en automatiseringstjeneste som kan bearbeide alle filer som kommer inn i kildebøtta. Den lar deg kjøre et python-script på alle filer som kommer inn i kildebøtta.\nLes mer om hvordan du kan bruker tjenesten her.\n\n\n\nI denne delen deles noen tips og triks for å jobbe med Altinn3-dataene på Dapla. Fokuset vil være på hvordan du kan lese inn og transformere xml-filer fra skjema-dataene.\n\n\nFor å se innholdet i en mappe gir det mest mening å bruke Google Cloud Console. Her kan du se både filer og mapper i et pek-og-klikk grensesnitt. Men du kan ikke se på innholdet i filene der. Til det må du bruke Jupyter.\nAnta at vi ønsker å liste ut alle som leverte skjema den 10. mars 2023. Da kan vi bruke gcsfs til å gjøre det6:\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Print the list of parquet files\nxml_files\nHer bruker vi fs.glob()-funksjonen fra gcsfs til å loope gjennom alle undermapper av gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/, og legge alle filer som slutter med .xml til listen xml_files. Dermed har vi et Python-objekt som kun brukes til å hente inn de filene vi ønsker. Vi bruker da det vi vet om mappestrukturen i Team SUV sin bøtte som vi så tidligere i Figur 1.\n\n\n\nNoen ganger kan det være nyttig å se en xml-fil direkte. Da kan vi lese den inn i Jupyter og printe den ut. Her er et eksempel på hvordan vi kan gjøre det, samtidig som vi formatterer den med xml.dom.minidom:\nfrom xml.dom.minidom import parseString\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\n# Sett inn filstien din her\nfile = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/form_dc551844cd74.xml\"\n\ndom = parseString(fs.cat_file(file))\npretty_xml = dom.toprettyxml(indent=\"  \")\nprint(pretty_xml)\nVi brukte da fs.cat_file() til å hente inn innholdet i mappen, gjorde det om til xml-object, og formatterer outputen med xml.dom.minidom. Resultatet vil da se ut noe sånt som dette:\n&lt;?xml version=\"1.0\" ?&gt;\n&lt;melding xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" dataFormatProvider=\"SERES\" dataFormatId=\"7079\" dataFormatVersion=\"47315\"&gt;\n  &lt;InternInfo&gt;\n    &lt;raNummer&gt;RA-XXXX&lt;/raNummer&gt;\n    &lt;skjemaVersjon&gt;1.0&lt;/skjemaVersjon&gt;\n    &lt;undersoekelsesNr&gt;XXXXX&lt;/undersoekelsesNr&gt;\n    &lt;visOppgaveByrde&gt;0&lt;/visOppgaveByrde&gt;\n    &lt;visBrukeropplevelse&gt;0&lt;/visBrukeropplevelse&gt;\n    &lt;delregNr&gt;2XXXXX&lt;/delregNr&gt;\n    &lt;periodeFritekst&gt;3. kvartal 2022&lt;/periodeFritekst&gt;\n    &lt;periodeFomDato&gt;2022-07-01&lt;/periodeFomDato&gt;\n    &lt;periodeTomDato&gt;2022-09-30&lt;/periodeTomDato&gt;\n    &lt;periodeType&gt;KVRT&lt;/periodeType&gt;\n    &lt;periodeNummer&gt;3&lt;/periodeNummer&gt;\n    &lt;periodeAAr&gt;2022&lt;/periodeAAr&gt;\n    &lt;periodeDato&gt; &lt;/periodeDato&gt;\n    &lt;enhetsIdent&gt;XXXXXX&lt;/enhetsIdent&gt;\n    &lt;enhetsType&gt;BEDR&lt;/enhetsType&gt;\n    &lt;enhetsOrgNr&gt;XXXXXXXXX&lt;/enhetsOrgNr&gt;\n    &lt;enhetsNavn&gt;STATISTISK SENTRALBYRÅ &lt;/enhetsNavn&gt;\n    &lt;enhetsGateadresse&gt;Akersveien 26&lt;/enhetsGateadresse&gt;\n    &lt;enhetsPostnr&gt;0177&lt;/enhetsPostnr&gt;\n    &lt;enhetsPoststed&gt;OSLO&lt;/enhetsPoststed&gt;\n    &lt;enhetsAvdeling&gt;AVD XXXX&lt;/enhetsAvdeling&gt;\n    &lt;reporteeOrgNr&gt;XXXXXXXXX&lt;/reporteeOrgNr&gt;\n  &lt;/InternInfo&gt;\n  &lt;Kontakt&gt;\n    &lt;kontaktPersonNavn&gt;OLA NORDMANN&lt;/kontaktPersonNavn&gt;\n    &lt;kontaktPersonEpost&gt;ola@tull.no&lt;/kontaktPersonEpost&gt;\n    &lt;kontaktPersonTelefon&gt; &lt;/kontaktPersonTelefon&gt;\n    &lt;kontaktInfoBekreftet&gt;1&lt;/kontaktInfoBekreftet&gt;\n    &lt;kontaktInfoKommentar&gt;Dette er en et eksempel for Dapla-manualen&lt;/kontaktInfoKommentar&gt;\n  &lt;/Kontakt&gt;\n  &lt;Skjemadata&gt;\n    &lt;ledigeStillinger&gt;75&lt;/ledigeStillinger&gt;\n    &lt;datoPrefill&gt; &lt;/datoPrefill&gt;\n  &lt;/Skjemadata&gt;\n&lt;/melding&gt;\n\n\n\nDe færreste ønsker å jobbe direkte med XML-filer. Derfor er det nyttig å kunne transformere XML-filene til et mer brukervennlig format, f.eks. en Pandas Dataframe. Her er et eksempel på hvordan vi kan gjøre det med Pandas:\n\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nfrom dapla import FileClient\n\ndef single_xml_to_dataframe(file_path: str) -&gt; pd.DataFrame:\n    fs = FileClient.get_gcs_file_system()\n\n    with fs.open(file_path, mode=\"r\") as f:\n        single_xml = f.read()\n\n    root = ET.fromstring(single_xml)\n    intern_info = root.find(\"InternInfo\")\n    kontakt = root.find(\"Kontakt\")\n    skjemadata = root.find(\"Skjemadata\")\n\n    data = []\n    all_tags = set()\n\n    for element in intern_info:\n        all_tags.add(element.tag)\n\n    for element in kontakt:\n        all_tags.add(element.tag)\n\n    for element in skjemadata:\n        all_tags.add(element.tag)\n\n    for tag in all_tags:\n        element = intern_info.find(tag)\n        if element is None:\n            element = kontakt.find(tag)\n        if element is None:\n            element = skjemadata.find(tag)\n        if element is not None:\n            value = element.text\n            data.append(value)\n        else:\n            data.append(None)\n\n    # Include the full path to the XML file in the data list\n    data.append(file_path)\n\n    # Create the DataFrame inside the function\n    df = pd.DataFrame([data], columns=list(all_tags) + [\"pathToXmlFile\"])\n    path_to_xml_file_split = df[\"pathToXmlFile\"].str.split(\"/\")\n    return df\n\n# Run the function\nfile_path = \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/24/28c34dce4232_e7bed8a4-def5-42dc-b1e2-b9fc66beb769/form_28c34dce4232.xml\"\ndf = single_xml_to_dataframe(file_path=file_path)\nI funksjonen så søker vi etter alle elementer under taggene intern_info, kontakt og skjemadata. Dvs. at vi lager en dataframe med en rad, der vi tar med all data fra xml-filen. Til slutt legger vi til en kolonne med filstien til filen. Det kan være nyttig senere hvis man gå tilbake til xml-filen for å sjekke om konverteringen til en dataframe er riktig.\nFunksjonen single_xml_to_dataframe() kan brukes til å loope over en liste med filstier (vi viste hvordan man lager en slik liste tidligere i kapitlet). Men et typisk problem som vil oppstå da er at xml-filene har forskjellig antall elementer i seg, som gir dataframes med ulikt antall kolonner, og derfor vil ikke Pandas vite hvordan man konkatinerer disse. For å fikse dette må du modifisere funksjonen til å ta høyde for dette.\n\n\n\nHvis vi ønsker å kopiere filer manuelt (dvs. ikke med Transfer Service) fra Team SUV sine bøtter til egen kildebøtte, kan vi gjøre det fra Jupyter. Vi har tidligere sett hvordan vi kan kopiere enkeltfiler mellom to bøtter. Under viser vi hvordan man kan kopiere alle filer under en viss filsti med kommandoen fs.copy():\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\nI koden over så kopierer vi alle filer under from_path til to_path. Vi bruker recursive=True for å sørge for at vi kopierer alle filer under from_path.\nI eksempelet over så kopierer vi over alt som er from_path og dets undermapper. I tilfellet med Altinn-data så ligger det også pdf-filer av skjemaet som kanskje ikke ønsker å kopiere. I de tilfellene kan vi først søke etter de filene vi ønsker å kopiere og legge de i en liste. Deretter kan vi kopiere over en og en fil fra listen. Denne tilnærmingen er veldig nyttig hvis vi ønsker å filtrere ut filer som ikke er XML-filer, eller vi ønsker en annen mappestruktur en den som ligger i from_path. Her er en måte vi kan gjøre det på:\nfrom dapla import FileClient\n\n# Lager en filsystem-instans av GCS\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Stien du ønsker å kopiere til.\n# Koden under foutsetter at du har med gs:// først\nto_folder = \"gs://ssb-prod-dapla-felles-data-delt/\"\n\n# Kopierer over filene\nfor file in xml_files:\n    from_bucket = file.split(\"/\")[0]\n    to_bucket = to_folder.split(\"/\")[2]\n    to_path = file.replace(\n        from_bucket, to_bucket\n    )\n    fs.cp(file, to_path)\nI koden over så bruker vi fs.glob() og ** til å søke rekursivt etter alle xml-filer under filstien gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10. Deretter kopierer vi over filene til egen kildebøtte med fs.cp(). Når vi skal kopiere over til en ny bøtte må vi bestemme oss for hvor filene skal ligge og hvor mye av den gamle filstien vi skal beholde. Anta at vi skal beholde hele mappestrukturen i Team SUV sin bøtte. Da kan vi egentlig bare erstatte ra0678-01-altinn-data-prod-e17d-ssb-altinn/ med den nye bøtte-navnet, og vi vil få den samme strukturen som i Team SUV sin bøtte."
  },
  {
    "objectID": "altinn3.html#forberedelse",
    "href": "altinn3.html#forberedelse",
    "title": "Altinn 3",
    "section": "",
    "text": "Når skjema-dataene kommer inn fra Altinn 3 til Team SUV sitt lagringsområde, så er det en del ting som er verdt å tenke på:\n\nHvordan er filene organisert hos Team SUV?\nFilstrukturen der Team SUV lagrer dataene som kommer inn fra Altinn 3, har en klar struktur. Du kan selv gå inn å kikke på dataene, enten fra Jupyter eller Google Cloud Console, og bli bedre kjent med strukturen3. Figur 1 viser en hvordan en typisk filsti ser ut på lagringsområdet til Team SUV. Det starter med navnet til bøtta som Team SUV har opprettet for skjemaet. Deretter viser den innvitteringsdato, deretter et teknisk navn, og til slutt selve skjemanavnet.\n\n\n\n\nFigur 1: Typisk filsti for et Altinn3-skjema.\n\n\n\nHvordan organisere dataene i din kildebøtte?\nNår vi bruker Transfer Service til å synkronisere innholdet i Team SUV sitt lagringsområde til Dapla-teamet sitt lagringsområde, så er det mest hensiktmessig å fortsette å bruke mapppe-strukturen som Team SUV har. Grunnen er at vi ikke kan legge på noe logikk som lager en ny struktur. Tjenesten bare kopierer over data. Men du kan kan lage et nytt toppnivå-mappe som du ønsker at dataene skal synkroniseres til. F.eks. at alle Altinn-dataene legger seg inn i en mappe som heter altinn. Det er spesielt nyttig hvis du har flere datakilder som skal ligge i samme kildebøtte.\nUnike skjemanavn\nSkjemanavnet du ser i Figur 1 er unike. Dvs. at hvis en oppgavegiver sender inn flere skjemaer på samme dag, så er fortsatt skjemanavnet unikt. Det er viktig å være klar over når man bruker automatiseringstjenesten for kildedata senere. Siden tjenesten trigges per fil, og man er avhengig av unike navn for å ikke skrive over filer, så er det nyttig å vite at man kan videreføre skjemanavnet i overgangen fra kildedata til inndata."
  },
  {
    "objectID": "altinn3.html#transfer-service",
    "href": "altinn3.html#transfer-service",
    "title": "Altinn 3",
    "section": "",
    "text": "Når vi skal overføre filer fra Team SUV sin bøtte til vår kildebøtte, så kan vi gjøre det manuelt fra Jupyter som forklart her.. Men det er en bedre løsning å bruke en tjeneste som gjør dette for deg. Transfer Service er en tjeneste som kan brukes til å synkronisere innholdet mellom bøtter på Dapla, samt mellom bakke og sky. Når du skal ta i bruk tjenesten for å overføre data mellom en bøtte fra Team SUV sitt prosjekt altinn-data-prod, til en kildedata-bøtte i Dapla-teamet ditt, så gjør du følgende:\n\nFølg denne beskrivelsen hvordan man setter opp overføringsjobber.\nEtter at du har trykket på Create Transfer Job velger du Google Cloud Storage på begge alternativene under Get Started. Deretter går du videre ved å klikke på Next Step.\nUnder Choose a source så skal du velge hvor du skal kopiere data fra. Trykk på Browse. I vinduet som dukker opp trykker du på 🔻-ikonet ved siden av Project ID. I søkevinduet som dukker opp søker du opp altinn-data-prod og trykker på navnet. Da får du listet opp alle bøttene i altinn-data-prod prosjektet. Til slutt trykker du på bøtta som Team SUV har opprettet for undersøkelsen4 og klikker Select til nederst på siden. Trykk deretter Next step for å gå videre.\nUnder Choose a destination så skal du velge hvor du skal kopiere data til. Dette steget er ganske likt som forrige, men du skal nå velge ditt eget projekt og kildebøtta der. Trykk på Browse. I vinduet som dukker opp trykker du på 🔻-ikonet ved siden av Project ID. I søkevinduet som dukker opp søker du opp prod-&lt;ditt teamnavn&gt; og trykker på navnet. Da får du listet opp alle bøttene i ditt team sitt prosjekt. Velg kildebøtta som har navnet ssb-prod-&lt;teamnavn&gt;-data-kilde. Hvis du ønsker å kopiere data til en undermappe i bøtta, så trykker du på &gt;-ikonet ved bøttenavnet og velger ønsket undermappe5. Til slutt trykker du på Select til nederst på siden. Trykk deretter Next step for å gå videre.\nUnder Choose how and when to run job velger du Run with custom frequency og Starting now i Batch-modus. Hvis du ønsker å overføre så ofte som mulig, så velger du Custom frequency og Repeat every 1 Hours. Til slutt trykker du Next Step nederst på siden.\nUnder Choose Settings så legger du til en kort beskrivelse av jobben du har opprettet. Under de andre valgene kan du gjøre følgende:\n\nUnder Advanced transfer Options trenger du ikke gjøre noen endringer.\nUnder When to overwrite kan du velge If different.\nUnder When to delete kan du velge Never.\nUnder Manifest trenger du ikke huke av.\nUnder Logging options velger du samme som vist i Figur 2.\n\n\n\n\n\nFigur 2: Valg av opsjoner for logging i Transfer Service\n\n\nTil slutt trykker du Create for å aktivere tjenesten. Den vil da sjekke Team SUV sin bøtte hver time og kopiere over alle filer som ikke allerede eksisterer statistikkteamets kildebøtte."
  },
  {
    "objectID": "altinn3.html#automatiseringstjeneste-for-kildedata",
    "href": "altinn3.html#automatiseringstjeneste-for-kildedata",
    "title": "Altinn 3",
    "section": "",
    "text": "Når du har satt opp Transfer Service til å kopiere over filer fra Team SUV sin bøtte til statistikkteamets kildebøtte, så vil det potensielt komme inn nye skjemaer hver time. Disse kan du lese inn i Jupyter og jobbe med hvis du er kildedata-ansvarlig i teamet. Hvis du ikke er det så må du vente til dataene er tilgjengeliggjort i produkt-bøtta til teamet.\nSiden få personer innehar rollen som kildedata-ansvarlig så er det laget en automatiseringstjeneste som kan bearbeide alle filer som kommer inn i kildebøtta. Den lar deg kjøre et python-script på alle filer som kommer inn i kildebøtta.\nLes mer om hvordan du kan bruker tjenesten her."
  },
  {
    "objectID": "altinn3.html#tips-og-triks",
    "href": "altinn3.html#tips-og-triks",
    "title": "Altinn 3",
    "section": "",
    "text": "I denne delen deles noen tips og triks for å jobbe med Altinn3-dataene på Dapla. Fokuset vil være på hvordan du kan lese inn og transformere xml-filer fra skjema-dataene.\n\n\nFor å se innholdet i en mappe gir det mest mening å bruke Google Cloud Console. Her kan du se både filer og mapper i et pek-og-klikk grensesnitt. Men du kan ikke se på innholdet i filene der. Til det må du bruke Jupyter.\nAnta at vi ønsker å liste ut alle som leverte skjema den 10. mars 2023. Da kan vi bruke gcsfs til å gjøre det6:\nfrom dapla import FileClient\n\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Print the list of parquet files\nxml_files\nHer bruker vi fs.glob()-funksjonen fra gcsfs til å loope gjennom alle undermapper av gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/, og legge alle filer som slutter med .xml til listen xml_files. Dermed har vi et Python-objekt som kun brukes til å hente inn de filene vi ønsker. Vi bruker da det vi vet om mappestrukturen i Team SUV sin bøtte som vi så tidligere i Figur 1.\n\n\n\nNoen ganger kan det være nyttig å se en xml-fil direkte. Da kan vi lese den inn i Jupyter og printe den ut. Her er et eksempel på hvordan vi kan gjøre det, samtidig som vi formatterer den med xml.dom.minidom:\nfrom xml.dom.minidom import parseString\n\nfrom dapla import FileClient\n\n# Kobler oss på bøttene\nfs = FileClient.get_gcs_file_system()\n\n# Sett inn filstien din her\nfile = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/form_dc551844cd74.xml\"\n\ndom = parseString(fs.cat_file(file))\npretty_xml = dom.toprettyxml(indent=\"  \")\nprint(pretty_xml)\nVi brukte da fs.cat_file() til å hente inn innholdet i mappen, gjorde det om til xml-object, og formatterer outputen med xml.dom.minidom. Resultatet vil da se ut noe sånt som dette:\n&lt;?xml version=\"1.0\" ?&gt;\n&lt;melding xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" dataFormatProvider=\"SERES\" dataFormatId=\"7079\" dataFormatVersion=\"47315\"&gt;\n  &lt;InternInfo&gt;\n    &lt;raNummer&gt;RA-XXXX&lt;/raNummer&gt;\n    &lt;skjemaVersjon&gt;1.0&lt;/skjemaVersjon&gt;\n    &lt;undersoekelsesNr&gt;XXXXX&lt;/undersoekelsesNr&gt;\n    &lt;visOppgaveByrde&gt;0&lt;/visOppgaveByrde&gt;\n    &lt;visBrukeropplevelse&gt;0&lt;/visBrukeropplevelse&gt;\n    &lt;delregNr&gt;2XXXXX&lt;/delregNr&gt;\n    &lt;periodeFritekst&gt;3. kvartal 2022&lt;/periodeFritekst&gt;\n    &lt;periodeFomDato&gt;2022-07-01&lt;/periodeFomDato&gt;\n    &lt;periodeTomDato&gt;2022-09-30&lt;/periodeTomDato&gt;\n    &lt;periodeType&gt;KVRT&lt;/periodeType&gt;\n    &lt;periodeNummer&gt;3&lt;/periodeNummer&gt;\n    &lt;periodeAAr&gt;2022&lt;/periodeAAr&gt;\n    &lt;periodeDato&gt; &lt;/periodeDato&gt;\n    &lt;enhetsIdent&gt;XXXXXX&lt;/enhetsIdent&gt;\n    &lt;enhetsType&gt;BEDR&lt;/enhetsType&gt;\n    &lt;enhetsOrgNr&gt;XXXXXXXXX&lt;/enhetsOrgNr&gt;\n    &lt;enhetsNavn&gt;STATISTISK SENTRALBYRÅ &lt;/enhetsNavn&gt;\n    &lt;enhetsGateadresse&gt;Akersveien 26&lt;/enhetsGateadresse&gt;\n    &lt;enhetsPostnr&gt;0177&lt;/enhetsPostnr&gt;\n    &lt;enhetsPoststed&gt;OSLO&lt;/enhetsPoststed&gt;\n    &lt;enhetsAvdeling&gt;AVD XXXX&lt;/enhetsAvdeling&gt;\n    &lt;reporteeOrgNr&gt;XXXXXXXXX&lt;/reporteeOrgNr&gt;\n  &lt;/InternInfo&gt;\n  &lt;Kontakt&gt;\n    &lt;kontaktPersonNavn&gt;OLA NORDMANN&lt;/kontaktPersonNavn&gt;\n    &lt;kontaktPersonEpost&gt;ola@tull.no&lt;/kontaktPersonEpost&gt;\n    &lt;kontaktPersonTelefon&gt; &lt;/kontaktPersonTelefon&gt;\n    &lt;kontaktInfoBekreftet&gt;1&lt;/kontaktInfoBekreftet&gt;\n    &lt;kontaktInfoKommentar&gt;Dette er en et eksempel for Dapla-manualen&lt;/kontaktInfoKommentar&gt;\n  &lt;/Kontakt&gt;\n  &lt;Skjemadata&gt;\n    &lt;ledigeStillinger&gt;75&lt;/ledigeStillinger&gt;\n    &lt;datoPrefill&gt; &lt;/datoPrefill&gt;\n  &lt;/Skjemadata&gt;\n&lt;/melding&gt;\n\n\n\nDe færreste ønsker å jobbe direkte med XML-filer. Derfor er det nyttig å kunne transformere XML-filene til et mer brukervennlig format, f.eks. en Pandas Dataframe. Her er et eksempel på hvordan vi kan gjøre det med Pandas:\n\nimport xml.etree.ElementTree as ET\nimport pandas as pd\nfrom dapla import FileClient\n\ndef single_xml_to_dataframe(file_path: str) -&gt; pd.DataFrame:\n    fs = FileClient.get_gcs_file_system()\n\n    with fs.open(file_path, mode=\"r\") as f:\n        single_xml = f.read()\n\n    root = ET.fromstring(single_xml)\n    intern_info = root.find(\"InternInfo\")\n    kontakt = root.find(\"Kontakt\")\n    skjemadata = root.find(\"Skjemadata\")\n\n    data = []\n    all_tags = set()\n\n    for element in intern_info:\n        all_tags.add(element.tag)\n\n    for element in kontakt:\n        all_tags.add(element.tag)\n\n    for element in skjemadata:\n        all_tags.add(element.tag)\n\n    for tag in all_tags:\n        element = intern_info.find(tag)\n        if element is None:\n            element = kontakt.find(tag)\n        if element is None:\n            element = skjemadata.find(tag)\n        if element is not None:\n            value = element.text\n            data.append(value)\n        else:\n            data.append(None)\n\n    # Include the full path to the XML file in the data list\n    data.append(file_path)\n\n    # Create the DataFrame inside the function\n    df = pd.DataFrame([data], columns=list(all_tags) + [\"pathToXmlFile\"])\n    path_to_xml_file_split = df[\"pathToXmlFile\"].str.split(\"/\")\n    return df\n\n# Run the function\nfile_path = \"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/24/28c34dce4232_e7bed8a4-def5-42dc-b1e2-b9fc66beb769/form_28c34dce4232.xml\"\ndf = single_xml_to_dataframe(file_path=file_path)\nI funksjonen så søker vi etter alle elementer under taggene intern_info, kontakt og skjemadata. Dvs. at vi lager en dataframe med en rad, der vi tar med all data fra xml-filen. Til slutt legger vi til en kolonne med filstien til filen. Det kan være nyttig senere hvis man gå tilbake til xml-filen for å sjekke om konverteringen til en dataframe er riktig.\nFunksjonen single_xml_to_dataframe() kan brukes til å loope over en liste med filstier (vi viste hvordan man lager en slik liste tidligere i kapitlet). Men et typisk problem som vil oppstå da er at xml-filene har forskjellig antall elementer i seg, som gir dataframes med ulikt antall kolonner, og derfor vil ikke Pandas vite hvordan man konkatinerer disse. For å fikse dette må du modifisere funksjonen til å ta høyde for dette.\n\n\n\nHvis vi ønsker å kopiere filer manuelt (dvs. ikke med Transfer Service) fra Team SUV sine bøtter til egen kildebøtte, kan vi gjøre det fra Jupyter. Vi har tidligere sett hvordan vi kan kopiere enkeltfiler mellom to bøtter. Under viser vi hvordan man kan kopiere alle filer under en viss filsti med kommandoen fs.copy():\nfrom dapla import FileClient\n\n# Setter opp en filsystem-instans mot GCS\nfs = FileClient.get_gcs_file_system()\n\nfrom_path = \"gs://ssb-prod-arbmark-skjema-data-kilde/ledstill/altinn/2022/11/21/\"\nto_path = \"gs://ssb-prod-dapla-felles-data-delt/altinn3/\"\nfs.copy(from_path, to_path, recursive=True)\nI koden over så kopierer vi alle filer under from_path til to_path. Vi bruker recursive=True for å sørge for at vi kopierer alle filer under from_path.\nI eksempelet over så kopierer vi over alt som er from_path og dets undermapper. I tilfellet med Altinn-data så ligger det også pdf-filer av skjemaet som kanskje ikke ønsker å kopiere. I de tilfellene kan vi først søke etter de filene vi ønsker å kopiere og legge de i en liste. Deretter kan vi kopiere over en og en fil fra listen. Denne tilnærmingen er veldig nyttig hvis vi ønsker å filtrere ut filer som ikke er XML-filer, eller vi ønsker en annen mappestruktur en den som ligger i from_path. Her er en måte vi kan gjøre det på:\nfrom dapla import FileClient\n\n# Lager en filsystem-instans av GCS\nfs = FileClient.get_gcs_file_system()\n\n# Henter ut alle xml-filer under en filsti\nxml_files = fs.glob(\"gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10/**.xml\")\n\n# Stien du ønsker å kopiere til.\n# Koden under foutsetter at du har med gs:// først\nto_folder = \"gs://ssb-prod-dapla-felles-data-delt/\"\n\n# Kopierer over filene\nfor file in xml_files:\n    from_bucket = file.split(\"/\")[0]\n    to_bucket = to_folder.split(\"/\")[2]\n    to_path = file.replace(\n        from_bucket, to_bucket\n    )\n    fs.cp(file, to_path)\nI koden over så bruker vi fs.glob() og ** til å søke rekursivt etter alle xml-filer under filstien gs://ra0678-01-altinn-data-prod-e17d-ssb-altinn/2023/3/10. Deretter kopierer vi over filene til egen kildebøtte med fs.cp(). Når vi skal kopiere over til en ny bøtte må vi bestemme oss for hvor filene skal ligge og hvor mye av den gamle filstien vi skal beholde. Anta at vi skal beholde hele mappestrukturen i Team SUV sin bøtte. Da kan vi egentlig bare erstatte ra0678-01-altinn-data-prod-e17d-ssb-altinn/ med den nye bøtte-navnet, og vi vil få den samme strukturen som i Team SUV sin bøtte."
  },
  {
    "objectID": "altinn3.html#footnotes",
    "href": "altinn3.html#footnotes",
    "title": "Altinn 3",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nForslag til e-post til Team SUV etter at teamet er opprettet:\nVi har opprettet et Dapla-tema som heter &lt;ditt teamnavn&gt; for å jobbe med skjema &lt;RA-XXXX&gt;. Kan dere gi oss tilgang til riktig lagringsområde og også gi vår Transfer Service lesetilgang.↩︎\nEn typisk prosessering som de fleste vil ønske å gjøre er å konvertere fra xml-formatet det kom på, og over til parquet-formatet.↩︎\nDu kan gå inn i Google Cloud Console og søke opp prosjektet til Team SUV som de bruker for å dele data. Det heter altinn-data-prod, og du finner bøttene ved å klikke deg inn på Cloud Storage↩︎\nBøttenavnet starter alltid med RA-nummeret til undersøkelsen.↩︎\nAlternativt oppretter du en mappe direkte vinduet ved å trykke på mappe-ikonet med en +-tegn i seg.↩︎\nFor å jobbe mot datat i GCS som i et “vanlig” filsysten kan vi bruke FileClient.get_gcs_file_system() fra dapla-toolbelt.↩︎"
  },
  {
    "objectID": "jobbe-med-kode.html#forberedelser",
    "href": "jobbe-med-kode.html#forberedelser",
    "title": "Jobbe med kode",
    "section": "Forberedelser",
    "text": "Forberedelser\nFør du kan ta i bruk ssb-project så er det et par ting som må være på plass:\n\nDu må ha konfigurert Git etter SSB sin standard (les mer om hvordan her).\nHvis du ønsker at ssb-project også skal opprette et GitHub-repo for deg må du også følgende være på plass:\n\nDu må ha en GitHub-bruker (les hvordan her)\nSkru på 2-faktor autentifisering for GitHub-brukeren din (les hvordan her)\nVære koblet mot SSBs organisasjon statisticsnorway på GitHub (les hvordan her)\nOpprette Personal Access Token (PAT) og godkjenne det for bruk mot statisticsnorway (les hvordan her)\n\n\nDet er også å anbefale at du lagrer PAT lokalt slik at du ikke trenger å forholde deg til det når jobber med Git og GitHub. Hvis du har alt dette på plass så kan du bare fortsette å følge de neste kapitlene."
  },
  {
    "objectID": "jobbe-med-kode.html#opprett-ssb-project",
    "href": "jobbe-med-kode.html#opprett-ssb-project",
    "title": "Jobbe med kode",
    "section": "Opprett ssb-project",
    "text": "Opprett ssb-project\n\n\n\n\n\n\nHar du Github bruker? Noe funksjonalitet i ssb-project krever det. Finn ut hvordan ved å lese forrige kapittel.\n\n\n\nssb-project lar deg opprette en prosjekt-mappe med og uten GitHub-repo. La oss ta for oss hver av alternativene.\n\nUten GitHub-repo\nFor å opprette et nytt ssb-project uten GitHub-repo gjør du følgende:\n\nÅpne en terminal. De fleste vil gjøre dette i Jupyterlab på bakke eller sky og da kan de bare trykke på det blå ➕-tegnet i Jupyterlab og velge Terminal.\nFør vi kjører programmet må vi være obs på at ssb-project vil opprette en ny mappe der vi står. Gå derfor til den mappen du ønsker å ha den nye prosjektmappen. For å opprette et prosjekt som heter stat-testprod så skriver du følgende i terminalen:\n\nssb-project create stat-testprod\n\n\nHvis du stod i hjemmemappen din på når du skrev inn kommandoen over i terminalen, så har du fått mappestrukturen som vises i Figur 1. 2. Den inneholder følgende :\n\n.git-mappe som blir opprettet for å versjonshåndtere med Git.\nsrc-mappe som skal inneholde all koden som utgjør produksjonsløpet.\ntests-mappe som inneholder tester du skriver for koden din.\nLICENCE-fil som skal benyttes for public-repos i SSB.\npoetry.lock-fil som inneholder alle versjoner av Python-pakker som blir brukt.\nREADME.md-fil som brukes for tekstlig innhold på GitHub-siden for prosjektet.\n\n\n\n\n\n\nFigur 1: Mappen som ble opprettet av ssb-project.\n\n\n\n\n\n\nMed Github-repo\nOver så opprettet vi et ssb-project uten å opprette et GitHub-repo. Hvis du ønsker å opprette et GitHub-repo også må du endre kommandoen over til:\nssb-project create stat-testprod --github --github-token='blablabla'\nKommandoen over oppretter en mappestruktur slik vi så tidligere, men også et ssb-project som heter stat-testprod med et GitHub-repo med samme navn. Som du ser så må vi da sende med opsjonen --github og PAT med opsjonen --github-token='blablabla'. Repoet i GitHub ser da ut som i Figur 2.\n\n\n\nFigur 2: GitHub-repo som er opprettet av ssb-project\n\n\n\n\n\n\n\n\nNår du har opprettet et nytt ssb-project, eller bygget et eksisterende prosjekt, så kan det ta rundt 30 sekunder før kernelen viser seg i Jupterlab-launcher. Vær tålmodig!"
  },
  {
    "objectID": "jobbe-med-kode.html#installere-pakker",
    "href": "jobbe-med-kode.html#installere-pakker",
    "title": "Jobbe med kode",
    "section": "Installere pakker",
    "text": "Installere pakker\nNår du har opprettet et ssb-project så kan du installere de python-pakkene du trenger fra PyPI. Hvis du for eksempel ønsker å installere Pandas, et populært data wrangling bibliotek, så kan du gjøre følgende:\n\nÅpne en terminal i Jupyterlab.\nGå inn i prosjektmappen din ved å skrive\n\ncd &lt;sti til prosjektmappe&gt;\n\nLag en branch/utviklingsbranch som f.eks. heter install-pandas:\n\ngit checkout -b install-pandas\n\nInstaller Pandas ved å skrive følgende\n\npoetry add pandas\n\n\n\nFigur 3: Installasjon av Pandas med ssb-project\n\n\nFigur 3 viser hvordan dette vil se ut i en Jupyterlab-terminal. Kommandoen for å installere noe er poetry add etterfulgt av pakkenavnet. Vi ser også at den automatisk legger til Pandas-versjonen i filen poetry.lock. Les mer om hvordan man installerer pakker her."
  },
  {
    "objectID": "jobbe-med-kode.html#push-til-github",
    "href": "jobbe-med-kode.html#push-til-github",
    "title": "Jobbe med kode",
    "section": "Push til GitHub",
    "text": "Push til GitHub\nNår du nå har installert en pakke så har filen poetry.lock endret seg. La oss for eksempelets skyld anta at du ønsker å bruke Git til å dokumentere denne hendelsen, og dele det med en kollega via GitHub. Hvis vi har opprettet et ssb-project med et GitHub-repo så kan vi gjøre akkurat dette:\n\nVi kan stage alle endringer med følgende kommando i terminalen når vi står i prosjektmappen:\n\ngit add -A\n\nVidere kan commit en endring, dvs. ta et stillbilde av koden i dette øyeblikket, ved å skrive følgende:\n\ngit commit -m \"Installert pandas\"\n\nPush det opp til GitHub3. Anta at vi gjorde dette i branchen install-pandas som ble opprettet tidligere. Da kan vi skrive følgende:\n\ngit push --set-upstream origin install-pandas\nMer kommer her."
  },
  {
    "objectID": "jobbe-med-kode.html#bygg-eksisterende-ssb-project",
    "href": "jobbe-med-kode.html#bygg-eksisterende-ssb-project",
    "title": "Jobbe med kode",
    "section": "Bygg eksisterende ssb-project",
    "text": "Bygg eksisterende ssb-project\nNår vi skal samarbeide med andre om kode så gjør vi dette via GitHub. Når du pusher koden din til GitHub, så kan samarbeidspartnere pulle ned koden og jobbe videre med den. Men når de henter ned koden så vil de bare hente ned selve koden, ikke pakker og Python-versjonen som ble brukt. De må installere alt som du hadde installert. I tillegg trenger de en kernel hvis de skal jobbe i Jupyterlab. ssb-project gjør det svært enkelt å bygge opp det du trenger, siden det virtuelle miljøet har all informasjon om hva som trengs.\nFor at samarbeidspartneren din skal kunne bygge miljøet på nytt, må de ha gjort en minimal konfigurering av Git. Les mer om hvordan du frem for å gjøre dette her.\nFor å bygge opp et eksisterende miljø gjør du følgende:\n\nFørst må du kopiere prosjektet ned lokalt, eller klone repoet med git-terminologi\n\ngit clone https://github.com/statisticsnorway/&lt;prosjektnavn&gt;\n\nGå inn i mappen du klonet\n\ncd &lt;prosjektnavn&gt;\n\nSkape et virtuelt miljø og installere en tilsvarende Jupyter kernel med\n\nssb-project build"
  },
  {
    "objectID": "jobbe-med-kode.html#rydd-opp-etter-deg",
    "href": "jobbe-med-kode.html#rydd-opp-etter-deg",
    "title": "Jobbe med kode",
    "section": "Rydd opp etter deg",
    "text": "Rydd opp etter deg\nDet vil være tilfeller hvor man ønsker å slette et ssb-project, enten fordi man ikke trenger koden lenger eller fordi man bare testet litt.\n\nLokalt\nHvis man jobber med flere prosjekter så kan det fort bli mange Jupyter kerneler hengende igjen. Derfor er det også mulighet å kjøre\nssb-project clean stat-testprod\nsom sletter Jupyter-kernelen og de installerte pakkene i prosjektet. Hvis du også ønsker å slette selve mappen med kode må du gjøre det manuelt4:\nrm -rf ~/stat-testprod/\nProsjektmappen over lå direkte i hjemmemappen min og hjemmemappen på Linux kan alltid referes til med et tilda-tegn ~.\n\n\nArkiver GitHub-repo\nGitHub-repoer som er opprettet under SSB sin organinasjons statisticsnorway på GitHub kan ikke slettes, bare arkiveres. Grunnen er at hvis man oppdager en sårbarhet senere så er det viktig å kunne se repoet for å forstå hva som har skjedd.\nHvis du ikke trenger et GitHub-repo lenger kan man arkivere repoet. Det gjør du på følgende måte:\n\nGi inn i repoet Settings slik som vist med rød pil i Figur 4.\n\n\n\n\nFigur 4: Settings for repoet.\n\n\n\nUnder General scroller du deg ned til delen som heter Danger Zone og velger Archive this repository, slik som vist på Figur 5.\n\n\n\n\nFigur 5: Arkivering av et repo.\n\n\n\nI dialogboksen som dukker opp fyller du inn reponavnet som beskrevet og trykker på I understand the consequences, archive this repository.\n\nNår det er gjort så er repoet lesbart, men man kan ikke jobbe med det. Men som vi ser av @#fig-github-repo-settings-archive-warning kan man omgjøre arkiveringen senere hvis det skulle være ønskelig."
  },
  {
    "objectID": "jobbe-med-kode.html#hva-med-r",
    "href": "jobbe-med-kode.html#hva-med-r",
    "title": "Jobbe med kode",
    "section": "Hva med R?",
    "text": "Hva med R?\nVi har foreløpig ikke integret R i ssb-project. Grunnen er at det mest populære virtuelle miljø-verktøet for R, renv, kun tilbyr å passe på versjoner av R-pakker og ikke selve R-installasjonen. Det er en svakhet som trolig gjør det vanskeligere enn nødvendig å gjenskape tidligere publiserte resultater med ssb-project. I tillegg klarer den ikke å gjenkjenne pakker som blir brukt i ipynb-filer.\nPlanen er å finne et annet verktøy enn renv som kan også reprodusere R-versjonen. Team Statistikktjenester ser nærmere på hvilke alternativer som finnes og vil tilby noe i fremtiden.\nI mellomtiden kan man bruke renv slik det er beskrevet her for skymiljøet, og med denne modifiseringen for bakkemiljøet."
  },
  {
    "objectID": "jobbe-med-kode.html#footnotes",
    "href": "jobbe-med-kode.html#footnotes",
    "title": "Jobbe med kode",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nCLI = Command-Line-Interface. Dvs. et program som er skrevet for å brukes terminalen ved hjelp av enkle kommandoer.↩︎\nFiler og mapper som starter med punktum er skjulte med mindre man ber om å se dem. I Jupyterlab kan disse vises i filutforskeren ved å velge View fra menylinjen, og deretter velge Show hidden files. I en terminal skriver man ls -a for å se de.↩︎\nÅ pushe til GitHub uten å sende ved Personal Access Token fordrer at du har lagret det lokalt så Git kan finne det. Her et eksempel på hvordan det kan gjøres.↩︎\nDette kan også gjøres ved å høyreklikke på mappen i Jupyterlab sin filutforsker og velge Delete.↩︎"
  },
  {
    "objectID": "sas.html",
    "href": "sas.html",
    "title": "SAS",
    "section": "",
    "text": "SAS"
  },
  {
    "objectID": "hva-er-dapla-team.html",
    "href": "hva-er-dapla-team.html",
    "title": "Hva er Dapla-team?",
    "section": "",
    "text": "For å kunne jobbe med skarpe/ekte data på Dapla må man opprette et et Dapla-team. Et dapla-team er en gruppe personer som jobber med ett eller flere emneområder på SSBs dataplattform/Dapla. Når man oppretter et Dapla-team i SSB får man følgende:\n\nLagringsområde for data i Google Cloud Storage (GCS)1\nTilgangskontroll til lagringsområdet\nTjeneste for synkronisering av data mellom bakke og sky (Transfer Service)\n\nEt Dapla-team er ikke bare et sett med Google-tjenester knyttet til en gruppe ansatte. Det er også knyttet arbeidsprosesser og rutiner til et Dapla-team som er bestemt av SSB selv. I det følgende forklarer vi begge deler.\n\n\nNår man oppretter et Dapla-team så får man tilgang til et sett med ferdig konfigurerte ressurser og tjenester i GCP. For å få en god forståelse for hvordan disse ressursene og tjenestene fungerer sammen med andre Dapla-team, er det viktig å skjønne hvordan den felles stukturen på GCP er bygd opp.\nAnta at noen oppretter et Dapla-team som heter Arbmark skjema. De vil da få tildelt et teknisk navn som er arbmark-skjema. Sistnevnte navn vil bli brukt i alle ressursene som blir opprettet for teamet.\nSSB har sin egen organisasjon på GCP. Derfor heter toppnivået ssb. Under SSB ligger det 3 Folders2: development, production og staging, som vist i Figur 1.\nUnder hver Folder ligger det ett eller to Google-prosjekter. Det som brukes til statistikkproduksjon ligger under production, mens det som brukes til utvikling og testing ligger under henholdsvis development og staging. I det følgende vil vi kun fokusere på production, som er det som brukes til statistikkproduksjon.\n\nFoldersProjectsBuckets\n\n\n\n\n\nFigur 1: Folders under SSB sin organisasjon på GCP\n\n\n\n\n\n\n\nFigur 2: Prosjektene som opprettes i et Dapla-team.\n\n\n\n\n\n\n\nFigur 3: Bøttene som opprettes i et Dapla-team.\n\n\n\n\n\nI Figur 2 ser vi hvilke prosjekter som blir opprettet for Dapla-teamet arbmark-skjema. Under production ligger prosjektene arbmark-skjema-ts og prod-arbmark-skjema. Det første prosjektet brukes til å synkronisere data mellom bakke og sky. prod-arbmark-skjema er det som brukes til i å lagre data i en statistikkproduksjon.\nI Figur 3 ser vi hvordan lagringsbøttene plasserer seg under prosjektene. I neste kapittel forklarer vi hva de ulike lagringsbøttene skal brukes til.\n\n\n\n\n\n\nHva er en bøtte?\n\n\n\nVi kommer til å bruke ordet bøtte mye i denne delen, og det er derfor ryddig å forklare hva det er.\nEn bøtte er et lagringsområde for data i GCP. En bøtte inneholder objekter av data og metadata som kan organiseres på en slik måte at det likner på filer organisert i mapper og undermapper. Objektene i bøtter er lagret “distribuert”, det vil si at de ligger lagret på ulike maskiner ute i “skyen”, og kan nås via en tjeneste i GCP som heter Cloud Storage (GCS). Bøtter er noe annet enn mapper, og har derfor fått et eget ord på engelsk (buckets).\nHvis vi skulle sammenlignet det med våre systemer på bakken vil det ligne mye på en diskstasjon, for eksempel X- og S-disken.\n\n\n\n\nLagringsområdene for Dapla-team består av Google Cloud Storage (GCS) buckets. Disse bøttene følger en navnestandard som henger sammen med SSBs datatilstander og tilgangsroller. I det følgende forklarer vi hvordan de ulike bøttene er tenkt strukturert.\n\n\nUnder prosjektet prod-arbmark-skjema ligger det 3 bøtter som er tilgjengelig for alle i Dapla-teamet. Disse bøttene er:\n\nssb-prod-arbmark-skjema-data-delt\nLagring av data som skal deles med andre i SSB. Kan innholde inndata, klargjorte data, statistikkdata og utdata.\nssb-prod-arbmark-skjema-data-kilde\nLangtidslagring av kildedata (se definisjon). Kan kun inneholde kildedata.\nssb-prod-arbmark-skjema-data-produkt\nLagring av data i statistikkproduksjon. Kan innholde inndata, klargjorte data, statistikkdata og utdata.\n\nDisse lagringsområdene er nært knyttet til de ulike datatilstandene som blir beskrevet senere.\n\n\n\nUnder prosjektet arbmark-skjema-ts ligger det 2 bøtter som kun er tilgjengelig for kildedata-ansvarlig (data-admins) i Dapla-teamet. Disse bøttene er:\n\nssb-arbmark-skjema-ts-data-synk-opp\nHer ligger data som er blitt synkronisert opp fra bakke til sky.\nssb-arbmark-skjema-ts-data-synk-ned\nHer ligger data som skal synkroniseres ned fra sky til bakke.\n/ssb/cloud_sync/arbmark-skjema/\nLagringsområdet på bakken for synkronisering av data mellom bakke og sky.\n\nKun kildedata-ansvarlig har lese- og skrivetilgang til disse bøttene. Det er også kildedata-ansvarlig som kan sette opp jobber med Transfer Service for å synkronisere data mellom bakke og sky.\n\n\n\n\nVed opprettelsen av et Dapla-team så blir du bedt om å plassere medlemmene i teamet i en av tre ulike tilgangsroller. Disse er:\n\ndata-admins\nHar lese- og skrivetilgang i alle lagringsområdene i Dapla-teamet. Siden dette er en priveligert rolle med potensiell tilgang til sensitiv data, så er det kun noen få personer som skal ha denne rollen i et Dapla-team.\ndevelopers\nHar lese- og skrivetilgang i alle lagringsområdene i Dapla-teamet, med unntak av ssb-prod-arbmark-skjema-data-kilde, og bøttene i prosjektet arbmark-skjema-ts. Dvs. at alle som jobber med statistikkproduksjon tilknyttet teamets data, og som ikke er data-admin, skal ha denne rollen.\nconsumers\nMedlemmer fra andre Dapla-team som har behov for tilgang til dette teamets data. De får lesetilgang til ssb-prod-arbmark-skjema-data-delt.\n\ndata-admin har i tillegg til lagringsområdene på sky, tilgang til denne mappen på bakken: /ssb/cloud_sync/arbmark-skjema. Her kan de legge filer som de ønsker å flytte til skyen.\nTabell 1 viser hvilke roller som har tilgang til hvilke bøtter/mapper.\n\n\nTabell 1: Tilgangsroller og lagringsområder\n\n\n\n\n\n\n\n\n\ndata-admin\ndeveloper\nconsumer\n\n\n\n\narbmark-skjema-kilde\nX\n\n\n\n\narbmark-skjema-produkt\nX\nX\n\n\n\narbmark-skjema-delt\nX\nX\nX\n\n\narbmark-skjema-synk-opp\nX\n\n\n\n\narbmark-skjema-synk-ned\nX\n\n\n\n\n/ssb/cloud_sync/arbmark_skjema/\nX\n\n\n\n\n\n\n\n\n\nTeam Statistikktjenester jobber med en tjeneste for å automatisere overgangen fra kildedata til inndata. Denne tjenesten vil bli tilgjengelig for alle Dapla-team.\n\n\n\n\nI tillegg til man får tilgang til spesifikke GCP-tjenester ved opprettelse av et Dapla-team, så er det også lagt opp til noen spesifikke arbeidsprosesser rundt bøtter og tilgangsroller. I denne delen forklarer vi hvordan dette forholder seg GCP-tjenestene vi beskrev i forrige del.\n\n\nEt viktig konsept på Dapla er datatilstander. Disse er definert i definert i vårt interne dokument Datatilstander av Standardutvalget (2021). I dokumentet presiseres det at tilstandene kildedata, klargjorte data og statistikkdata er obligatoriske for statistikkprodusenter på Dapla.\nI tillegg har Direktørmøtet (2022) konkretisert hvordan klassifisering og tilgangskontroll skal utføres på DAPLA. Under beskriver vi hvordan de to dokumentene påvirker et Dapla-team.\n\n\n\nKildedata er data som er produsert av andre enn SSB. Det kan være data fra andre statlige etater, eller fra private aktører. Kildedata er ofte i form av en fil, eller en mappe med filer. Kildedata skal lagres i bøtten kilde i GCS.\nKildedata skal lagres i den formen den kom til SSB i kildebøtta. Det vil ofte forekomme at disse dataene er sensitive og at de kan inneholde informasjon som ikke skal brukes videre i statistikkproduksjon. Derfor er det kun data-admin som skal ha tilgang til denne bøtten. Og det bør være så få som mulig på teamet som har rollen data-admin, spesielt hvis det er sensitive data.\ndata-admin har ansvaret for å sørge for at kildedata behandles på en måte som gjør at den tilgjengeliggjøres for resten av teamet. Typisk vil dette innebære3:\n\npseudonymisering\ndataminimering\nkvalitetssikring\nkonvertering til et felles format\n\nDet er ikke tenkt at data-admin skal måtte kjøre dette manuelt, men at det skal være en del av en automatisk prosess som kjøres hver gang en ny fil kommer inn i kildebøtta. Det er kun ved mistanke om feil i datafangsten, som gir tjenestlige behov for data-admins til å se data i klartekst, at data-admins bruker tilgangen sin til å se på data i kildebøtta.\n\n\n\nNår kildedata har blitt transformert og beveget seg over i en av de andre datatilstandene, vil det ligge i produkt-bøtta og være tilgjengelig for alle med developers-tilgangen.\nI produktbøtta skal det lagres tre typer data:\n\nInndata\nKlargjorte data\nStatistikkdata\nUtdata\n\nLes mer om det her.\n\n\n\nNår andre Dapla-team skal ha tilgang til data fra ditt team, må de søke om å få tilgangsrollen consumer i ditt team. Du må dermed tilgjengeliggjøre dataene som skal deles i din delt-bøtte.\nMer kommer."
  },
  {
    "objectID": "hva-er-dapla-team.html#google-tjenester",
    "href": "hva-er-dapla-team.html#google-tjenester",
    "title": "Hva er Dapla-team?",
    "section": "",
    "text": "Når man oppretter et Dapla-team så får man tilgang til et sett med ferdig konfigurerte ressurser og tjenester i GCP. For å få en god forståelse for hvordan disse ressursene og tjenestene fungerer sammen med andre Dapla-team, er det viktig å skjønne hvordan den felles stukturen på GCP er bygd opp.\nAnta at noen oppretter et Dapla-team som heter Arbmark skjema. De vil da få tildelt et teknisk navn som er arbmark-skjema. Sistnevnte navn vil bli brukt i alle ressursene som blir opprettet for teamet.\nSSB har sin egen organisasjon på GCP. Derfor heter toppnivået ssb. Under SSB ligger det 3 Folders2: development, production og staging, som vist i Figur 1.\nUnder hver Folder ligger det ett eller to Google-prosjekter. Det som brukes til statistikkproduksjon ligger under production, mens det som brukes til utvikling og testing ligger under henholdsvis development og staging. I det følgende vil vi kun fokusere på production, som er det som brukes til statistikkproduksjon.\n\nFoldersProjectsBuckets\n\n\n\n\n\nFigur 1: Folders under SSB sin organisasjon på GCP\n\n\n\n\n\n\n\nFigur 2: Prosjektene som opprettes i et Dapla-team.\n\n\n\n\n\n\n\nFigur 3: Bøttene som opprettes i et Dapla-team.\n\n\n\n\n\nI Figur 2 ser vi hvilke prosjekter som blir opprettet for Dapla-teamet arbmark-skjema. Under production ligger prosjektene arbmark-skjema-ts og prod-arbmark-skjema. Det første prosjektet brukes til å synkronisere data mellom bakke og sky. prod-arbmark-skjema er det som brukes til i å lagre data i en statistikkproduksjon.\nI Figur 3 ser vi hvordan lagringsbøttene plasserer seg under prosjektene. I neste kapittel forklarer vi hva de ulike lagringsbøttene skal brukes til.\n\n\n\n\n\n\nHva er en bøtte?\n\n\n\nVi kommer til å bruke ordet bøtte mye i denne delen, og det er derfor ryddig å forklare hva det er.\nEn bøtte er et lagringsområde for data i GCP. En bøtte inneholder objekter av data og metadata som kan organiseres på en slik måte at det likner på filer organisert i mapper og undermapper. Objektene i bøtter er lagret “distribuert”, det vil si at de ligger lagret på ulike maskiner ute i “skyen”, og kan nås via en tjeneste i GCP som heter Cloud Storage (GCS). Bøtter er noe annet enn mapper, og har derfor fått et eget ord på engelsk (buckets).\nHvis vi skulle sammenlignet det med våre systemer på bakken vil det ligne mye på en diskstasjon, for eksempel X- og S-disken.\n\n\n\n\nLagringsområdene for Dapla-team består av Google Cloud Storage (GCS) buckets. Disse bøttene følger en navnestandard som henger sammen med SSBs datatilstander og tilgangsroller. I det følgende forklarer vi hvordan de ulike bøttene er tenkt strukturert.\n\n\nUnder prosjektet prod-arbmark-skjema ligger det 3 bøtter som er tilgjengelig for alle i Dapla-teamet. Disse bøttene er:\n\nssb-prod-arbmark-skjema-data-delt\nLagring av data som skal deles med andre i SSB. Kan innholde inndata, klargjorte data, statistikkdata og utdata.\nssb-prod-arbmark-skjema-data-kilde\nLangtidslagring av kildedata (se definisjon). Kan kun inneholde kildedata.\nssb-prod-arbmark-skjema-data-produkt\nLagring av data i statistikkproduksjon. Kan innholde inndata, klargjorte data, statistikkdata og utdata.\n\nDisse lagringsområdene er nært knyttet til de ulike datatilstandene som blir beskrevet senere.\n\n\n\nUnder prosjektet arbmark-skjema-ts ligger det 2 bøtter som kun er tilgjengelig for kildedata-ansvarlig (data-admins) i Dapla-teamet. Disse bøttene er:\n\nssb-arbmark-skjema-ts-data-synk-opp\nHer ligger data som er blitt synkronisert opp fra bakke til sky.\nssb-arbmark-skjema-ts-data-synk-ned\nHer ligger data som skal synkroniseres ned fra sky til bakke.\n/ssb/cloud_sync/arbmark-skjema/\nLagringsområdet på bakken for synkronisering av data mellom bakke og sky.\n\nKun kildedata-ansvarlig har lese- og skrivetilgang til disse bøttene. Det er også kildedata-ansvarlig som kan sette opp jobber med Transfer Service for å synkronisere data mellom bakke og sky.\n\n\n\n\nVed opprettelsen av et Dapla-team så blir du bedt om å plassere medlemmene i teamet i en av tre ulike tilgangsroller. Disse er:\n\ndata-admins\nHar lese- og skrivetilgang i alle lagringsområdene i Dapla-teamet. Siden dette er en priveligert rolle med potensiell tilgang til sensitiv data, så er det kun noen få personer som skal ha denne rollen i et Dapla-team.\ndevelopers\nHar lese- og skrivetilgang i alle lagringsområdene i Dapla-teamet, med unntak av ssb-prod-arbmark-skjema-data-kilde, og bøttene i prosjektet arbmark-skjema-ts. Dvs. at alle som jobber med statistikkproduksjon tilknyttet teamets data, og som ikke er data-admin, skal ha denne rollen.\nconsumers\nMedlemmer fra andre Dapla-team som har behov for tilgang til dette teamets data. De får lesetilgang til ssb-prod-arbmark-skjema-data-delt.\n\ndata-admin har i tillegg til lagringsområdene på sky, tilgang til denne mappen på bakken: /ssb/cloud_sync/arbmark-skjema. Her kan de legge filer som de ønsker å flytte til skyen.\nTabell 1 viser hvilke roller som har tilgang til hvilke bøtter/mapper.\n\n\nTabell 1: Tilgangsroller og lagringsområder\n\n\n\n\n\n\n\n\n\ndata-admin\ndeveloper\nconsumer\n\n\n\n\narbmark-skjema-kilde\nX\n\n\n\n\narbmark-skjema-produkt\nX\nX\n\n\n\narbmark-skjema-delt\nX\nX\nX\n\n\narbmark-skjema-synk-opp\nX\n\n\n\n\narbmark-skjema-synk-ned\nX\n\n\n\n\n/ssb/cloud_sync/arbmark_skjema/\nX\n\n\n\n\n\n\n\n\n\nTeam Statistikktjenester jobber med en tjeneste for å automatisere overgangen fra kildedata til inndata. Denne tjenesten vil bli tilgjengelig for alle Dapla-team."
  },
  {
    "objectID": "hva-er-dapla-team.html#prosesser-og-arbeidsrutiner",
    "href": "hva-er-dapla-team.html#prosesser-og-arbeidsrutiner",
    "title": "Hva er Dapla-team?",
    "section": "",
    "text": "I tillegg til man får tilgang til spesifikke GCP-tjenester ved opprettelse av et Dapla-team, så er det også lagt opp til noen spesifikke arbeidsprosesser rundt bøtter og tilgangsroller. I denne delen forklarer vi hvordan dette forholder seg GCP-tjenestene vi beskrev i forrige del.\n\n\nEt viktig konsept på Dapla er datatilstander. Disse er definert i definert i vårt interne dokument Datatilstander av Standardutvalget (2021). I dokumentet presiseres det at tilstandene kildedata, klargjorte data og statistikkdata er obligatoriske for statistikkprodusenter på Dapla.\nI tillegg har Direktørmøtet (2022) konkretisert hvordan klassifisering og tilgangskontroll skal utføres på DAPLA. Under beskriver vi hvordan de to dokumentene påvirker et Dapla-team.\n\n\n\nKildedata er data som er produsert av andre enn SSB. Det kan være data fra andre statlige etater, eller fra private aktører. Kildedata er ofte i form av en fil, eller en mappe med filer. Kildedata skal lagres i bøtten kilde i GCS.\nKildedata skal lagres i den formen den kom til SSB i kildebøtta. Det vil ofte forekomme at disse dataene er sensitive og at de kan inneholde informasjon som ikke skal brukes videre i statistikkproduksjon. Derfor er det kun data-admin som skal ha tilgang til denne bøtten. Og det bør være så få som mulig på teamet som har rollen data-admin, spesielt hvis det er sensitive data.\ndata-admin har ansvaret for å sørge for at kildedata behandles på en måte som gjør at den tilgjengeliggjøres for resten av teamet. Typisk vil dette innebære3:\n\npseudonymisering\ndataminimering\nkvalitetssikring\nkonvertering til et felles format\n\nDet er ikke tenkt at data-admin skal måtte kjøre dette manuelt, men at det skal være en del av en automatisk prosess som kjøres hver gang en ny fil kommer inn i kildebøtta. Det er kun ved mistanke om feil i datafangsten, som gir tjenestlige behov for data-admins til å se data i klartekst, at data-admins bruker tilgangen sin til å se på data i kildebøtta.\n\n\n\nNår kildedata har blitt transformert og beveget seg over i en av de andre datatilstandene, vil det ligge i produkt-bøtta og være tilgjengelig for alle med developers-tilgangen.\nI produktbøtta skal det lagres tre typer data:\n\nInndata\nKlargjorte data\nStatistikkdata\nUtdata\n\nLes mer om det her.\n\n\n\nNår andre Dapla-team skal ha tilgang til data fra ditt team, må de søke om å få tilgangsrollen consumer i ditt team. Du må dermed tilgjengeliggjøre dataene som skal deles i din delt-bøtte.\nMer kommer."
  },
  {
    "objectID": "hva-er-dapla-team.html#footnotes",
    "href": "hva-er-dapla-team.html#footnotes",
    "title": "Hva er Dapla-team?",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nLagringsområdet tilsvarer stammene på bakken.↩︎\nFolders er et element som en organisasjon kan bruke for å organisere underenheter i GCP. I SSB er Folders brukt til å skille mellom produksjons-, test- og utviklingsmiljøer. Statistikkseksjonene trenger ikke å forholde seg noe særlig til det tekniske rundt Folders, bortsett fra at miljøene for test og produksjon er satt opp noe annerledes. Hvis du ønsker kan du lese mer om Folders kan gå til Google sine sider.↩︎\nHvilken behandling av dataene som kreves vil avhenge av datakildene og kan variere fra statistikk til statistikk.↩︎"
  },
  {
    "objectID": "automatisering.html",
    "href": "automatisering.html",
    "title": "Automatisering",
    "section": "",
    "text": "Denne tjenesten er under utvikling og kan ikke anses som klar for produksjon.\n\n\n\nFor å redusere tilgang til PII1 oppfordres alle Dapla-team til å ha en automatisert prosessering av kildedata. Dapla tilbyr dette som en 100% selvbetjent løsning. Kildedata (Standardutvalget 2021, 5) prosesseres til inndata gjennom et gitt sett av operasjoner(se Figur 1). Kildedataprosesseringsløsningen som tilbys på Dapla er bygget på at hver kildedatafil behandles individuelt. Mer komplekse operasjoner som går på tvers av flere filer bør utføres på inndata eller senere datatilstander.\n\n\n\n\n\n\nDet er kun teamets kildedataansvarlige som skal aksessere kildedata.\n\n\n\n\n\n\n\n\n\nTeamets kildedataansvarlige tar ansvar for å prosessere kildedata til inndata på en forsvarlig måte.\n\n\n\n\n\n\n\n\nFigur 1: Operasjoner som inngår i kildedataprosessering\n\n\n\nInndata er kildedata som er transformert til SSBs standard lagringsformat. Variabelnavn og -innhold er uendret bortsett fra at:\n\nDirekte identifiserende variabler (f.eks. fødselsnummer) er pseudonymisert\nTegnsett, datoformat, adresse m.m. er endret til SSBs standardformat\nDet benyttes standard kodeverk (Klass) der det er mulig (f.eks. kjønn)\nDataene er minimert slik at kun variablene som er nødvendige i den videre produksjonsprosessen inngår.\n\n\n(Standardutvalget 2021, 8)\nVed transformasjon fra kildedata til inndata er det ikke anbefalt å gjennomføre operasjoner som:\n\nGår på tvers av flere filer\nLegger til nye felt\nEndrer navn på felt\nAggregerer data\n\n\n\n\n\n\nAutomatiseringsløsningen krever at teamets Google-prosjekt kan lese fra teamets Infrastructure as Code (IaC) repo på Github. Følg instruksjonene her for å sette opp dette. Dette er en engangsjobb som må gjøres av en som har administratortilgang til IaC-repoet.\n\n\n\n\n\nKilder konfigureres i teamets Infrastructure as Code (IaC) repo på Github. Det kan finnes basert på følgende formulering: github.com/statisticsnorway/&lt;teamnavn&gt;-iac. Kilder konfigureres under stien automation/source_data/prod på repoet.\n\n\n\nHver kilde konfigureres ved hjelp av to filer:\n\nconfig.yaml som konfigurer hvilken filsti som skal prosesseres i teamets kildedatabøtte. Eksempel:\n\nfolder_prefix: source-folder/2022\n\nprocess_source_data.py som kjøres når en kildedatafil blir prosessert. Her må man skrive en python-funksjon med en bestemt metodesignatur som ser slik ut:\n\n def main(source_file):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         source_file: fully qualified file name of the source data file.\n         Example: gs://ssb-prod-my-project-data-kilde/source-folder/2022/data.xml\n     \"\"\"\nDisse filene må legges til i en mappe per kilde under automation/source_data/prod i IaC repoet. Se eksemplet under for en detaljert forklaring.\n\n\n\n\n\n\nMappenavnet for kilder i IaC repoene blir brukt som navn på ressurser. Dette betyr at de enesete tillatte tegnene i mappenavnet er bokstaver, tall, bindestrek og underscore. Det er ikke tillatt med mellomrom eller andre spesialtegn. Mappenavnet kan bestå av opptil 20 tegn.\n\n\n\n\n\n\nDette avhenger av om prosesseringsscriptet kan behandle alle kildefilene på samme måte, eller om det vil være variasjoner som gjør at prosesseringen bør splittes opp i uavhengige prosesseringsscript.\nGrunner til å differensiere mellom kilder kan være:\n\nKildedatafilene har forskjellig filformat (f.eks xml eller json)\nKildedataene har ulike felter\nKildedataene inneholder PII2 eller ikke\n\n\n\n\n\n\n\n\n\n\nDisse instruksjonene forutsetter at ditt Google-prosjekt er koblet til Github.\n\n\n\n\nOpprette skriptet process_source_data.py som prosesserer kildedatafilen til inndata. Dette kan testes av kildedataansvarlige manuelt på Jupyter for å verifisere at dataene blir prosessert som ønsket.\nI en branch i teamets IaC repo, legg til filene config.yaml og process_source_data.py i en ny mappe (valgfritt navn) under automation/source_data/prod. Se eksemplet under for en detaljert forklaring av formatet.\nLag en PR på branchen og få den godkjent av kildedataansvarlige.\nVent til alle tester er ferdige. Det skal stå “All checks have passed” før man går videre, hvis testene feiler følg stegene her. \nSkrive atlantis apply i en kommentar på PRen for å opprette det nødvendige infrastruktur for å prosessere kilden.\nMerge PRen.\nSjekk resultatet av det automatiske bygget.\nVerifisere at nye filer lagt i kildedatabøtten blir prosessert til inndata som forventet.\n\n\n\n\n\nLa oss si at et team (småbakst) har to datakilder levert av ulik dataeiere på ulik formater. Den ene er om boller og er på csv format og den andre er om rundstykker og er på json format. Kildedataansvarlige i teamet bestemmer seg for at filene i boller/ er like nok å prosesseres som en kilde, og at filene i rundstykker/ kan prosesseres som en annen kilde.\n\n\nssb-prod-smaabakst-data-kilde\n├── boller\n│   ├── hvetebolle\n│   │   ├── 2018-salg.csv\n│   │   ├── 2019-salg.csv\n│   │   ├── ...\n│   ├── kanelbolle\n│   │   ├── 2018-salg.csv\n│   │   ├── 2019-salg.csv\n│   │   ├── ...\n│   └── skolebolle\n│       ├── 2018-salg.csv\n│       ├── 2019-salg.csv\n│       ├── ...\n└── rundstykker\n    ├── haandverker\n    │   ├── apr-2022-resultater.json\n    │   ├── aug-2022-resultater.json\n    │   ├── ...\n    └── havre\n        ├── apr-2022-resultater.json\n        ├── aug-2022-resultater.json\n        ├── ...\n\n\n\nsmaabakst-iac\n└── automation\n    └── source_data\n        └── prod\n            ├── boller\n            │   ├── config.yaml\n            │   └── process_source_data.py\n            └── rundstykker\n                ├── config.yaml\n                └── process_source_data.py\n\n\n\nfolder_prefix: boller\nVerdien for folder_prefix tilsvarer en “fil sti” i kildedatabøtte. I dette tilfellet vil nye filer lagt til under boller/ trigge en prosessering.\n\n\n\nfolder_prefix: rundstykker\n\n\n\n\n\nMed prosesseringsscriptet mener vi filen process_source_data.py som ligger i en mappe per kilde under automation/source_data/prod. Dette scriptet vil bli kalt hver gang det blir lagt til en ny fil i kildedatabøtten.\nDette betyr at hvis f.eks. 10 filer blir lagt til i kildedatabøtten samtidig så vil det startes opp 10 individuelle Python-prosesser som kaller denne main-metoden med forskjellig source_file. Parameteren source_file vil inneholde hele filstien inkl. filnavn. Så en enkel flytteoperasjon fra kildedatabøtten til inndatebøtten (uten noen form for konvertering) vil kunne uttrykkes slik:\nimport dapla as dp\n\ndestination_bucket_name = \"ssb-prod-smaabakst-data-produkt\"\n\n def main(source_file):\n    # source_file er f.eks: gs://ssb-prod-smaabakst-data-kilde/boller/hveteboller/2018-salg.csv\n    df = dp.read_pandas(source_file,  file_format=\"csv\")\n\n    # Eksempel på konvertering fra xml til parquet-format\n    dp.write_pandas(df, f\"gs://{destination_bucket_name}/inndata/boller/hveteboller/2018-salg.parquet\")\nAlternativt (uten noen form for konvertering)…\nfrom dapla import FileClient\n\nsource_bucket_name = \"ssb-prod-smaabakst-data-kilde\"\ndestination_bucket_name = \"ssb-prod-smaabakst-data-produkt\"\n\n def main(source_file):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         source_file: fully qualified file name of the source data file.\n     \"\"\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    fs = FileClient.get_gcs_file_system()\n    fs.copy(file_path, destination_path)\n\n\nDet anbefales å bruke Pythons logging modul for å logge, og ikke bruke print eller skrive til stdout/stderr. Det er satt opp en standard logger-konfigurasjon som skriver informasjonsmeldinger (log level info) til stdout og feilmeldinger (log level warning eller error) til stderr. Feil som ikke blir håndtert blir automatisk fanget opp og logget av automatiseringsløsningen. Eksemplet nedenfor logger en informasjonsmelding, en advarsel og en feilmelding:\nimport logging\nlogging.info('Til info')\nlogging.warning('Advarsel!')\nlogging.error('En feil oppstod!')"
  },
  {
    "objectID": "automatisering.html#operasjoner-som-inngår-i-kildedataprosessering",
    "href": "automatisering.html#operasjoner-som-inngår-i-kildedataprosessering",
    "title": "Automatisering",
    "section": "",
    "text": "Figur 1: Operasjoner som inngår i kildedataprosessering\n\n\n\nInndata er kildedata som er transformert til SSBs standard lagringsformat. Variabelnavn og -innhold er uendret bortsett fra at:\n\nDirekte identifiserende variabler (f.eks. fødselsnummer) er pseudonymisert\nTegnsett, datoformat, adresse m.m. er endret til SSBs standardformat\nDet benyttes standard kodeverk (Klass) der det er mulig (f.eks. kjønn)\nDataene er minimert slik at kun variablene som er nødvendige i den videre produksjonsprosessen inngår.\n\n\n(Standardutvalget 2021, 8)\nVed transformasjon fra kildedata til inndata er det ikke anbefalt å gjennomføre operasjoner som:\n\nGår på tvers av flere filer\nLegger til nye felt\nEndrer navn på felt\nAggregerer data"
  },
  {
    "objectID": "automatisering.html#ta-tjenesten-i-bruk",
    "href": "automatisering.html#ta-tjenesten-i-bruk",
    "title": "Automatisering",
    "section": "",
    "text": "Automatiseringsløsningen krever at teamets Google-prosjekt kan lese fra teamets Infrastructure as Code (IaC) repo på Github. Følg instruksjonene her for å sette opp dette. Dette er en engangsjobb som må gjøres av en som har administratortilgang til IaC-repoet.\n\n\n\n\n\nKilder konfigureres i teamets Infrastructure as Code (IaC) repo på Github. Det kan finnes basert på følgende formulering: github.com/statisticsnorway/&lt;teamnavn&gt;-iac. Kilder konfigureres under stien automation/source_data/prod på repoet.\n\n\n\nHver kilde konfigureres ved hjelp av to filer:\n\nconfig.yaml som konfigurer hvilken filsti som skal prosesseres i teamets kildedatabøtte. Eksempel:\n\nfolder_prefix: source-folder/2022\n\nprocess_source_data.py som kjøres når en kildedatafil blir prosessert. Her må man skrive en python-funksjon med en bestemt metodesignatur som ser slik ut:\n\n def main(source_file):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         source_file: fully qualified file name of the source data file.\n         Example: gs://ssb-prod-my-project-data-kilde/source-folder/2022/data.xml\n     \"\"\"\nDisse filene må legges til i en mappe per kilde under automation/source_data/prod i IaC repoet. Se eksemplet under for en detaljert forklaring.\n\n\n\n\n\n\nMappenavnet for kilder i IaC repoene blir brukt som navn på ressurser. Dette betyr at de enesete tillatte tegnene i mappenavnet er bokstaver, tall, bindestrek og underscore. Det er ikke tillatt med mellomrom eller andre spesialtegn. Mappenavnet kan bestå av opptil 20 tegn.\n\n\n\n\n\n\nDette avhenger av om prosesseringsscriptet kan behandle alle kildefilene på samme måte, eller om det vil være variasjoner som gjør at prosesseringen bør splittes opp i uavhengige prosesseringsscript.\nGrunner til å differensiere mellom kilder kan være:\n\nKildedatafilene har forskjellig filformat (f.eks xml eller json)\nKildedataene har ulike felter\nKildedataene inneholder PII2 eller ikke\n\n\n\n\n\n\n\n\n\n\nDisse instruksjonene forutsetter at ditt Google-prosjekt er koblet til Github.\n\n\n\n\nOpprette skriptet process_source_data.py som prosesserer kildedatafilen til inndata. Dette kan testes av kildedataansvarlige manuelt på Jupyter for å verifisere at dataene blir prosessert som ønsket.\nI en branch i teamets IaC repo, legg til filene config.yaml og process_source_data.py i en ny mappe (valgfritt navn) under automation/source_data/prod. Se eksemplet under for en detaljert forklaring av formatet.\nLag en PR på branchen og få den godkjent av kildedataansvarlige.\nVent til alle tester er ferdige. Det skal stå “All checks have passed” før man går videre, hvis testene feiler følg stegene her. \nSkrive atlantis apply i en kommentar på PRen for å opprette det nødvendige infrastruktur for å prosessere kilden.\nMerge PRen.\nSjekk resultatet av det automatiske bygget.\nVerifisere at nye filer lagt i kildedatabøtten blir prosessert til inndata som forventet.\n\n\n\n\n\nLa oss si at et team (småbakst) har to datakilder levert av ulik dataeiere på ulik formater. Den ene er om boller og er på csv format og den andre er om rundstykker og er på json format. Kildedataansvarlige i teamet bestemmer seg for at filene i boller/ er like nok å prosesseres som en kilde, og at filene i rundstykker/ kan prosesseres som en annen kilde.\n\n\nssb-prod-smaabakst-data-kilde\n├── boller\n│   ├── hvetebolle\n│   │   ├── 2018-salg.csv\n│   │   ├── 2019-salg.csv\n│   │   ├── ...\n│   ├── kanelbolle\n│   │   ├── 2018-salg.csv\n│   │   ├── 2019-salg.csv\n│   │   ├── ...\n│   └── skolebolle\n│       ├── 2018-salg.csv\n│       ├── 2019-salg.csv\n│       ├── ...\n└── rundstykker\n    ├── haandverker\n    │   ├── apr-2022-resultater.json\n    │   ├── aug-2022-resultater.json\n    │   ├── ...\n    └── havre\n        ├── apr-2022-resultater.json\n        ├── aug-2022-resultater.json\n        ├── ...\n\n\n\nsmaabakst-iac\n└── automation\n    └── source_data\n        └── prod\n            ├── boller\n            │   ├── config.yaml\n            │   └── process_source_data.py\n            └── rundstykker\n                ├── config.yaml\n                └── process_source_data.py\n\n\n\nfolder_prefix: boller\nVerdien for folder_prefix tilsvarer en “fil sti” i kildedatabøtte. I dette tilfellet vil nye filer lagt til under boller/ trigge en prosessering.\n\n\n\nfolder_prefix: rundstykker"
  },
  {
    "objectID": "automatisering.html#skrive-prosesseringsscriptet",
    "href": "automatisering.html#skrive-prosesseringsscriptet",
    "title": "Automatisering",
    "section": "",
    "text": "Med prosesseringsscriptet mener vi filen process_source_data.py som ligger i en mappe per kilde under automation/source_data/prod. Dette scriptet vil bli kalt hver gang det blir lagt til en ny fil i kildedatabøtten.\nDette betyr at hvis f.eks. 10 filer blir lagt til i kildedatabøtten samtidig så vil det startes opp 10 individuelle Python-prosesser som kaller denne main-metoden med forskjellig source_file. Parameteren source_file vil inneholde hele filstien inkl. filnavn. Så en enkel flytteoperasjon fra kildedatabøtten til inndatebøtten (uten noen form for konvertering) vil kunne uttrykkes slik:\nimport dapla as dp\n\ndestination_bucket_name = \"ssb-prod-smaabakst-data-produkt\"\n\n def main(source_file):\n    # source_file er f.eks: gs://ssb-prod-smaabakst-data-kilde/boller/hveteboller/2018-salg.csv\n    df = dp.read_pandas(source_file,  file_format=\"csv\")\n\n    # Eksempel på konvertering fra xml til parquet-format\n    dp.write_pandas(df, f\"gs://{destination_bucket_name}/inndata/boller/hveteboller/2018-salg.parquet\")\nAlternativt (uten noen form for konvertering)…\nfrom dapla import FileClient\n\nsource_bucket_name = \"ssb-prod-smaabakst-data-kilde\"\ndestination_bucket_name = \"ssb-prod-smaabakst-data-produkt\"\n\n def main(source_file):\n     \"\"\"Function is called when a file is added to the source-data bucket.\n     Args:\n         source_file: fully qualified file name of the source data file.\n     \"\"\"\n    destination_path = file_path.replace(source_bucket_name, destination_bucket_name)\n    fs = FileClient.get_gcs_file_system()\n    fs.copy(file_path, destination_path)\n\n\nDet anbefales å bruke Pythons logging modul for å logge, og ikke bruke print eller skrive til stdout/stderr. Det er satt opp en standard logger-konfigurasjon som skriver informasjonsmeldinger (log level info) til stdout og feilmeldinger (log level warning eller error) til stderr. Feil som ikke blir håndtert blir automatisk fanget opp og logget av automatiseringsløsningen. Eksemplet nedenfor logger en informasjonsmelding, en advarsel og en feilmelding:\nimport logging\nlogging.info('Til info')\nlogging.warning('Advarsel!')\nlogging.error('En feil oppstod!')"
  },
  {
    "objectID": "automatisering.html#footnotes",
    "href": "automatisering.html#footnotes",
    "title": "Automatisering",
    "section": "Fotnoter",
    "text": "Fotnoter\n\n\nPersonidentifiserende Informasjon↩︎\nPersonidentifiserende Informasjon↩︎"
  }
]